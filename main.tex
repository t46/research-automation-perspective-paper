% \documentclass{article}
\documentclass{book}

\let\cleardoublepage\clearpage

\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
% \setlength{\parindent}{0pt}

% \usepackage{natbib}
\newenvironment{abstract}{}{}
\usepackage{abstract}

\hypersetup{
    colorlinks=true,
    linkcolor=[RGB]{255,34,255},
    citecolor=[RGB]{255,34,255},
}

\title{Autonomous Artificial Researcher Perspective Paper}
\author{Shiro Takagi}

\begin{document}
\sloppy
\maketitle
\tableofcontents

\begin{abstract}
    In this paper, I propose a perspective on the realization of machine learning algorithms that can conduct research autonomously. First, I look back at what kind of activities research, which is the subject of automation, involves. Next, I review existing research attempting to automate each component of these research activities. Based on these, I propose ideas for the realization of artificial researchers who can conduct research autonomously, and suggest a roadmap and action plan to achieve them.
\end{abstract}

\chapter{Introduction}

\section{Background}
Research has greatly supported the development of humanity so far. It has enabled agents to deepen their understanding of themselves and the world around them, and in doing so, acquire new capabilities. In this sense, research is an extremely important activity.

\section{Purpose of this perspective paper}

The purpose of this paper is to discuss the possibility of automating this activity of research, that is, the realization of intelligent agents capable of conducting research autonomously. After summarizing attempts at research automation so far, I intend to present my personal perspective on promising ideas, necessary elements, and ways to proceed in order to create artificial researchers. In particular, this paper is aimed at machine learning researchers and developers, with the goal of increasing the number of colleagues aiming for research automation by providing concrete possible actions.

One thing to note here is that my aim is not to automate or substitute the current research tasks, but to create intelligent agents capable of conducting research. In other words, as long as it is research, automated research may at first glance seem to be far removed from the current research activities. This is because the current research activity may not necessarily be the absolute optimal form in light of its purpose. 

First, the optimal form is always influenced by the context of history. Of course, the optimum in the era when letterpress printing was mainstream is different from the optimum in the era when the Internet became infrastructure. Research itself has changed its form over time, and the optimal form of research in the present or future can naturally change. Moreover, as Nielsen and Qiu say \cite{nielsen}, it is believed that we have only explored a very limited range regarding the practice of research. The establishment of current research practices is a very recent event in human history, and we may not yet have arrived at the optimum way of intellectual production. Furthermore, current research assumes that humans are the main creators of knowledge. This naturally imposes human cognitive constraints, which may significantly limit the range of activities that can be conducted for knowledge production. 

Therefore, instead of thinking about how to automate and streamline current tasks, I discuss the possibility of realizing intelligent agents that can autonomously perform the fundamental and core elements of research activity in a more optimal form, and what is necessary for this.

\section{Continual update of this paper}

Additionally, this paper is intended to be updated on an ongoing basis. If you find points that you think are lacking or inaccuracies in understanding, please send a Pull Request on GitHub. Based on that, I will revise the content of the paper as needed.

\section{Notes}
\textcolor{red}{TODO: Add any notes and policies of this perspective paper, e.g. free from human convention, limited knowledge on philosophy, scope, etc}

\textcolor{red}{TODO: Add the explanations on what's the aim of each section}

\textcolor{red}{TODO: Add lots of high-level illustration}

\chapter{What is Research?}

To create an agent capable of conducting research autonomously, it is crucial to understand what research is in the first place. Therefore, in this chapter, I would like to discuss the characteristics of what is called research.

However, my aim here is not to identify a universal, historically independent, and absolutely singular definition of what research is. It seems almost impossible to specify such a thing \cite{chalmers2013thing,sep-scientific-method}. I do not totally believe that I, a layperson in the philosophy of science, could provide an answer to a question that has yet to be resolved even within the philosophy of science.

Rather, my goal is to endeavor to describe various characteristics of research to the extent that it can provide a starting point for engineers and information scientists who aim for autonomous research capabilities. To that end, I first discuss a possible working definition of research.

\section{A Working Definition of Research}

\subsection{Research as Knowledge Production}

One commonly mentioned definition of research is the act of generating new knowledge. For instance, research of mathematics create unknown proofs of theorems, physics elucidates unknown natural laws, and engineering produces blueprints for never existed things. I will adopt this definition as the working definition in this paper. \textbf{Therefore, whenever I refer to research in the following, please interpret it as discussing the novel knowledge production.}

The reason why I deliberately use the word research instead of science is because I want to include fields like humanities and arts, which are not typically referred to as science, within the scope of automation in the long run. Science refers to a methodology for generating knowledge, and I believe that new knowledge is not necessarily produced only by scientific methods. I believe that so-called humanities and arts also share the commonality of producing new knowledge. Therefore, the definition of generating new knowledge can be said to encompass these fields as well.

Science is an extremely precise and robust  framework for knowledge production. Because of its power and popularity, most of the existing analysis on research is about science. Therefore, although this paper uses the word research, the automation of science will be at the center of my discussion. I plan to explore the analysis towards the automation of research in humanities and arts in future.

\subsection{Knowledge Production as Belief Revision}
Before delving into how humans have been producing knowledge, let's think a bit deeper about what it means to produce knowledge. Defining knowledge rigorously is a philosophical debate that has not yet been settled \cite{sep-epistemology}, and I won't delve into it deeply here. Instead, I would like to provide some primitive ideas as a machine learning researcher that can serve as a starting point for further discussions on the automation of research.

\subsubsection{Knowledge as Belief}
I stated that I will provisionally define research as the generation of new knowledge. So let's first consider what knowledge is. The question of what is the thing called knowledge has been a subject of philosophical debate for a long time in the field of \textit{epistemology}.

In epistemology, knowledge has been traditionally considered to be \textit{justified true belief (JTB)} \cite{sep-epistemology}. The term ``true'' is difficult to define rigorously in philosophical terms, but for the purpose of discussion, let's think of it as something being fact. ``Belief'' can be provisionally understood as someone's thought or conviction about something. And ``justified'' means that it is deemed reasonable to hold such a belief. The meaning, necessity, and sufficiency of justification has been discussed in epistemology as a central point of contention. However, I won't touch the discussions here because I am 
 not well-versed in epistemology, and detailed discussions on that topic would deviate from the scope of this paper.

Of course, there is much debate about whether JTB truly is knowledge, and many philosopher agree that JTB is not the sufficient condition of knowledge. While these characteristics are not be sufficient conditions, there is generally some agreement that they seem to be necessary. Also, as even epistemological discussions revolve around these notions, they seem to provide a reasonable starting point just for the purpose our discussion. Therefore, for the sake of further argumentation, let's tentatively understand the generation of knowledge as the process of justifying, validating, or confirming beliefs in some form.

\subsubsection{Research as Belief Revision}
I'm not sure if it is necessary for JTB to be strictly true in order for an action to be considered research, but intuitively, it seems somewhat valid as a description of research that it is updating a belief about a certain object. For example, let's consider a scenario where a hypothesis is proposed for a certain phenomenon, and then it is subjected to a test. This is a common practice in research. Now, suppose that through this testing, the hypothesis is shown to be plausible. This means that as a result of the test, the belief in the correctness of the hypothesis is strengthened. In the nature of research, which aims to generate new knowledge, it is necessary to engage in inference and justification regarding the unknown. Even if a hypothesis is not explicitly stated, some form of implicit hypothesis generation and testing should occurs in research. Therefore, the view that research involves the update of belief seems to be reasonably valid.

Let me explain a bit more about my thoughts on updating beliefs, or the reason why I say ``the hypothesis has been validated'' without claiming that the hypothesis turned out that the only absolute truth but rather just stating that the belief has been strengthened. This is partly because, in almost all fields that rely on empirical methods, except for deductive disciplines like mathematics and logic, it is often impossible for verification to definitively determine the correctness of a hypothesis. Most research evaluates the validity of a hypothesis based on observed cases, relying on inductive reasoning \cite{sep-scientific-method}. However, inductive reasoning is believed to be unable to rigorously determine the truth or falsehood of propositions without assumptions, as deduction does \cite{sep-induction-problem}. Due to this situation, I am cautious with my choice of words to emphasize that only the belief is updated.

However, I don't believe that because inductive reasoning doesn't rigorously determine truth or falsehood in the same way deduction does, it renders the reliance on inductive reasoning meaningless. The problem of what assumptions would justify inductive reasoning or, even if they cannot be justified, what it means for us to consider them rational is an extremely difficult issue that remains unanswered in philosophy. However, the many assumptions discussed there seem to be very natural to us humans. As an example, it is said that we need to assume that ``under the same conditions, the same phenomenon will continue to hold'' (principle of uniformity of nature) \cite{sep-induction-problem}, which is an extremely intuitive assumption without which we could hardly even go about our daily lives. Therefore, while I don't know what exactly constitutes the basis of inductive reasoning, it would seem to be a deeply rooted and unwavering belief that is grounded in our perception and experiences.

I understand that justifying the validity of inductive reasoning based on our experiences would be circular reasoning, and we cannot guarantee that these assumptions always hold ever. However, the extremely strong beliefs that feel justified based on our perception and experience seem much more reliable than some random conjecture. In this sense, relying on inductive reasoning feels meaningful to me. \footnote{I naively explored the differences in strength within beliefs, the idea that beliefs rooted directly in perception are more robust, and the notion that anchoring a belief on those foundations enhances its reliability. However, the question of how justified beliefs are grounded and the nature of their structure is a highly complex problem with extensive discussions  \cite{sep-epistemology}. If I understand it correctly, my position may align somewhat with what is called \textit{empirical foundationalism}. Properly engaging with these discussions and considering where to seek the foundations of knowledge seems crucial in debating the new manner of generating knowledge. However, due to my limited philosophical knowledge and the ability to delve deeper into philosophical discussions, this paper will not further pursue these arguments.}

% \textcolor{red}{TODO: Add excuse for empirical foundationalism}
% The idea that there is ``strength'' in beliefs may be related to a position called \textit{foundationalism} in epistemology, which claims that ``\textit{our justified beliefs are structured like a building: they are divided into a foundation and a superstructure, the latter resting upon the former. Beliefs belonging to the foundation are basic. Beliefs belonging to the superstructure are nonbasic and receive justification from the justified beliefs in the foundation.}'' \cite{sep-epistemology}. In particular, I believe that the more fundamental beliefs to which these beliefs belong are rooted in human perceptual experience and empirical knowledge. This position is commonly referred to as \textit{empirical foundationalism} \cite{sep-epistemology}.

What I want to emphasize here is that research can be seen as linking or replacing the weak belief in the plausibility of newly conceived hypotheses with such extremely strong beliefs. For example, believing in the effectiveness of statistical methods is strongly related to believing in the effectiveness of inductive reasoning. Therefore, if the validity of a hypothesis is confirmed using statistical methods, we would believe it as highly reliable. Hence, considering research as the updating of beliefs can be reasonably felt, and I don't intend to argue that research is meaningless just because it is the belief revision. 

\subsection{Novelty of Knowledge}
I stated that the acquisition of knowledge is the updating of beliefs. However, as defined earlier, research is considered to be the process of producing ``new'' knowledge or transforming the unknown into the known. No matter how firmly a belief is confirmed, if it is already known, it cannot be called research. Therefore, it seems necessary to carefully discuss what it means for knowledge to be unknown or novel.

I said in the paragraph above that ``research is the process of transforming the unknown into the known.'' However research can also be seen as the updating of beliefs, as I have explained. Therefore, the binary depiction of an object suddenly transitioning from the states of unknown to known does not seem appropriate. Rather, it seems more reasonable to consider that beliefs continuously change and we just call some group of belief states unknown and others known, for convenience.

I don't know precisely what it means to be unknown. This is a difficult problem, but let's consider it naively. First, research begins with a particular question. The state of not knowing the answer to this question is what I consider the state of being unknown. In other words, it can be thought of as a state where we don't know what the candidate hypotheses, which are the potential answers, are like, or a state where we know the candidate hypotheses but don't know their plausibility. These are states where we have not been able to find hypotheses or sets of hypotheses that can be assigned a particularly high degree of confidence from the set of potential answers to a given question. Therefore, provisionally and casually, it might be said that the state of ``not having highly confident beliefs (or a set of beliefs) for a particular question'' is unknown, and the state of having highly confident justified beliefs is known.

Of course, there are issues with this clarification. For example, it is unlikely that we can select a single highly confident hypothesis from the entire set of possible hypotheses. Also, rather than feeling equally confident about all possible hypotheses, it seems that we implicitly distinguish between hypotheses that seem relevant and those that do not. Furthermore, it is unclear to what extent we consider a state to be unknown based on the degree of confidence. However, these are highly challenging philosophical discussions, so I will refrain from delving further into them and, for now, would like to conclude with the vague and provisional definition above and move on to the next topic.

\subsection{Publicity of Knowledge}
\subsubsection{Knowledge as Human Knowledge}
In this paper, I declared that I will regard knowledge as belief. I believe that, in addition to this, the research implicitly assumes that the knowledge generated is understandable to other individuals. This is because even if a belief is strong, if it is something that only one person has, we would not consider or acknowledge it as knowledge by research. Beliefs, knowledge, and understanding are indeed subjective concepts by nature, but it seems that as humans, we demand that they go beyond subjectivity and become comprehensible to others in some form. Therefore, I believe research is the act of not only changing an individual's belief but also changing multiple individuals' beliefs. Although I don't think it is possible for multiple people to hold exactly the same belief or for their beliefs to change in exactly the same direction, we at least need to use a way to change a collection of human beliefs in a similar direction.

And I believe that we achieve this by reducing hypotheses to strong convictions that everyone, regardless of individual differences, possesses as human beings. For example, assumptions underlying inductive reasoning, as I described earlier, will fall into this category. Another strong conviction humans believe in is that the more we observe an increasing number of results derived from a certain hypothesis, the more reliable and certain that hypothesis feels. Research need to be objective because it's required to generate knowledge for humanity, and I believe it is because we strive to reduce hypotheses to such strong convictions that it is called objective. 

I admit that the objectivity of science is also a challenging issue. There are many of philosophical discussions, and I do not possess the ability to encompass all of its complexities. Please note that what I have stated here is merely the humble intuition of an machine learning researcher.

\subsubsection{Knowledge as Agents' Knowledge}
I mentioned that research is the act of generating knowledge for humanity. However, I believe this is merely because humans have been the ones conducting research thus far. I believe that there could be knowledge for beings other than humans and hence research for them as well. First, knowledge is based on beliefs, so if a non-human agent holds beliefs, it would be reasonable to consider that they possess knowledge, even if they may not resemble human knowledge on the surface. The nature of beliefs is a complex matter to define concretely. However, even in current machine learning, models have confidence levels and prediction errors, which I believe are not completely unrelated to what we consider beliefs. Furthermore, even if the knowledge is not understandable to humans, if it is understood among a group of agents, meaning it is reduced to a shared belief, then it can be called ``knowledge for that group of agents.'' If a group of agents has means to update a belief to such a strong shared belief, then I consider it as research. From these reasons, I believe that research is conceivable for entities other than humans. And it feels like this is a highly significant conclusion in terms of contemplating the development of intelligent systems capable of conducting research.

\textcolor{red}{TODO: Add relations with relativisim, maybe added to alignment section?}
% This position may have some similarities with a famous stance in philosophy of science that argues everything is, at its core, subjective, relative or a matter of belief \cite{chalmers2013thing,kerlinger1973structure,howson2006scientific}. However, what I want to emphasize here is that I think the characteristic of research lies in tying beliefs about an object to stronger beliefs. And I believe that it is this strength of conviction, which seems self-evident to many people, that lends objectivity and persuasiveness to the claims of science.

\subsection{Rigorous Methods of Confirmation}
While this discussion may deviate slightly from the definition of research, I believe that one of the distinguishing characteristics of research is its critical and rigorous approach to confirming whether a hypothesis is truly valid. The means employed to verify a hypothesis in research are conducted with extreme precision and caution. In our daily lives, we often casually ``validate'' hypotheses, but these activities are generally not considered research because they lack the meticulousness and objectivity that are integral to the research process. I admit that empirical methods cannot definitively verify hypotheses as deduction does and numerous auxiliary hypotheses are implicitly generated, making it challenging to determine whether a single experiment is truly supporting or rejecting a hypothesis. However, research is designed and executed with much greater care and precision than everyday ``validation'' activities. While neither approach can provide absolute certainty, it would be unproductive to equate them, considering the stark differences in their meticulousness and design.

% This is not definition but practically I think that one characteristics of research is its rigor of confirmation method. Firstly, you may wonder if anything unknown would suffice. I believe that research does not choose its subject and anything unknown is acceptable. Rather, what's important is the strictness of the methodology – whether the unknown truly became closer to the known, whether it was concluded into a stronger belief. Generally, the method employed by what we call research is designed with extreme precision, and as a result, it seems to withstand rigorous evaluations. This can be considered a major feature that distinguishes research from other various activities.

\subsection{Interim Summary}
In this section, I have discussed a provisional working definition of research. I started with the naive intuition that research is the endeavor to generate new knowledge. I then explained that knowledge is belief, the production of knowledge involves updating beliefs, and the produced knowledge needs to be novel and supported by the common strong beliefs of a community. Lastly, I discussed based on these conclusions the possibility of research conducted by agents other than humans.

What I want to emphasize here is not that we should aim for knowledge systems that are incomprehensible to humans or anything of that sort. The meaning and value of the conclusion from the paragraph above will be discussed later, but what I want to emphasize here is that such conclusions naturally arise from just looking at the abstract definition of research. And I believe that this abstraction is crucial for striving towards the realization of artificial intelligence capable of conducting research. To clarify, I am not aiming for the automation of tasks associated with current research, which is a critical distinction. And the discussions above suggests that, for example it matters to consider how to acquire the means of reducing beliefs to shared beliefs and something like that. 

Reflecting on what research entails is highly important as it provides clarity on what we should truly strive for and offers guiding principles for our efforts. The definition discussed here is merely a provisional one based on the naive perspective of an AI researcher. By combining insights from philosophers of science, epistemology, and researchers in the field, we can engage in a deeper analysis to develop more fruitful and reliable guidelines.

% The reason I deliberately distinguished between mere knowledge and knowledge for humanity is because I believe that there could be knowledge for beings other than humans. As I have repeated, understanding is subjective, so if machines start conducting research in the future, it is natural to think that there will be unknowns for machines and beliefs for machines. Therefore, it's possible that we could live in a world where humans produce knowledge for humans, machines produce knowledge for machines, we could live in a world where knowledge is produced for all agents including machines and humans, or we could continue living in a world where knowledge is produced solely for humans. In this sense, I believe one of the problems that will be questioned in the future is whose objectivity and whose belief we are talking about. I will discuss this point in more detail later.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% When a hypothesis survives a test, it was said that our belief in the likelihood of the hypothesis strengthens. Empirical science implicitly assumes a principle that relies on inductive reasoning as the basis for these tests. For example, we believe that if the number of observations consistent with a certain claim increases, that claim is more likely to be valid (this is called the principle of confirmation). Additionally, we hold the belief that unless other factors change, what has held true so far will continue to hold true (this is called the principle of uniformity of nature). These principles of confirmation and uniformity are the foundations of inductive reasoning, and they are unavoidable in empirical science. However, both the principle of confirmation and the principle of uniformity are merely beliefs, and there is no guarantee anywhere that these are ``correct''. The reason these can serve as the foundation for testing is because these beliefs are far more solid than the belief in the likelihood of a hypothesis that someone has just recently presented. That is, empirical science could be described as an endeavor to update the likelihood of a hypothesis by tying the belief held about a certain hypothesis to a more solid belief. While mathematics and logic are rare examples, considering that all other research endeavors are fundamentally empirical, \textbf{it may be possible to rephrase the endeavor of research, that is, the production of knowledge, as largely an endeavor to update our belief in the correctness of a certain object.} In fact, it is widely accepted in philosophy that knowledge requires belief \cite{sep-epistemology}. More formally, it's believed that "\textit{the three conditions—truth, belief, and justification—are individually necessary}" for knowing a fact \cite{sep-epistemology}.


% \subsection{Disclaimer Regarding the Characteristics of Research}
% I have mentioned that research is an endeavor to transform the unknown into the known, but you may question what distinguishes it from other activities that appear to do the same. This issue arises from the fact that I have not properly defined knowledge in this paper. In this paper, I won't discuss this in detail but will limit myself to a brief disclaimer.


% \subsection{The Previous Discussions Regarding the Definition of Research (Scientific Discovery).}

\section{Research Process}

\textcolor{red}{TODO: clarify what I mean by ``essential'', e.g. not constrained by human convention, computation (not representation nor hardware), objective (not method), may change to Marr's three levels}

\textcolor{red}{TODO: more focus on the implication for research automation}

% It is believed that research began with individual and concrete tasks. Among them, common actions were patterned and crystallized as a scientific method. We currently recognize this abstract set of behaviors as research. For example, hypothetico-deductive method and hypothesis testing are abstracted scientific method.

% Also, researchers use a research paper as a medium of knowledge transfer. Therefore, there are patterned activities related to a research paper. Examples of these include conducting surveys, gathering information from papers, and writing a thesis.

% Note that these are necessary tasks just because we use a paper as a medium of knowledge transfer, but they may not necessarily be indispensable for generating new knowledge. There are other such tasks as well. For example, peer review and fund raising are essential to current research practices in society, but they may not necessarily be indispensable for knowledge production.

% In this way, various tasks arise in conjunction with research. When considering the automation and optimization of research, it is desirable to consider streamlining all of these tasks. However, in this article, we focus on the process from determining a research topic to publishing a research paper. We will refer to this process simply as the \textit{research process} from here on.

% \subsection{Overview}

% As mentioned earlier, research is an attempt to turn the unknown into the known. Therefore, the research process can be seen as a function that takes the unknown as input and outputs the known. However, in reality, a single research paper may not be enough to turn the unknown into the known. Therefore, in practice, the research process is considered to be a procedure that takes the unknown as input, and outputs a text that describes the procedures and their results, as well as their interpretation, in order to turn the unknown into the known.

% First, let me structure the common research process. In particular, I will base the structuring of the research process on the method of empirical science, which many researches rely on as a foundation. However, I believe that this framework can be applied to other research activities, such as mathematics, as well. I will explain the reason for this later.

% The research process, especially that of empirical science, is carried out through the following steps: topic decision, hypothesis generation, verification design, verification, and analysis of experimental results. The outputs of these steps are then written into a paper, which undergoes peer review and is eventually published.

% Note that some commonly seen items, such as surveys, are not included here for a reason. First, as mentioned earlier, gathering information from papers is only a means of knowledge transfer through the use of a thesis. Second, information extraction from papers can be done at any stage of the research process. Thus, I believe that processing related to a paper, such as \textit{reading papers} and \textit{writing a paper}, needs to be considered separately from the aforementioned research process.

\subsection{Overview}

\textcolor{red}{TODO: reconsider the research process, structure of knowledge production system, and the scope of this paper}

In the previous chapter, I discussed the definition of research. In this chapter, I will focus on the high-level abstract structuring of the research process while paying attention to its functional aspects. By emphasizing the functional aspects, I mean paying attention to the role that each step plays in knowledge production. By higher-level abstract structuring, I intend to focus on the processes that is as universal as possible across research fields, regardless of the specific domain. The purpose of this kind of structuring is to clarify what kind of modules should be created as intermediate steps of research when aiming for research automation. In the following, I will structurize the research process into a chronological sequence for the purpose of clarity. However, it is important to note that the focus lies not on the temporal order nor human convention, but rather on the functionality in relation to knowledge production and the inputs and outputs of each of these processes. Also, as previously mentioned, because humans are currently the primary knowledge generators, there are many constraints that come from human society. Thus I will do my best to distinguish and organize what is dependent on humans and what is not. Before delving into specific discussions, let me first explain the scope of this chapter. After that, I will discuss the outline to be addressed in this section, followed by the main discussions.

\subsubsection{Scope of the Discussion}
First, I think that the activities related to research can be broadly divided into two phases: the process of knowledge production, typically in the form of papers or other publications, and the phase of maintaining, sharing, evaluating, and utilizing the knowledge. While the latter phase is a crucial aspect of knowledge, this discussion primarily focuses on knowledge production, so it will not be extensively addressed here. However, it's important to note that the distinction between the two phases is not strictly delineated. For instance, how knowledge is used often influences the production process. Additionally, research evaluation and revision, as exemplified by the ideas of Popper and others, are ongoing processes, and as mentioned earlier, belief updates are not set in stone. Thus, it may not be good idea to exclude discussions about evaluation simply because they mainly take place after publication. As such, given the difficulty in making a strict demarcation, I will make sure to touch upon elements that are considered essential for knowledge production even within the latter phase. The aforementioned distinction is merely a matter of emphasis, and it should be understood that it does not imply completely disregarding the latter phase. \textcolor{red}{Add the fig of knowledge production / use phases}

Furthermore, factors influenced by social constraints such as funding acquisition and collaboration agreements that typically occur before the research are not addressed in this context. Knowledge production can be seen as a function that derives knowledge from inputs such as people, capital, resources, and information. Viewing it this way, the acquisition of money or human capital can be considered inputs to the function rather than being inputs themselves. Regarding information acquisition, however, specific aspects strongly related to research will be addressed separately. I admit that these factors are also crucial elements in the pursuit of true autonomy in research, so they will be subject to future discussions. \textcolor{red}{Add the fig of knowledge production system}

In conclusion, the following text focuses on the process of research, where unknowns are input as research questions and knowledge is output as research outcomes. For simplicity, we will refer to this process as the \textit{research process}. It is worth reiterating that this distinction is provisional, intended to facilitate further discussion and the development of better categorizations or broader automation in the future.

\textcolor{red}{TODO: add the excuse that research is social activity}

\subsubsection{Outline of the Structure}
In this section, I will first discuss the functionally essential elements for knowledge production, which is the abstract structure of the research process that has been emphasized throughout. Next, I will talk about peer review, which is a widely practiced convention in current research. While its necessity for knowledge production is uncertain for me, I will present thoughts on the role it is believed to play in knowledge production. Lastly, I will discuss the ``techniques,'' which may not be a necessary condition for research but plays an extremely universal and significant role in knowledge production across various fields in the current society.


In the first part, I will explain three elements that I think are functionally necessary in knowledge production: formulating questions, generating hypotheses, and testing hypotheses. Constructing questions is equal to deciding the unknown to be investigated, which is necessary by definition for research, which is the art of bringing the unknown closer to the known. Generating hypotheses is necessary when dealing with the unknown, although they may be implicit and the extent may vary. If inference about the unknown were always correct, the unknown would not have been unknown in the first place. Therefore, uncertainty accompanies the process of inference when dealing with the truly unknown. The third element involves strengthening the conviction in proposed beliefs. This element is indispensable, as it plays a role in converting beliefs into knowledge. While the term ``verification'' is used here, it does not imply a definitive determination of the truth or falsehood of propositions, as commonly used in philosophy. It is used in the sense of just strengthening beliefs as used in everyday language. Thus, ``confirmation'' may be a more accurate term, but verification is more widely recognized, so I will use that. As a practice of human science, there are cases where hypothesis generation and justification are not strictly separate \cite{arabatzis2006inextricability}. However, functionally, discovery and justification are two distinct phase, I will discuss them separately here. These are the elements that I consider functionally necessary for research. \textcolor{red}{TODO: However, we can identify discovery with justification because both are belief updates. I'll add this point wherever in this paper.} Fig. \ref{fig:research_process}.

\textcolor{red}{TODO: Add fig like this. add this sentence to the explanation of the fig "We approach the unknown to the known by asking questions, providing tentative answers to them, and then verifying the validity of those answers. " } Fig. \ref{fig:research_process}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{figs/researchprocess.jpg}
    \caption{Caption}
    \label{fig:research_process}
\end{figure}


In the second part, I will discuss the potential role of peer review. The reason for discussing peer review is primarily because it is a widely practiced convention across various fields. While peer review is a social convention of human societies, whether it is necessary or not for research, it is impossible to avoid discussing something that has been so widely accepted when considering research automation. Aside from the feeling of obligation, I will discuss the seemingly important role of peer review, which spans both the preceding knowledge production and the subsequent knowledge utilization. I will also explore the possibility that the redundancy in the functions of peer review has played an essential role in the knowledge production of human societies thus far. Lastly, I will present my personal viewpoint on how to perceive this aspect in the context of research automation.


In the third part, I will discuss information retrieval, literacy, data analysis, and deductive reasoning. If there is absolutely no access to any information whatsoever, knowledge production is considered impossible. Research, like other activities, requires acquiring information. Therefore, techniques for information retrieval, including searching and questioning, are essential in research across various fields. In particular, research builds upon existing knowledge, often in the form of papers, to generate new knowledge. Thus, I will focus on discussing aspects related to research, such as paper searching. The second aspect is literacy, specifically the ability to read and write. We express all kinds of information through text. Therefore, the ability to read documents is inherently important in information acquisition. Additionally, I express knowledge through the generation of papers, which serve as a medium for information exchange. Therefore, knowledge production ultimately involves generating written texts. These discussions revolve around the ``representation'' of knowledge and information, which are constrained by societal practices. However, without these skills, conducting research in human society would be impossible, making them necessary abilities across all fields. Hence, I will discuss reading and writing skills. The third aspect is data analysis. As mentioned earlier, a significant portion of research is empirical. Therefore, data analysis is a necessary skill regardless of the field. Hence, I will discuss this topic. The fourth aspect is deductive or systematic reasoning. This is essential in mathematics and natural sciences and is required across various fields in the natural sciences. Therefore, I will revisit this topic for further discussion. The latter two aspects, data analysis and deduction, are indispensable in research. However, they should be consciously distinguished as ``techniques'' or ``methods'' aimed at achieving the ``purpose'' of hypothesis generation and hypothesis testing, respectively. In this paper, I pay careful attention to making this distinction. \textcolor{red}{TODO: may be moved to each subsection}

% Next, I will discuss a high level description of how human beings have been conducting research. I'll structurize the abstract pattern of the process (which I will call\textit{ research process}) from determining the unknown to it turning into known. 


% \subsubsection{Note}

% note, direction
% Though my structuring may seem to represent scientific methods, I believe this pattern cam apply to other research fields, such as mathematics and humanities as well. When describing the structure, I will make a conscious effort to clearly distinguish between essential elements for knowledge production and those that are not. As previously mentioned, because humans are currently the primary knowledge generators, there are many constraints that come from human society. When considering the possibility of machines conducting research in the future, it will be important to distinguish and organize what is dependent on humans and what is not.

% I believe that the conduct of human research activities can be roughly divided into three stages: knowledge production, knowledge evaluation, and knowledge sharing. 

% Although these may not necessarily be distinctly separable from each other, I adopt this classification because it is useful for advancing discussion. The process of knowledge production consists largely of the steps: problem determination, hypothesis generation, and hypothesis verification. And in this process, the ability to read and write documents and analyze data are required as necessary skills. Below, I will examine each of these in more detail. 


% This structuring is tentative and there may be a better way to structure the research process. However, I have created this structure for practical purposes in order to move the discussion forward. I hope the structure of this article be a starting point for conceiving a better structurization. I believe that structuring and deepening understanding of the elements that are essentially important for knowledge production is extremely crucial when aiming for the automation of research.

% Though I explained that research is belied revision, it would be convenient to see as a function that takes the unknown as input and outputs the known.  % TODO: rearrange

\subsection{Question Formulation}
Research is an endeavor to bring the unknown closer to the known. Therefore, it is necessary to first determine what unknown we aim to make known. And this unknown often takes the form of questions. For example, ``Why do deep neural networks with a large number of parameters generalize well?'', ``How can we prevent the problem of vanishing gradients?'', and like these. These are commonly referred to as \textit{research questions} or \textit{research problems}. Therefore, in this paper, I will refer to the step of determining this unknown as \textit{question formulation}.

\textcolor{red}{TODO: should describe question construction itself first. What is research question or research problem?}

\subsubsection{Unknownness}
As I have reiterated, it is a necessary condition for research that the answer to a question is unknown, or in other words, that there is a high degree of uncertainty. Therefore, it is essential in research to have methods that ensure the answer to a posed question is truly unknown, or to formulate questions that truly have unknown answers.

Knowledge for humanity is primarily disseminated through research outcomes. Therefore, when examining all the research outcomes that have been generated thus far and finding that none of them provide an answer to a specific question, it seems reasonable to conclude that the question possesses sufficient uncertainty to warrant further investigation as a research endeavor. In particular, humanity has developed the culture to preserve the research outcomes in the form of papers. Therefore, it seems feasible to assess the unknown nature of an inquiry by examining all academic papers. However, it is impossible to review them all due to constraints in terms of time, technology, and cognitive limitations. Therefore, it is realistic to consider a question as unknown if it has been sufficiently and comprehensively explored through an extensive examination of these academic papers. In practice, we conduct literature reviews to synthesize existing research, identify research gaps in existing studies, and thereby ascertain the unknowness of our own questions or construct question for which the answers are unknown \cite{schryen2015theory}. \textcolor{red}{TODO: Consider where I will explain about literature review}

% In the previous statement, it was mentioned that as long as the unknown is truly unknown and it can be approached towards becoming known, there should be no problem. The process of approaching the known will be explained in the next section, and here I will delve a bit more into the determination of unknownness. 

However, in reality, such rigorous literature research is not always conducted in every case. Currently, researchers often demonstrate the unknownness of the answer to the question by referencing only a few related works and explaining that none of them have yet resolved the unknown. And when the paper is evaluated by reviewers, who are a small group of experts, if it is determined that the question has indeed not been answered so far, the provisional recognition of the unknown nature of the question is granted. This means that a subjective evaluation criterion is being used, where researchers and a small number of reviewers consider a question as unknown when none of their known studies provide an answer.

% This implies the use of subjective evaluation criteria, where researchers examine several papers considered ``major literature'' in a field and consider them as unknown if none of them have provided an answer. Furthermore, as mentioned later, we evaluate the quality of research outcomes by having them assessed by a small number of experts in the same field. If these researchers determine that the previous studies have been sufficiently comprehensive, the determination of unknownness is considered somewhat valid. In other words, ultimately, the evaluation by a few experts may serve as the basis for establishing the unknownness.

This current convention stems from the cognitive constraint that there is a limit to the literature that humans can examine. Since unknownness is a fundamental aspect of research, ideally, it should be evaluated objectively and rigorously. For instance, it would be desirable to quantitatively state which journals, what types of papers, and how many have been examined, and the result indicating their unknownness. Although systematic reviews already employ such approaches, there is, of course, a limit to the number of papers that can be evaluated manually and selection biases cannot be removed. \textcolor{red}{TODO: Add the explanation of systematic review, problems of it, and how AI can mitigate them.}

\subsubsection{\textcolor{red}{Good Question}}

The idea that formulating ``good'' questions is critical is widely accepted in the research community. I have not found a unified consensus on the definition of good research, but there have been various discussions on the elements that good research possesses. For example, \cite{hulley2007designing} proposed that good research question should satisfies FINER criteria (feasible, interesting, novel, ethical, and relevant) and \cite{alon2009choose} claims that a good problem is one that is most feasible and interesting to oneself. \textcolor{red}{TODO: Add more discussion on ``good'' questions, examples, discussion, what is good, what is important, specific question is good, etc.}

% This means that we distinguish between questions that are ``good'' and those that are not, based on certain criteria. 

% This means that we distinguish between questions that are ``important'' and those that are not, based on certain criteria. For example, \cite{alon2009choose} claims that a good question is one that solves challenges facing the research community. Likewise, we consider a question to be important if it generates knowledge that greatly contributes to a certain purpose. Valuing the degree of contribution to a purpose also implies viewing research as a form of problem-solving. \textcolor{red}{TODO: Add explanation of what this sentence means}

Thus, in realizing an agent that autonomously constructs questions, it may become important to consider how to automatically determine ``goodness'' of the questions. To achieve this, it would be important to first understand in more detail what kind of questions we consider ``good''. \textcolor{red}{TODO: Add possible directions}

% It seems that we still lack an understanding of what kind of research questions are important, but in the field called \textit{science of science} \cite{wang2021}, which studies the research itself, scientific studies on research impact are also advancing. The insights gained here may provide important knowledge in building new algorithms and optimization metrics.

\subsubsection{Relativity of Value}

So far, I have explained that we aim to construct ``good'' questions. This suggests that we conduct a value judgment of questions under a certain criteria of goodness. However, in light of the definition of knowledge production, value judgement is not necessarily required for knowledge production to be as it is. From the perspective of knowledge production, as long as the unknown is truly unknown and it be rigorously approached towards becoming known, there should be no problem. The unknown can be anything arbitrary, and knowledge production itself does not demand a specific nature for it. The value of knowledge is inherently dependent on context, so the significance or goodness of a certain knowledge is not determined a priori from the moment it is generated. Goodness becomes an issue only when that knowledge is intended to be generated within a society, implicitly assuming that some member of the society will use it in some form. In other words, the demand for importance is a constraint imposed by society not by knowledge production.

What I want to argue here is not that we should ignore the value because it is not a necessary condition for knowledge production. In the first place, it is inherently impossible for all actions to be value-neutral. Moreover, the realm of possible unknowns is too vast, so without any constraints, only nonsensical questions would arise. What I want to assert is that value is a relative concept to society and is determined externally. This is the choice of us on what objective we would like to maximize by knowledge production.


This provides important implications when attempting to create an artificial intelligence that autonomously conducts research, at least in two aspects. Firstly, it suggests the possibility of adopting values that are different from the ones we currently employ, thereby expanding the potential for research. I have explained that we make judgments about certain questions being good or important based on some criteria or standards. However, there may be questions that are not considered ``important'' according to current criteria but actually be extremely significant. As mentioned earlier, the value of knowledge is determined by its usage and context, and it can vary over time and in different environments. Therefore, it is highly challenging to determine the importance of knowledge during the stage of knowledge production. Even in the same environment, it is difficult to assess the significance of knowledge. This is because knowledge results from complex accumulations, leading to new insights, and there is a intricate chain of connections before a particular knowledge becomes recognized as important within a society. In addition, we are bound by various cognitive limitations inherent to being human. Therefore, we can only assess the importance of knowledge within the confines of these limitations. Given these circumstances, it is highly possible that the current adopted criteria for value judgments are missing out on the production of potentially important knowledge. The development of knowledge production systems that embrace new value judgment criteria can be expected to increase the potential for generating such knowledge by expanding the scope of exploration. If an artificially intelligent system capable of autonomous research is developed, it can be expected that research based on these new criteria will become more feasible. This could potentially enable the resolution of many previously unsolved problems that were not attainable before.

Kitano referred to the science in which humans adopt their own value judgment criteria to determine questions and hypotheses as \textit{value-driven science} \cite{kitano2021nobel}. He argued that advancing \textit{exploration-driven science}, which focuses on more comprehensive and thorough exploration rather than criteria based on specific values, is important for societal development. As mentioned earlier, I believe that a completely value-neutral system is impossible. However, I agree with the idea that employing new and diverse criteria would matter for future research. By adopting more diverse and extensive criteria than the value judgment criteria humans have used thus far, we could expand the exploration space of knowledge. The realization of research through artificial intelligence is expected to open up possibilities for such a future.

% Science based on the importance of questions discussed above is a \textit{value-driven science} \cite{kitano2021nobel}. However, as previously mentioned, these may be due to cognitive constraints imposed by human society, including the inability to handle knowledge that is deemed ``unimportant.'' Therefore, when automating question generation, it may be possible to explore a wide range of questions, including those that were previously considered ``unimportant.'' In doing so, it is possible that knowledge that was not considered ``important'' according to previous criteria could actually be extremely significant. This is referred to as \textit{exploration-driven science} \cite{kitano2021nobel}, and it could become a new form of research liberated from constraints imposed by humans. 

% Indeed, it is impossible to create a completely value-neutral system. All agent systems must have some form of bias. However, what I would like to emphasize is the potential to incorporate biases different from the criteria previously used by humans, and how this can enable us to consider more diverse approaches to research. This highlights the importance of creating agents capable of autonomously conducting research.

The second implication when attempting to create an artificial researcher is that if we do not intentionally impose any constraints, there is a possibility that the intelligence produced may not align with the knowledge we desire. This is an important point and thus will be discussed in a separate chapter.

\subsubsection{Question Construction and Autonomy}

Questions do not arise from nowhere. There is always something before reaching a question. In the example I mentioned earlier, for instance, the question may arise as a result of a literature review. The, why did you even conduct that literature review? It could be because there is a significant research theme you want to know about in that field. And then why are you interested in that research theme? It could be because the topic of the first paper you encountered during graduate school was fascinating, or it could be because you have been interested in it since childhood. And there may be causes behind those as well.

In this way, identifying where a question begins is a hard problem. If you think seriously about it, it will lead to an infinite regress. It may not be limited to the generation of questions alone when we say that thinking seriously leads to an infinite regress, but I specifically pointed out this aspect regarding question generation because it is the initial stage of knowledge production in the formulation in this paper. And it becomes a significant problem when we want to realize an autonomous artificial researcher. Can we say that question formulation is autonomous if the literature to read were given? Can we say that question formulation is autonomous if research theme were given? Naturally, it is impossible to create a system that autonomously produces knowledge without any constraints, so some form of inductive bias is necessary. However, it is a non-trivial question to determine at which stage of question generation we should allow autonomy and what kind of inductive bias is needed to do so. 

This is an important question, but it is not constructive to come to a standstill simply because we cannot answer it. To begin with what we can do, let's take a look at what has been said about the generation of questions specifically.

\subsubsection{\textcolor{red}{How to Practically Construct a Question}}

There has been much discussion on how to actually generate questions. Off course, these discussions primarily focus on how to formulate good questions. Therefore, please note that the examples mentioned here are proposals for generating such kind of questions.

One typical approach to formulating a research question is to conduct a literature survey, identify research gaps in existing studies, and propose a question that aims to fill those gaps.

\textcolor{red}{TODO: Add survey of how to construct questions; gap spotting, problemization, etc}

Next, let's consider the process of constructing purpose-driven research questions. When aiming to conduct impactful research, I believe that constructing purpose-driven research questions is crucial. In this approach, we first set the ultimate objective. Then, we identify the most critical bottlenecks, or sub-goals, that are essential to achieving that objective. We once again consider sub-goals for these identified bottlenecks. This process is repeated, converging on more specific and feasible sub-goals that are of high importance. Finally, we frame the question to address these sub-goals as the research question. 

In practice, we seem to determine the questions we should tackle in this way, implicitly and explicitly. For example, let's say that someone try to answer a question of ``How neural networks have reasoning capability?'' in his/her study. This question may come from a thought process of ``I want to create artificial general intelligence, which requires systematic thinking, that needs ...'' In this case, the final purpose is to achieve ``artificial general intelligence'', and the question addressed as a result is ``ow neural networks have reasoning capability?'' In other words, when we want to conduct important research, we follow a process that starts with the goal we want to achieve, considers the tree of important unknowns that should be clarified for its achievement, and sets the end of that tree as the research question. This process is summarized in Fig. \ref{fig:unknown_tree}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{figs/unknown_tree.jpeg}
    \caption{Caption}
    \label{fig:unknown_tree}
\end{figure}

% Of course, the purpose mentioned here may be a sub-goal of a higher-level goal. For example, the goal of ``creating general artificial intelligence'' may be a sub-goal of a more fundamental goal of ``satisfying intellectual curiosity,'' and the goal of ``satisfying intellectual curiosity'' may be biologically demanded for better exploration of the environment. These can lead to an infinite regression when considered strictly, so I won't delve into it any further here, but it could become an important issue when considering how to realize fully autonomous agent to construct questions.

\subsection{Hypothesis Generation}
We began by explaining that we first formulate the unknown we are addressing in the form of a question. The process of finding answers to this question is research. Here, because the answer to this question is, of course, unknown, it requires inference as to what the answer could be. As a result of this inference, a plausible answer is formulated. This process corresponds to the \textit{hypothesis generation}. Hypothesis generation is the inference about the unknown and the definition of research is to transform unknown to known. Thus, every research including deductive research must entail hypothesis generation implicitly or explicitly. In this sense, hypothesis generation is a core of research.

A hypothesis is the very object of beliefs that can become knowledge in response to a question. If a hypothesis withstands proper testing, the belief in its plausibility strengthens. Conversely, if a hypothesis does not withstand testing, the belief that it is not plausible strengthens. Therefore, the former generates knowledge that the answer to question A is hypothesis B, while the latter generates knowledge that the answer to question A is not hypothesis B. Of course, the ability to obtain such conclusions depends on the verification method, which will be explained in the following section.

Within a single process of knowledge production, they serve the role of being subject to verification and are mere beliefs if not tested or refuted. However, when considering the entire ecosystem of knowledge production, hypotheses play an extremely important role. This is because research is the act of generating new knowledge based on past knowledge and hypotheses are potential knowledge, justifying them indirectly influences future knowledge production.

% If it were not necessary to make inferences about the unknown, it would probably be because the question was not unknown in the first place. Thus that is not research by definition. It may not be explicitly stated, but as long as it is research, it implicitly involves some form of hypothesis generation. Therefore, the generation of a hypothesis, or the inference about the unknown, is essential for research in nature. Let's say, mathematics, a pure deductive field of research. In searching for lemmas to prove a theorem, we sometimes make predictions such as ``this lemma might be useful'' and examine it with few specific examples to check its usefulness. This may be considered as implicitly establishing a hypothesis and roughly verifying it. 

% In particular, science, by explicitly dividing the proposal of this plausible temporary answer and the corroboration of its tentative certainty into steps known as hypothesis generation and hypothesis verification, has enabled these processes to be carried out more systematically and rigorously. This is the well-known hypothesis-testing method, or scientific method. In this approach, a prediction about the unknown is explicitly stated as a hypothesis, and a procedure called verification is established to evaluate the validity of this hypothesis. Through this verification process, the evaluation of the hypothesis is conducted and the uncertainty towards the target unknown is reduced. This is the knowledge production based on the hypothesis testing method. 

% As a practice of human science, there are cases where discovery and justification are not strictly separate in actual scientific endeavors \cite{arabatzis2006inextricability}. However, functionally, discovery and justification can be classified separately, and it seems convenient to consider them as distinct in knowledge production that is not dependent on human conventions. Therefore, we will discuss them separately here.

% Although we said that hypothesis generation is the important part of scientific method, the hypothesis generation is not necessarily unique to the empirical science. It is a task that inevitably arises when dealing with the unknown. 

\subsubsection{Plausible Hypothesis and Unknownness}

While predictions that are highly unlikely to be confirmed, such as random guesses, can still qualify as research, they do not contribute much knowledge because they are expected to be easily rejected without undergoing rigorous testing. It becomes a waste of resources to invest in validating such predictions. Therefore, it seems to be crucial for ``meaningful'' research to propose hypotheses that are somewhat plausible.

It is immediately evident that this is a non-trivial problem. This is because research, despite being an endeavor to answer unanswered questions, requires considering plausible candidate answers for those questions. Here, if the unknown under investigation is entirely unrelated to existing knowledge, it is impossible to make meaningful predictions about it. This is because predictions are based on experiences and data. In other words, high novelty and high uncertainty indicate a complex structure that cannot be immediately predicted from past knowledge and experiences. It implies that constructing plausible hypotheses necessitates the ability to discern these complex structures and patterns from past experiences. While it is difficult to determine what is unknown and what constitutes a complex structure, these can be crucial points to consider when advancing the automation of research.

% \subsubsection{Note on Unknown}
% Let me make a note here about the term unknown again. I wrote that if an answer can be immediately derived by inference, it was not unknown in the first place. However, research is an activity to transform the unknown into the known, so in some form, unknown should eventually approach the known (reducing uncertainty). Moreover, if there is something that seems to be completely unrelated to the known or experiences (though it is hard to imagine), it seems inherently impossible to bring it closer to the known. In that sense, it is reasonable to think that the unknown has some relationship with the known. Therefore, the above description might be more accurate if it refers to cases where the confidence in the answer is lower, or the path to the answer is complex, and uncertainty is high. This is similar to the discussion on the definition of unknown in the previous section.

\subsubsection{\textcolor{red}{Existing Discussion on Hypothesis Generation}}
\textcolor{red}{TODO: add studies}

%％％％％％

Since 19th centuries, the act of producing knowledge, in particular hypothesis, and that of verification of it have been distinguished as the context of discovery and context of justification \cite{sep-scientific-discovery}. And during the most of the 20th centuries, the discovery caught much less attention by philosophers of science. 

In engineering discussions, there is often explicit formulation of sets or candidates of hypotheses, and the discovery of hypotheses in such situations is often discussed \cite{simon1973does,kitano2021nobel,bengio2022ml4sci}. However, when automating hypothesis generation it is also important to consider how such candidates arise in the first place. There have been attempts to understand this process, such as highlighting the importance of analogy \cite{thagard1984conceptual} or mental model \cite{nersessian1999model}, but the question of how to generate ``good'' hypotheses still remains unanswered. The generation of initial hypothesis candidates has been discussed in the context of creativity in science. As you are well aware, however, current machine learning models are already capable of executing creative tasks effectively \cite{sep-creativity}. Thus, there is a debate about how much emphasis should be placed on creativity when considering the development of artificial intelligence capable of generating new hypotheses.

% \subsubsection{Good Hypothesis}

\subsubsection{Machine Prediction and Hypothesis}
I have explained that hypothesis generation is inference about the unknown, in other words, it is a prediction. Thus, in terms of its functionality to knowledge production, it is sufficient if a agent can do prediction. As you all know, machine learning models are functions to do predictions about unknown data. In this sense, prediction by machine learning can be equated with the generation of hypotheses. Therefore, when considering the automation of research, hypothesis generation can be dealt with as just a prediction task in a broad sense. As emphasized above, what I aim for is not merely imitating human knowledge production, but rather the autonomous practice of knowledge production that is not bound by human conventions. From that perspective, the act of generating hypotheses may be assimilated into a larger category rather than simply being labeled as prediction.

The issue that becomes important here is the problem of representing knowledge and hypotheses. As mentioned earlier, research is the process of generating knowledge for a society constructed by certain agents. Therefore, it is necessary for the produced knowledge to be interpretable, or at least usable, by at least some members of the society, even if not by all of them. In human society, it seems that knowledge is made possible by expressing it in a form understandable to the members of the society, such as natural language or mathematical language.

I stated that hypotheses are simply predictions. However, even if their validity is verified in a manner that other members find acceptable, if the content of those predictions cannot be interpreted, it seems that the predictions generated there cannot be called "knowledge of that society." This is because common beliefs are not formed. It may not necessarily be the method currently adopted by human society, but it seems necessary to adopt a form of representation that all members can interpret the content in some way.

% As emphasized above, what I aim for is not merely imitating human knowledge production, but rather the autonomous practice of knowledge production that is not bound by human conventions. And in terms of its functionality, hypothesis generation is identical to inference towards the unknown. In that sense, while understanding how humans generate hypotheses can be very useful in considering how to achieve effective hypothesis generation, I do not consider it is always necessary.

% Of course, hypothesis may have additional constraints on top of the prediction. For instance, there might be an implicit assumption that a hypothesis is something described in natural language. However, we think this is only due to the conventions of current human society. What is functionally important for the purpose of knowledge production is to provide a tentative answer to the unknown, regardless of its form or property. In this sense, in this paper, we take the position that hypothesis generation is the prediction.

\textcolor{red}{This (commented out sentences) is about hypothesis generation automation so will be moved to survey section of perspective section}
% Indeed, much of the current discourse on hypothesis generation focuses on specific research domains or discusses the exploration of hypotheses based on hypotheses that humans have explicitly defined. I understand this situation because hypothesis is an answer to a tentative question and hence it varies depending on the question. For example, if a study aims to enhance our understanding of a particular phenomenon, then the description or mechanism of that phenomenon would become the hypothesis. Similarly, if the objective is to solve a particular problem, then the solution to that problem would be considered the hypothesis. However, it is essential to consider how we can realize agents that construct candidates rather than just search for them, regardless of the specific research domain. This is a crucial question to address when contemplating artificial researchers.

\subsubsection{Prediction, Explanation, and Understanding}
Humans, in order to derive hypotheses in response to questions, often require thinking and deliberation on why those hypotheses are considered plausible. Without such cognitive processes, it is difficult to generate hypotheses in many cases. This is not merely a cognitive constraint; it has played a significant role in the advancement of research. The explanation of how a hypothesis is derived and the reasoning behind its plausibility have provided crucial additional information for understanding the subject of study. This has assisted people in gaining a better understanding of the phenomenon.

As mentioned earlier, predictions do not necessarily require such processes. Even without explanations for how such predictions arise, one can make inferences, and if they are rigorously tested, they can be considered knowledge. However, knowledge lacking such important additional information is expected to contribute less to the understanding of other knowledge and the comprehension of the entities targeted by the knowledge system. This research and understanding problem becomes significant even when artificial intelligence engages in research.

\textcolor{red}{TODO: Add more}

% \subsubsection{Others}

% In engineering research, as part of new knowledge, it is often required to propose actual design plans or algorithms. This can be considered as having a similar function to a hypothesis in the sense that it is a proposal for addressing a problem and is evaluated in some way.

% In mathematical research, proving a theorem that was previously unknown is the production of knowledge. However, since mathematics is a deductive system, if the proof is correctly executed, it can be said to be ``correct'' in that sense. In other words, the proof itself is both the proposal and the verification. Therefore, mathematics is not a type of work that separates hypothesis and verification.



\subsection{Hypothesis Verification}

\textcolor{red}{TODO: add detailed explanation}

I consider verification to be a particularly crucial process in the generation of knowledge. Questions and hypotheses can be generated in arbitrary ways, and they can take any form. However, verification must not be taken like this. Verification must be done in a manner that can convincingly update the shared beliefs of other members of society. Therefore, it is the most demanding process in the research journey in terms of rigor and caution. I believe that the rigor of this process is what truly sets research apart from other activities. The rigor and systematicity of the verification process is considered the very reason why science or research is referred to as rigorous in many cases \cite{sep-scientific-method,hoyningen2008systematicity,haack2003defending}.

I also recognize that the verification process is the most flexible and variable stage, as it can significantly differ depending on the subject of research. For instance, if one wishes to study the behavior of rat, they may need to train the rat, whereas investigating elementary particles may require the use of elementary particle accelerators. In the field of history, the existence of historical records might serve as evidence, while in mathematics, the verification process revolves around the proofs themselves. Due to this high degree of flexibility, I think that this stage becomes the most challenging aspect in automating research processes.

\subsubsection{How to Justify Beliefs}
In many empirical sciences, statistical methods occupy a privileged position as the primary means of verifying hypotheses. Therefore, it seems important to consider in what sense we can say that a statistical method can validate hypotheses, while I admit that the methods of verification vary greatly depending on the type of question or hypothesis and it cannot be said that there is any universal method for verification. The use of inferential statistics methods is widely accepted, but it is a challenging issue to known how various statistical techniques, each with their own approach, can provide evidence to substantiate a hypothesis based on the data in the real world \cite{otsuka2022thinking,sober2008evidence,sep-statistics}. Thus, what constitutes the verification of a hypothesis, or in other words, the justification of a belief, is a very difficult debate in which it is likely challenging to arrive at a unified answer. Please note that the discussions mentioned earlier, such as the uniformity of nature, pertain to the validity of induction itself. In statistical methods, however, induction is assumed and the discussion begins with formulating this uniformity by assuming a probability model. 


This discussion of statistical method provides important insights for the pursuit of developing AI capable of conducting autonomous research. As can be seen from the assumption of being rooted in machine learning, the validity of inductive inference itself is generally accepted and not a practical concern in creating such AI. However, how to make agents acquire the methods of justification is a important issue. If the criterion for justifying beliefs is to be acquired completely autonomously from scratch, it may very well lead to criteria that are meaningless for humans. Additionally, it seems that the criteria for justification employed by humans are diverse. When properly considering the reasons that these criteria are believed to provide justification, we can recognize that there are highly intricate structures involved. In light of such considerations, the extent to which one can understand and acquire criteria for justification from the criteria humans already employ is a nontrivial problem. Even empirical sciences, which employ highly universal verification methods based on statistical approaches, face such difficulties. Therefore, aiming for autonomous acquisition of verification methods in broader fields of research would likely pose even greater challenges. These issues will require further in-depth discussions in the future.

\subsubsection{Uncertainty of Verification}
Verification is a highly challenging process when examined closely. Firstly, as mentioned earlier, inductive approaches cannot verify hypotheses in the same way as deductive reasoning. Also, I notice that hypothetico-deductive method, which involves verifying claims derived from a hypothesis to confirm its validity, is still widely used today. However, verifying a deduced claim does not support the hypothesis because there may be many hypothesis that can result in the same deduced claim. In response to these, Karl Popper proposed that while hypotheses cannot be confirmed, they can be falsified \cite{sep-scientific-method}. However, in practice, the verification of hypotheses involves implicitly relying on numerous auxiliary hypotheses. When using experimental apparatus, it requires many assumptions to trust them. Even when an experiment fails, determining whether the hypothesis was incorrect or the experiment itself was flawed is not as straightforward as one might think. Thus, there is inherent uncertainty in attributing the results of verification to a specific cause \cite{chalmers2013thing,sep-physics-experiment,sep-scientific-underdetermination}. Furthermore, all reasoning and observational evidence are inevitably influenced by some form of theories, individuals, or societal factors \cite{sep-science-theory-observation}. Therefore, it is necessary to carefully examine them to ensure that they are not distorted by unintended influences. \textcolor{red}{TODO: add more}

I do not intend to claim that these uncertainties undermine the reliability of research. Such uncertainties exist in almost all human endeavors. Research, among these activities, strives to confront these uncertainties with great care and rigor. Above all, the fact that the results generated through research have effectively supported our lives demonstrates their efficacy.

I mentioned the difficulties inherent in verification to highlight their significance, particularly when considering autonomous artificial intelligence capable of self-verification. Ideally, autonomous agents are expected to establish their own methods and criteria for verification. However, if verification inherently carries such difficulties and uncertainties, the more rigorously we consider it, the more paralyzed the agent becomes, as it seemingly cannot accomplish anything. \textcolor{red}{TODO: Add explanation}

% A characteristics of research can be found in the systematicity, rigor, and objectivity of research practice \cite{sep-scientific-method,hoyningen2008systematicity,haack2003defending}. 

% In particular, I believe that a characteristic of research lies not in the way of determining questions or generating hypotheses, but in the fact that the verification of hypotheses is done in an extremely rigorous and careful manner. 

% It could be said that I'm taking a view similar to the new experimentalism, placing emphasis on verification in research, or experiments \cite{chalmers2013thing,mayo1996error}.

% Of course, generating a hypothesis is not a simple task. What we want to say is that, as long as any method of hypothesis generation is properly verified, it is considered research, and no matter how properly a hypothesis is proposed, if the verification is sloppy, it is not considered research. This means that verification may be at the heart of knowledge production. In other words, in order to create artificial intelligence that produces knowledge, it is important to consider how to create an intelligence that can perform verification.

\subsubsection{Towards Autonomous Verification}
\textcolor{red}{TODO: May be moved to perspective chapter?}

As mentioned above, I believe that the process of verification is highly diverse. Therefore, achieving an end-to-end approach immediately is difficult, and I think it requires even stronger constraints, at least in the short term, compared to other processes. For a practical first step for the verification process automation, I propose tentatively dividing the verification process into three stages: \textit{verification design}, \textit{verification instantiation}, and \textit{verification execution}. Then, the verification execution can be broadly categorized into the processes of data generation and data analysis, while this categorization may apply for only empirical sciences. \textcolor{red}{TODO: Add Fig}

\textbf{Verification Design.} At this stage, we contemplate how to validate the hypotheses and proceed to formulate a verification plan. In a verification plan, the agent writes about the hypotheses, verification criteria (what will be considered as evidence for verification), and the procedures for conducting the verification. Going beyond a high-level description, it may be beneficial to write these plans in as much detail and procedural clarity as possible. The Fig. provides an example in the context of machine learning. The idea is to create a blueprint for a pipeline where verification is automatically executed by faithfully following the plan. I consider this important because the verification process offers a high degree of flexibility, making it extremely challenging to achieve an end-to-end automation, at least in the short term. \textcolor{red}{TODO: Add Fig}

Furthermore, for artificial intelligence to perform autonomous verification, it seems essential that the AI not only adopts certain verification criteria but also be able to explain the meaning behind them and how they contribute to the verification process. Even humans may sometimes engage in this without conscious awareness, such as conducting hypothesis testing because ``everyone does it.'' In that sense, this requirement may be somewhat challenging. However, if the aim is to truly enable AI to autonomously conduct research, it appears crucial for the AI to have a proper understanding of what constitutes verification and be able to design it itself or, at the very least, explain it adequately.

\textbf{Verification Instantiation.} At this stage, the research plan that has been developed is translated into an executable instance. For example, if the research plan states, ``Train the model B with the dataset A...'' the necessary steps would involve acquiring dataset A from the appropriate source, formatting it to be compatible with the model's input requirements, and preparing the data for training, etc. Similarly, if the plan states, ``When a rat presses the switch B, food is dispensed...'' the agent has to prepare rats, food, and create a machine that dispenses food upon pressing the switch, and so on. This process involves translating the plan, expressed in language, into physical instance in the real world.

As evident upon reading, this process is highly challenging to automate. Even research confined to the realm of computers, such as research of computer science, requires accomplishing a vast number of complex tasks. For research that necessitates physical realization in the real world, the development of robotics and embodied agents are necessary. Regarding the question of where and how to tackle these problems, I will provide my perspective in the later chapter. However, it is important to note that unless the research is constrained by questions and hypotheses, aiming for true automation will inevitably require overcoming these challenges, even if it takes a long-term approach.


% Once a hypothesis has been established, a verification plan is created to determine how to verify it. The specific method of verification depends on the subject being investigated, making this aspect of research difficult to structurize and automate in a unified way.

% However, in many empirical sciences, the likelihood of a hypothesis is evaluated based on statistical significance. This is done by \textit{hypothesis testing} in practice. As this is a hypothesis test, it can only reject the null hypothesis, rather than directly determining the correctness of the hypothesis. Therefore, it can only be said that the hypothesis has survived for the time being. The belief that the surviving hypothesis is more likely to be valid is the basis for decision-making.

% In any case, humans seem to use statistics or probability as the basis for assessing the validity of a hypothesis. In other words, we seem to concede to consider a hypothesis as plausible if something that cannot happen by chance, such as observing the same number repeatedly. This is based on the assumption of the ``principle of confirmation,`` which assumes that if the number of observations increases, it can be considered more reliable, and the ``principle of uniformity,`` which assumes that things will continue to proceed as they have been if the conditions remain the same. These beliefs ultimately serve as the basis for verification and scientific knowledge production. 

% I will not delve into the validity of these beliefs here. What matters is that our research activity follow a practice that ``when a hypothesis is present, and a certain criterion and procedure are prepared, and the hypothesis is considered valid according to that procedure, we consider it valid.''

% In theoretical research, sometimes there is no verification plan. Theory is a hypothesis, and its validity is determined separately through verification (not in the sense of whether it is mathematically valid, but for example whether it explains physical phenomena or not). However, in complex modern science, theorists propose a theory, and experimentalists verify it.

% Therefore, it is understood that in current research practice, shared knowledge in the form of papers may not necessarily provide a complete answer to a given question. This is similar to research on negative data. Negative data cannot solve the unknown initially declared, but it can reduce a certain degree of uncertainty towards it. This is because the validity of the presented hypothesis may have decreased somewhat. If this is the case, each research shared in the form of a paper may be more appropriate to describe as "reducing uncertainty towards the unknown," rather than "making the unknown known." This can become complicated when scrutinized strictly, so let's put this aside for now and continue to discuss how "producing new knowledge" is research.

% In reality, conducting research is expected to be done with limited resources (time, funding, computing resources, people, etc.). Therefore, it is necessary to consider these resources when determining the verification approach. After a research design is determined at an abstract level, the feasibility of the research plan is roughly evaluated through a simple problem setting. This is known as a pilot study.



\textbf{Verification Execution.} Finally, the instantiated research plan is executed according to the prescribed procedures. Typically,``experiments'' refers to the data generation process and subsequent analysis and interpretation are conducted separately. However, since we are discussing a verification plan in this context, all of them are in the same single plan. Therefore, please note that what emerges from executing the verification plan is not data but the verification results.

In many empirical studies, the data generated from experiments is often used not only for the validation of hypotheses but also for generating new hypotheses or giving some insights. However, as hypothesis generation and hypothesis validation play different roles in knowledge production, we do not assume any uses of the generated data beyond verification in this context. Of course, please note that this does not imply that such actions are prohibited in practice. Data analysis will be discussed in a separate section.

% As mentioned earlier, in the case of empirical sciences, testing is often performed. Therefore, data is first generated, processed, and finally verified using the processed data. If we summarize the process of generating and processing raw data as data generation, this process can be broadly divided into data generation and judgment based on verification criteria. It may be rather said that the act of research itself is a process of repeatedly generating data and performing some kind of processing on it.

% I separated the verification plan from the verification because I want to separate the description and execution of the process. The verification plan is analogous to coding, while the verification is more similar to executing the code.

% The output of this process is usually wrtitten in the result section in the paper.

\subsubsection{Verification and Alignment}

As I have explained repeatedly, research is the act of updating beliefs by reducing them to a common conviction that satisfies most members of society. Verification is precisely the act that carries out this belief update. Therefore, when considering knowledge production by agents that are not limited to humans, it is an important issue whether to completely rely on the criteria autonomously generated by the agent's society, including the act of verification. This is because what may lead to belief updates for a certain artificial intelligence group may not be the same for humans. In such a case, the knowledge produced there would be completely detached from humans. Therefore, when we want non-human intelligence to produce knowledge that is meaningful to humans, at least the criteria of verification should include elements that humans can agree with. I will discuss this issue again in a later section.

Furthermore, as mentioned earlier, the reason why verification, even as the update of collective beliefs, has brought about such progress and benefits to the human society may be because the subject of research is (in a broad sense) nature and because human beliefs themselves are inherently constrained by nature. When I say that human beliefs are constrained by nature, as explained in the section on induction, it means that what we strongly feel to be certain is a reaction to what we have been exposed to through the process of evolution and development as organisms, interacting with nature.

Considering this, when a fully autonomous artificial researcher is created, I think it is far from obvious whether they would autonomously conduct meaningful research on nature or be able to share knowledge with us humans without possessing at least these naturally constrained intuitions, sensory organs, and basis of beliefs. Therefore, when creating autonomous artificial researchers, it may be necessary to provide them with such nature-rooted beliefs in the form of some inductive bias.

\subsubsection{Continuity between Hypothesis Generation and Verification}

\subsubsection{\textcolor{red}{Add more discussion!! Everything I want to say!!!}}

\subsection{Peer Review}
In current research, a small group of experts review papers and the results that pass through their review are published. This process is called \textit{peer review}. Peer review is now considered an indispensable practice in research across a wide range of fields. However, it is said that peer review became commonplace like tody only from the 1970s onwards \cite{baldwin2018scientific}.

Peer review, particularly pre-publication peer review, is commonly regarded as playing the role of a gatekeeper of knowledge, determining what becomes knowledge and what does not. It enables the production of high-quality knowledge by assessing the quality before publication. Here, let's delve a bit deeper and examine the specific role that peer review plays in knowledge production.

I believe that peer review serves two distinct roles in knowledge production. The first role is evaluating the fulfillment of necessary conditions for the knowledge of a research proposal. This involves assessing aspects such as the validity of the methodologies used and the sufficiency of the content's novelty. This process could potentially be skipped if these evaluations were flawlessly conducted in every research endeavor during the knowledge production process. In that sense, it can be seen as functionally redundant in relation to knowledge production. However, redundancy does not imply futility. Firstly, humans naturally make mistakes and have blind spots that they may not be aware of, so multiple checks hold significant value. Additionally, research is fundamentally conducted on the assumption of goodwill, making it important to have a process to verify if any misconduct or ethical issues have occurred for the sake of a healthy knowledge production. Lastly, as emphasized repeatedly in this paper, verification is an act of updating collective beliefs. Therefore, the existence of a process to confirm whether the verified results are acknowledged by others seems essential in the context of knowledge production for that society. Considering these factors, I believe that the first point, which functionally exhibits redundancy, plays an important role in knowledge production.

The second role evaluates whether a proposed study has value to the research community and society at large. This involves judging factors such as the importance of the research, its clarity, and its ethical validity. This evaluation pertains to the submitted knowledge rather than the process of knowledge production itself. Rather, it can be seen as an anticipatory assessment of the value judgments that will be made when the knowledge is disseminated in society. In this sense, this can also be called a redundant process.

In summary, the first role can be interpreted as an extension of the knowledge production process, while the second role can be seen as a pre-emptive value judgment by society (Borrowing the words of Seetl, we can say that the former is the judgement of \textit{epistemic value} and the later is that of \textit{non-epistemic value} \cite{steel2010epistemic}). In other words, the act of peer review serves as a gate that connects the process leading to the production of knowledge with the subsequent processes. I summarized the review criteria of Nature and Neural Information Processing Systems in Table from the above perspectives. \textcolor{red}{TODO: Add table and a note on reproducibility in the table and footnote}

If peer review is considered redundant in knowledge production, then the necessity of the peer review process when conducting research with artificial intelligence, which may have fewer cognitive constraints than humans, becomes a topic of discussion. If there are no occurrences of human errors, it may not be necessary to have a redundant process. On the other hand, if consensus among multiple agents, as mentioned earlier, is crucial, then having a redundant process may still be necessary even if the entity involved is not human. Furthermore, even if a redundant evaluation is necessary, it may not necessarily need to take the form of pre-publication peer review. Pre-publication peer review is a custom that has emerged from human society's history, but there are also issues raised regarding the practice of peer review before publication \cite{heesen2021peer}. Instead, a system that continuously evaluates all research findings by a multitude of individuals during the process and after the production of results, continuously updating knowledge, may lead to more robust and high-quality knowledge production.

% The review evaluates papers from multiple different perspectives. For example, at NeurIPS 2022, papers are evaluated based on the criterias of originality, quality, clarity, and significance.

\subsection{Information Retrieval}
A simplified research process is a function that outputs knowledge when a given subject is inputted. However, in reality, to execute these individual processes, it is necessary to collect information from the world and use them as inputs. For example, to formulate a question with an unknown answer, one may need to search for papers. Similarly, to evaluate the performance of a proposed algorithm, one may need to acquire a dataset. In this way, research can be seen as an act of producing knowledge by taking in all the information from the world as inputs. Therefore, it is not an overstatement to say that information retrieval is an essential technique in knowledge production. 

Hope et al. have proposed a similar but more sophisticated depiction of this view in their perspective paper \cite{hope2022computational}. They regard the research as an interaction between a researcher’s inner cognitive world and the outer world and emphasize the importance of knowledge retrieval aligned with human cognitive world. Although the equivalent of the inner world in this paper is not the human cognitive world but the knowledge production system, the perspective of distinguishing between the external source of information and actual knowledge production mechanism shares similarities.

Information retrieval is a highly flexible topic as the methods vary depending on the research subject. Therefore, here I will first focus on explaining the technique that is relevant to research and necessary for any study, namely, literature survey. Subsequently, I will expand the discussion to machine learning models that involve computer operations, including browser manipulation, and explore the implications they have on research automation.

\subsubsection{\textcolor{red}{Survey}}
\textcolor{red}{TODO: Change}
Research is an endeavor to create knowledge based on existing studies. Therefore, the first step is to search for papers that should be read. In this article, we refer to this process as \textit{search}.

To find the necessary papers, you have to know what is written in each paper. Therefore, \textit{search} is closely related to \textit{reading}, which will be discussed in the next section. Here, For convenience, we will distinguish between the two: the former refers to finding the necessary papers from a large collection of papers, and the latter refers to extracting necessary information from the obtained single paper.

I shall make mention of the relationship between these concepts and the activity commonly referred to as a \textit{survey}. I define the survey as a series of processes of 1. searching for necessary papers, 2. extracting information from multiple papers, and 3. comparing them to make some kind of decision. Please note that comparison, searching, and reading are all closely related to each other in this context as well; for instance, proper comparison between papers is necessary for better searching.

The distinction between the aforementioned tasks of reading and searching, as well as the definition of survey, are based solely on the fact that we humans distinguish between them. However, if desired information could be directly obtained through natural language instructions from a large set of academic paper data, the tasks of searching, reading, and comparison would become an end-to-end process. Thanks to the remarkable development of large-scale language models in recent years, such a possibility has become a realistic one. Further details on this possibility will be discussed later.

\subsubsection{\textcolor{red}{ML Models that Operate Computers}}
\textcolor{red}{TODO: May not be here}

\subsection{\textcolor{red}{Literacy}}
\textcolor{red}{TODO: Reconsider what I want to say here}

It is not an exaggeration to say that language is one of the most significant features that sets humans apart from other animals. When we consider the impact brought about by recent language models, we can understand just how significant language is to us.

In human society, we express all sorts of information through language. Knowledge, too, is conveyed in the form of written documents, particularly academic papers. Therefore, in research, not only is the ability to acquire information crucial, but also the ability to handle language, or \textit{literacy}, is essential for communicating research findings in human society. The ability to handle language can be broadly categorized into \textit{reading} and \textit{writing} texts. In the context of research, specific abilities include being able to extract desired information from papers during reading and being able to generate ``good'' papers based on research outcomes during writing. Therefore, in this section, I would like to discuss what a research paper is and what constitutes a good research paper. Then, I will examine the implications it brings to research conducted with AI.

% \subsubsection{Reading}
% As previously mentioned, acquiring information from academic papers is a fundamental task necessary in all aspects of research.

% In particular, there may be cases where one does not even know where to find the necessary knowledge. Therefore, in order to obtain the required information, it is necessary to first search for the academic papers themselves where the information is stored. 
% Additionally, researchers sometimes have to compare multiple papers. Researchers need to demonstrate in the paper that the problem they are trying to solve is truly unknown, and that their proposal is truly novel.

% A survey combines all of these tasks. In other words, it is the process of information retrieval and extraction from multiple academic papers followed by decision-making.
\subsubsection{Characteristics of Research Paper}
An academic paper is a structured document that summarizes research procedures and findings. It is said to have originated around the 17th century but became more common in the 19th century. To read a research paper effectively or write a good research paper, it is crucial to first consider the role that a paper should fulfill. 

Above all, a research paper is an expression of knowledge. It serves as a kind of ``asset'' for humanity, where new knowledge is generated based on the knowledge. Therefore, it is expected to possess information related to knowledge production in as detailed and accurate a manner as possible.

Furthermore, since knowledge is intended to be used by third parties, a research paper always assumes the presence of a reader. Therefore, it is necessary for the paper to be easily understandable, which means it should have a low information acquisition cost. For instance, during peer review, clarity and delivery are evaluated, focusing on the value of the paper as a ``report'' of knowledge rather than the knowledge itself. One of the attempts to enhance comprehensibility is through the structure of the paper. The prevalence of structured papers as we know them today seems to have emerged in the 20th century \cite{harmon1989structure}. 
The structure of a research paper varies depending on the field, but in empirical scientific papers, a widely adopted format is known as IMRaD, which stands for Introduction, Methods, Results, and Discussion. Introduction, Methods, Results, and Discussion can be interpreted as having roles that express what questions were studied, how those questions were investigated, what discoveries were made, and what the significance of those discoveries is, respectively \cite{gastel2022write}.

Moreover, the social aspects can also influence the content of a research paper. For example, in the current academic community, emphasis is placed on publishing papers in top journals or getting papers accepted at top conferences. These factors are not only important for the researcher's reputation but also crucial for securing academic positions in a highly competitive job market. Due to these pressures, it is said that the motivation to present research findings attractively can lead to distortions in the content of the research or make it less comprehensible. In the sense of making the results more appealing, a research paper could be likened to a kind of ``artwork''.

\subsubsection{Implications for Autonomous Research}
Having provided an overview of academic papers, I would like to take this opportunity to express my opinion on the significance of literacy when conducting research with AI. First and foremost, it is important to note that literacy is merely a means for information acquisition and expression in the context of knowledge production. Therefore, effective reading of a research paper depends on the manner in which the paper is presented, and writing a good research paper relies on determining the most valuable expression of knowledge in relation to the specific use case. Note that the term ``valuable'' here refers to non-epistemic value rather than epistemic value.

In the previous section, we provided an overview of the various roles that research papers fulfill. Among them, we believe that the role of knowledge representation remains significant regardless of whether the producer of knowledge is human or machine, depending on the context. In other words, information such as research questions, hypotheses, experimental designs, and results will continue to be expressed. When considering machine readers, it is possible that there will be an increased demand for or capability to provide more detailed information or information closer to raw data.

Next, let's discuss clarity. Clarity is a relative concept that pertains to the reader. Traditionally, research papers have been designed for human researchers as the intended audience. However, if we shift the assumption to machine readers, the value of clarity in papers as perceived by humans is likely to undergo transformation. For instance, the IMRaD structure has historically evolved to make it more comprehensible for humans. However, whether this structure is optimal as an information source for machines may need to be reconsidered. Even without waiting for complete automation, there has been rapid development in language models using question-and-answer formats for extracting information from unstructured documents. Considering this, it may become more important to provide comprehensive and accurate information rather than devising structures that are simply easy to read.

\textcolor{red}{TODO: Conclusion}



% While academic papers are already structured into sections such as introduction, method, results, discussion, and conclusion, I believe that further sub-structuring of these sections could make it easier for readers to gather information. For example, the introduction section contains a broad range of elements, but breaking it down into more detailed subheadings could help readers more easily access the information they need.

% There are various techniques for writing academic papers, but they are all designed with the assumption that humans will be reading the paper. Papers are considered to be ``reports'' and are expected to provide information to readers at a low cost. Additionally, papers are usually peer-reviewed and published in academic journals, so it is necessary to write attractive and engaging papers that will be accepted by the best journals. In this sense, papers are also ``works of art.'' However, I believe that the essential nature of papers lies in their role as the foundation of knowledge production, making papers an asset in terms of their ``knowledge'' aspect.


\subsection{Data Analysis}
A considerable amount of research is empirical in nature, meaning that it involves inductive reasoning supported by some form of evidence rather than relying solely on complete deduction. Therefore, although the quantity, quality, and characteristics may vary, these studies all generate some kind of data. Consequently, the techniques for analyzing this data are recognized as crucial across a wide range of research domains, regardless of the specific field. \textcolor{red}{TODO: add discussion about data analysis itself}

Please note that when I talk about data analysis, we are not limiting it to any specific operation on the data. In other words, it could involve generating hypotheses from data, testing hypotheses, or conducting verification. Descriptive statistics, inferential statistics, and predictive statistics are all encompassed within data analysis.

I have extensively discussed inductive reasoning so far, and since statistical machine learning itself is data analysis, there may be no need to dedicate a separate chapter for its explanation. However, I chose to include this chapter because in the research process, discussions often separate data generation through experiments from the analysis of that data. In my understanding, both data generation through experiments and data analysis are two distinct tasks within the function of verification. By emphasizing that data analysis is a task, I believe it makes my position more understandable. Furthermore, you may raise doubts about the validity of categorizing the research process into a unidirectional ``research process'' by pointing out that in a single experiment, the same data can be used for both verification and generating new hypotheses, reflecting the trial-and-error nature of actual research. However, as I emphasized earlier, what is important is not the chronological sequence but the role that each process plays in knowledge generation. According to my organization, even if the same data is used, if it is used for verification, it is considered verification, and if it is used for generating the next hypothesis, it is considered hypothesis generation for the next study. Emphasizing that data analysis itself is a technique that does not have a meaning beyond manipulating the data helps convey my understanding that its relevance to knowledge production is relatively determined by how it is used in different knowledge production processes.

\subsection{Deductive System}
Up until now, I have primarily discussed inductive reasoning, but deductive systems such as logic and mathematics are indispensable and highly significant in research. Deduction is a logical reasoning method that derives conclusions from premises in such a way that the conclusion necessarily follows if the premises are accepted. It is said to guarantee truth-preservation because if the assumptions are true, the conclusion is guaranteed to be true. So far, it has been mentioned that it is ultimately impossible to prove the truth or falsehood of a hypothesis through empirical methods. In contrast, deduction is an extremely powerful method of verification because it can prove the truth or falsehood of a hypothesis. Furthermore, regardless of how counterintuitive the conclusion may be, if the premises are accepted and the inference rules are applied appropriately, we must accept the conclusion. In this sense, deduction allows us to think about the world free from human intuition and cognitive biases, making it a very powerful means of understanding the nature.

\subsubsection{Mathematics}
Mathematics has played an essential role in knowledge production, particularly in the field of natural sciences. There are various opinions on why mathematics is crucial in the realm of natural sciences. Firstly, as mentioned earlier, one reason is its deductive nature. As repeatedly stated, knowledge production involves accumulating evidences of belief in the face of the unknown. Conclusions derived through truth-preserving deduction immediately provide beliefs of the same strength as the premises, thus playing a vital role in unraveling the unknown. 

Secondly, mathematics is rigorous. While natural languages rely on context and often contain ambiguity, mathematics provides a precise and unambiguous means of representing and communicating knowledge. It has served as the language of science, playing a significant role in scientific discourse.

Lastly, mathematics is abstract. From the ancient time, even in the absence of formal deduction, mathematics dealt with the concepts such as numbers, which is greatly abstract concept of great interest to humans \cite{david2010history}. Through the introduction of symbolic representation and manipulation, mathematics has been further enhanced in its abstract nature. Moreover, mathematics not only abstracts reality but also engages in a cycle of further abstraction. By repeatedly abstracting the abstracted, it has constructed highly abstract systems \cite{bochner1966role}. Abstraction involves extracting only partial common properties of objects, and it is closely related to understanding, generating more universal knowledge. Additionally, through the premises established by abstract concepts, further deductions lead to a broader understanding of the world \cite{heisenberg2008abstraction}. I believe these characteristics make mathematics an indispensable part of knowledge production. 

Thus, the ability to utilize deductive systems is extremely important in expanding the realm of practically discoverable knowledge. Therefore, when it comes to achieving intelligence capable of autonomous research, it becomes crucial to consider how to construct new deductive systems or at least enable the utilization of existing deductive systems.

\section{Conclusion}

In this chapter, we discussed what research is. Research is the act of producing new knowledge for a community of constituents capable of forming shared beliefs. We characterized the production of new knowledge as the process of posing unanswered questions, generating hypotheses in response to those questions, and verifying them. Additionally, starting from the standpoint that knowledge is belief, we also presented the depiction of research as belief updating.

From the perspective that research is belief updating, the roles of question construction, hypothesis generation, and hypothesis verification can be represented in the following Fig. \ref{fig:beliefupdate}. 
\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{figs/beliefupdate.jpg}
    \caption{Research Process as Belief Update}
    \label{fig:beliefupdate}
\end{figure}
In the diagram, circles represent statements, and squares represent beliefs in response to those statements. Note that while circles were described as statements, they are not limited to textual representations as long as they are associated with beliefs. Beliefs are determined by the subject holding the belief (in this case, an agent) and the object of the belief (the statement), but please note that we are assuming a fixed agent in this context.

Firstly, in this world, there exist countless pairs of statements and corresponding beliefs (or latent beliefs). Posing a question corresponds to extracting a subset of unknown statements from this pool. More precisely, an unknown statement refers to a statement for which the agent is unsure whether to assign a strong or weak belief. Choosing corresponds to implicitly determining a function that assigns beliefs to each statement. For example, let's consider posing the question ``When did the universe begin?'' This answer to this question is unknown to humanity. By posing this question, the range of possible answers is narrowed down. However, it is not realistically possible for an agent to be aware of all statements and potential answers that could exist. Therefore, while I mentioned pairs of beliefs and statements, please consider that most of these beliefs are latent and potential.

Next, generating a hypothesis involves selecting one pair of a claim and a belief from among the potential claims that could serve as potential answers to the question. It is at this point, by choosing a hypothesis or considering multiple candidate hypotheses, that discourse becomes consciously acknowledged, and beliefs are substantively assigned. Finally, as we discussed in Chapter 2, verification involves gaining evidence for the belief in the chosen hypothesis, resulting in the updating of the belief state. The updated beliefs are represented in black in the diagram.

\chapter{Research Automation Literature Review}

\section{Pipeline}
While not necessarily using machine learning, there are several attempts to automate not a particular task but the entire research process. A seminal early works are Adam \cite{king2004functional}, and Eve \cite{williams2015cheaper}, closed-loop systems for scientific discovery. 

Additionally, the concept of a \textit{scientific workflow}, which represents data and computational processing pipelines in research as software, emerged in the early 2000s. The developments and advances in research related to scientific workflows are consolidated in this literature \cite{barker2008scientific,atkinson2017scientific}. Additionally, these papers \cite{deelman2019role,nouri2021exploring} discusses how machine learning contributes to streamline the each step in the scientific workflow.

These are not about creating machine learning agents that can do research. However, these are extremely important initiatives in terms of softwareizing the research process \cite{deelman2015pegasus,gil2011semantic}.

\section{Literacy}

\subsection{Searching}

Firstly, let me mention academic search engines that provide features to help researchers find relevant papers from a vast amount of literature \cite{googlescholar,semanticscholar,dblp,pubmed,citeseerx}. 

Specialized search systems have also been proposed for specific purposes. For example, some studies have been proposed systems to discover studis in other domeins \cite{kang2022augmenting}, difficulties, limitations and emerging hypotheses \cite{lahav2022search}, and author homepages \cite{patel2021author}. Also, in response to the COVID pandemic, several systems have emerged in recent times to search for COVID-19 related papers \cite{hope2020scisight}.

Instead of searching for papers manually, there are approaches that directly recommend papers to researchers. In practice, it is common for the specific aspects of academic papers that researchers want to compare to vary depending on the situation and research field. Therefore, researchers have invented the method allowing comparison in certain aspect of papers \cite{ostendorff2020aspect} and tailored to some particular research area \cite{breitinger2022recommending}. Also, some researchers study recommendation of authors instead of papers \cite{portenoy2022bursting}. A comprehensive summary of classical research on paper recommendation can be found in \cite{bai2019scientific}.

\subsection{Reading}
The majority of research on automation in research pertains to automating operations related to papers. Specifically, research on information extraction from papers constitutes the majority. Here, \textit{reading} refers to extracting information from a paper.

Several methods specialized in extracting specific information have been proposed. For instance, there are studies for extracting mathematical expressions \cite{greiner2020math,madisetty2021neural}, measure \cite{harper2021semeval,kohler2021s}, tabale and figure \cite{shen2022vila,hashmi2021current,zhuang2022resel,yamamoto2021visual}, dataset \cite{hou2019identification,kumar2021dataquest,prasad2019dataset}, and results \cite{kardas2020axcell}.

There are also studies that focus not on information extraction, but on determining the meaning of sentences written in papers. One representative example is research on citation classification, which involves understanding the intent behind the cited text \cite{pride2019act,kunnath2021meta,kunnath2022dynamic,kunnath2022act2,lauscher2021multicite}. Another example is topic/theme classification, which detect the main topic of the paper \cite{sadat2022hierarchical,mendoza2022benchmark,salatino2022cso}.

One of the most heavily researched areas of information extraction from scientific papers is summarization. Some studies propose methods to generate the contribution of a paper \cite{hayashi2020s}, scientific claims \cite{wright2022generating}, and lay summarization \cite{goldsack2022making}. Other studies have attempted to create better paper summaries using citation graph \cite{chen2022scientific,an2021enhancing}, or propose the summarization system \cite{erera2019summarization}.

Many of the earlier summarization studies only used limited information such as abstracts. In recent years, there have been proposed studies that generate summaries by reading the entire paper \cite{subramanian2019extractive,qi2022sapgraph,dong2020discourse,tretyak2020combination}.

Also, the number of papers has increased dramatically, and the time available for obtaining information from a single paper has become increasingly limited. Thus, some studies propose the methods to generate extremely short summaries, such as TLDR \cite{cachola2020tldr} and key phrases \cite{boudin2021keyphrase,garg2021keyphrase}.

To advance these summarization studies, some studies propose datasets \cite{yasunaga2019scisummnet,bastan2022sume} and annotation platforms \cite{el2022platform} for paper summarization. 

The early research on paper summarization, which was conducted relatively early, is well summarized in \cite{altmami2022automatic}. If you are interested, please also refer to this paper.

Up to this point, we have described methods that assume extracting specific information or summarizing papers. In contrast, there are studies that issue queries in natural language to retrieve desired information from papers. This has been formalized as a question-answering task, a more general problem setting \cite{lu2022learn,ruggeri2022argscichat,saikh2022scienceqa}. 

In the field of question-answering for academic papers, some web services have gained attention for its high performance \cite{elicit,scispace}. Elicit use large language models and compose them to write \textit{compositional language model programs}. Ought \cite{ought}, the provider of Elicit, publish the instructions of how to write compositional language model programs \cite{primer2022}. Also, they disclose how to update their system with their idea of \textit{process supervision} \cite{reppert2023iterated}. Therefore, for those who are interested in question-answering systems for scientific papers, I strongly recommend reading these papers and documents.

Lastly, many tools have been proposed to assist researchers in reading papers. These studies highlight rhetorical roles \cite{fok2023scim,lauscher2018arguminsci}, generate description to terminologies \cite{august2022generating,head2021augmenting,murthy2022accord}, simplify texts for non-experts \cite{august2022paper,jeblick2022chatgpt}, and allow interaction \cite{kang2022threddy,elicit,scispace}.

\subsection{Writing}
Research is the act of producing a novel knowledge on top of prior studies. The apt incorporation of previous literature and elucidation of the distinctions between the proposition and previous studies are essential. Consequently, some researchers have investigated to generate comparative arguments \cite{yu2022scientific} and others have studied to generate citation texts \cite{arita2022citation,gu2022controllable,wang2021autocite,xing2020automatic,funkquist2022citebench}. Additionally, several studies exist that, instead of directly producing text, aspire to assist in the writing process by recommending relevant literature for inclusion as citations \cite{farber2020citation,zhang2020dual,duma2019contextual,farber2018cite,gosangi2021use}. Furthermore, there exist investigations aimed at automating systematic reviews writing \cite{dones2022systematic}.

Scholarly articles are structured documents. This structural property enables researchers to generate texts per sections. Thereafter, 
researchers have endeavored to generate, for example, abstract \cite{kumarasinghe2022automatic,gao2022comparing,wang2019paperrobot}, related works \cite{li2022automatic,shah2021generating}, table description in result section \cite{moosavi2021scigen,moosavi2021learning}, conclusion, and future work \cite{wang2019paperrobot}. Wang et al. propose to generate even next research's probable title \cite{wang2019paperrobot}.

Similar to the situation with reading, proposals have emerged for systems to support writing \cite{narimatsu2021task}, as well as for datasets to train text generation \cite{chen2021scixgen}. In recent times, some researchers try to have GPT series to write academic papers \cite{transformer2022can}. 

\subsection{Scientific Language Models}

Whether engaging in reading or writing, the presence of a system that comprehends natural language is indispensable. In recent years, large-scale language models, trained on extensive textual data, have achieved significant success. Concurrently, numerous language models, specifically tailored to scientific documents, have also been proposed \cite{beltagy2019scibert,singh2022scirepeval,nadkarni2021scientific,cohan2020specter,gupta2022matscibert,taylor2022galactica}.

\section{Knowledge Production}
% The Process of Creating New Knowledge

% Modern research is constructed from three main phases: observation, hypothesis generation, and hypothesis validation <- not line but cycle

\subsection{Issue Discovery}
Lahav et al. have proposed a methodology for automating the discovery of prevailing challenges within the research community, as well as the emerging hypotheses to address them \cite{lahav2022search}.

\subsection{Hypothesis Generation}
A hypothesis is a proposition put forward in response to the problem or question that the research is focused on. These claims or proposals will be evaluated for their validity at a later stage through some form of procedure. These claims or proposals that involve uncertainty regarding its validity as an answer to the research objective are referred to as hypotheses in this context.

Because of its nature as an answer to the objective, what constitutes a hypothesis varies depending on the purpose of the research. For example, if a paper aims to enhance our understanding of a particular phenomenon, then the description or mechanism of that phenomenon would become the hypothesis. Similarly, if the objective is to solve a particular problem, then the solution to that problem would be considered the hypothesis. 

Therefore, I acknowledge that there are still few proposed methods for automatically generating hypotheses that are applicable to all research areas. However, there are already several attempts at automating the elements that are considered important in hypothesis generation.

\subsubsection{Analogy} 
% \subsubsection{Analogy} 
% TODO: Reframe this based on the cognitive aspect borrowing from "A Computational Inflection for Scientific Discovery"

One of the elements considered important in this regard is the automation of \textit{analogical reasoning}. While the specifics of the research objectives may differ, they all share a common aim of proposing solutions that transform the unknown into the known or unresolved into the resolved. However, it is impossible to make meaningful inferences about completely unrelated subjects that have no connection to the known. The objectives and potential proposals are expected to be similar to some extent to those that existed in the past. Therefore, discovering similarities in the structure of past objectives and the current objective, and applying the means that were effective in solving past problems to the new objective, can increase the likelihood of achieving the objective. In this sense, analogical reasoning, which involves inferences based on the structural similarities between two objects, is considered to be crucial for hypothesis generation \cite{hesse1965models,thagard_1984,gentner1993shift,holyoak1996mental,dunbar1997scientists,gentner2002analogy}. 

In recent years, several researchers have presented studies on automating scientific analogical reasoning for identifying the relationship between problems and their corresponding solutions using data from scientific papers \cite{kang2022augmenting,chan2018solvent}. 
Systems have been proposed that recommend researchers who are pursuing analogous objectives via divergent approaches \cite{portenoy2022bursting}. Similar to the case of analogical reasoning, this concentrates on abstract relationships, aiding in the generation of beneficial hypotheses by applying previously successful methods to novel techniques.

\subsubsection{Symbolic Regression} 

Modern science is composed of a cycle of observation, hypothesis generation, and hypothesis testing. In many fields, including physics, chemistry, and biology, mathematical models are often constructed as hypotheses from observational data. That is to say, formulating a mathematical representation that elucidates the phenomenon behind the data is an extremely critical step in science. One attempt to automate this endeavor is symbolic regression \cite{makke2022interpretable}, or equation discovery. While classical approaches to symbolic regression have traditionally employed methods such as evolutionary computation, recent years have seen the emergence of strategies utilizing deep neural networks \cite{petersen2019deep,udrescu2020ai,udrescu2020ai2,cranmer2020discovering,kamienny2022end,d2022deep}. Some researchers have proposed the frameworks \cite{landajuela2022unified,keren2023computational} and benchmarks \cite{matsubara2022rethinking} for symbolic regression. You can find a literature review of symbolic regression in \cite{makke2022interpretable}, and that of the early studies in \cite{kramer2023automated}.

\subsubsection{Others} 

Indeed, some methods have been proposed that don't generate hypotheses directly, but rather assist humans in generating hypotheses from experimental data 
 \cite{friederich2021scientific}.

\subsection{Verification}
\subsubsection{Verification Design}

Once a hypothesis is formulated, a plan is developed to test its validity. The design of this verification plan is far more flexible than that of hypothesis generation, making it more difficult to handle uniformly. To be more precise, while many sciences have standardized methods such as statistical tests for verification, there is a wide variety of methods for generating the data used for the verification. One study may require a huge machine to collide elementary particles, while another may use rats for behavioral experiments. Some studies may require the use of chemicals, while others can be simulated on a computer. Furthermore, even with standardized statistical tests, as mentioned earlier, automating their creation from scratch proves exceedingly challenging. It is readily apparent that devising standardized methodologies like statistical tests is difficult when one must not merely employ them as tools but also contemplate the very nature of what it means to verify, as well as the rationale behind adopting specific assumptions. Therefore, it may not be an overstatement to say that this aspect represents the biggest obstacle towards achieving complete automation of research in a unified manner.

Some researchers have tried to automate experimental design for quantum physics \cite{ruiz2022digital} or proposed to design workflow of scientific research as a software \cite{goble2020fair}. Additionally, research exists that proposes machine learning algorithms for formulating and executing experimental designs in a more abstract and simple manner \cite{herrmann2022learning}. The field of \textit{experimental design} has a long-standing history. Its primary objective is the automation of appropriate configuration and exploration for various conditions after you designed a base structure of an experiment. Specifically, research involving techniques such as Bayesian optimization for condition search has been conducted for some time \cite{chaloner1995bayesian,shahriari2015taking}.

\subsubsection{Verification Execution}

Once a verification plan has been devised, the process proceeds in accordance with it. As previously mentioned, the approach may vary considerably. However, in many scientific methodologies, statistical techniques are employed. In these instances, the verification process can be broadly divided into two stages: 1. data generation and 2. analysis of the generated data for validation.

As previously mentioned, data generation methods span a wide range. Among these, attempts have been made to automate the work of researchers within laboratories, an endeavor known as Laboratory Automation. For instance, 
some studies focus on automating cell culture tasks using humanoid robots \cite{ochiai2021variable},
% TODO: add more

% TODO: may differentiate the analysis for hypothesis generation from that for verification
We interpret data processed according to a certain criterion, assessing the validity of our inferences. Here, I make an explicit distinction between analysis for verification and analysis for hypothesis generation. Modern science is composed of a cycle of observation, hypothesis generation, and hypothesis testing. It's common to generate the next hypothesis to be tested from data produced for verification. However, this merely signifies that we conduct both hypothesis generation and testing through a somewhat inductive reasoning based on data. Therefore, in this context, we will focus on data analysis for hypothesis testing, while data analysis for hypothesis generation will be included in the hypothesis generation section introduced earlier.

To validate the plausibility of assertions, we currently employ statistical methods. There is research that automate the hypothesis testing \cite{gil2016automated}. 

Some researchers have engaged on automating data visualization and analysis \cite{bavishi2021vizsmith,bavishi2022tools}



\subsection{Peer Review}
Many studies have tried to automate peer review generation \cite{thelwall2019artificial,li2019generating,schulz2022future,yuan2022can,yuan2022kid,lin2021automated1,lin2021automated2,kumar2022investigations,bharti2022can,uban2021generating,wang2020reviewrobot}. While not generating peer reviews directly, studies focused on automating research paper assessment  can be said to be related to the peer review automation. \cite{kousha2022artificial,li2020multi,huang2018deep}. These studies have proposed the method to assess the quality \cite{thelwall2022predicting,thelwall2022can}, novelty \cite{pelletier2022novelpy,amplayo2019evaluating,shibayama2020measuring}, soundness \cite{cabanac2022decontamination}, and significance \cite{zong2022citation,xia2023review,soni2022predicting,manghi2021new,soni2021follow,van2020schubert,mckeown2016predicting}.

These investigations concern the automation of processes occurring subsequent to a manuscript's arrival at the hands of reviewers. Conversely, researchers also have investigated the automation before that process, such as determining the appropriate journal for submission \cite{michail2023journal} and assigning the reviewers \cite{zhao2022reviewer}.

While not centered on automation, certain studies engage in the scientific analysis of the review process \cite{shah2022challenges,verma2021attend,bharti2022confident,bharti2022betterpr,verma2022lack,kennard2022disapere}. These investigations serve to enhance our understanding of the nature of peer review and, in turn, provide valuable insights for the design of more effective automated review methodologies. Furthermore, automating the \textit{scientific claim verification} \cite{li2019scientific,wadden2020fact,wadden2022scifact,wadden2022multivers}, which determines the validity of a scientific claim through analysis of research paper corpora, is likely to contribute to the automation of the peer-review process.

\section{Knowledge Sharing}

Upon the completion of a study, the drafting of a manuscript, and its successful navigation of the peer-review process, the resulting findings are deemed to possess a degree of credibility as knowledge. Naturally, it would be hasty to assert that this alone births "correct" knowledge, as research demands iterative verification to confirm its validity. We convey such knowledge to others through various means, one of which is the presentation of research findings. To effectively communicate these outcomes, we create slides that elucidate our work. Studies also exist that strive to automate this aspect of the dissemination process \cite{sefid2019automatic}.

\section{Perspectives}

Gill presents an extremely exciting idea of conducting automated research by turning the entire scientific process into compositional and modular software with the literature review of her and her colleagues' work. \cite{gil2022will}. For example, Gill et al. have created a software of the semantic workflow of scientific data analysis and computation process  \cite{gil2011semantic}. 
Each step of this workflow modularizes the procedures in research. Not only can these make analysis more efficient, but they also allow for the analysis of the research process itself. Furthermore, common workflows can be identified from multiple workflows, enabling abstraction of cross-domain knowledge about research process. Additionally, Gill and her team have proposed DISK, a systematic framework for hypothesis testing and data analysis. DISK can automatically cycle through a series of processes, including the generation of hypotheses, the determination of data and methods to test them, the acquisition of data from shared repositories, the analysis of that data, and the modification of hypotheses. Furthermore, each hypothesis is associated with information on confidence level and analysis details, which significantly indicates the plausibility of the hypothesis. Additionally, as the hypothesis and its confidence level and analysis are continuously updated and the revision history is retained, it enables the continuous maintenance and update of scientific findings.

Kitano also harbors an ambition to automate the entirety of the research process \cite{kitano2021nobel}. This is a thought-provoking paper that is meticulously contemplated. Kitano underscores the ability of AI in automating science to execute exhaustive and thorough exploration as a significant strength. We, as humans, aim to generate hypotheses that yield impactful results (Kitano refers to this as a value-driven approach). However, the importance of research findings is context-dependent, and research that we humans deemed unimportant may become crucial if the presuppositions or the context alter. Kitano proposes to eschew this value-driven approach and implement an alternative, exploration-driven methodology to science, aiming for novel scientific discoveries that were unattainable by human capabilities. Besides, Kitano with many examples and detailed consideration, presents a plethora of stimulating ideas, such as the continuous hypothesis network update, a roadmap to achieve autonomous artificial scientists, and proposition of the Nobel Turing Challenge as a Grand Challenge to substantially advance these endeavors. I highly encourage those interested to delve into this fascinating read.

Hope et al. have written a captivating perspective paper on the automation of research, presenting a fresh and exciting viewpoint \cite{hope2022computational}. They introduce a human-centric idea aimed at efficiently extracting relevant information from the ever-expanding body of research data, tailored specifically to the tasks researchers are engaged in - a framework they term \textit{task-guided scientific knowledge retrieval}. They start by conceptualizing the act of research as an interaction between a researcher's \textit{inner cognitive world} and the \textit{outer world}, or \textit{scientific ecosystem}. Building on this, they underscore the vital role of representing and retrieving information that aligns with the inner cognitive world of researchers, deftly transforming the cognitive functions used in human research into algorithmic processes.

Extensive discourse transpires concerning scientific discoveries. Yet, discussions pertaining to scientific comprehension remain relatively unexplored. Hope et al. delve into the conundrum of what it entails for a machine learning agent to not only unearth scientific knowledge but also to comprehend it \cite{krenn2022scientific}. They adopt a human-centric stance, positing that an agent's ability to offer explanations comprehensible to human scientists signifies the existence of its scientific understanding.

\section{Applications}

\subsection{Mathematics}
The automation of mathematical proof, \textit{automated theorem proving} (ATP) , has been studied for a long time. Recently several effort has come up to improve ATP by using machine learning, and especially deep learning. The 
 early seminal work is led by Schulz \cite{schulz2001learning} and Urban \cite{urban2004mptp,urban2008malarea}. The first work applying deep learning to ATP is \cite{irving2016deepmath}. Subsequently, numerous studies have emerged on Automated Theorem Proving (ATP) using deep learning \cite{bansal2019holist}. Recent studies on this topic are well organized in the following paper, so I recommend reading it if interested \cite{rabe2021towards}.
 % TODO: more organized literature review

 While research has been accumulated on ATP, there is still not much research done on the automated theorem discovery with a few exceptions \cite{gao2014systematic}. In recent years, attempts have been made to help humans to find mathematical conjectures \cite{davies2021advancing} and
 automatically generate mathematical conjectures \cite{raayoni2021generating}  using machine learning.

\subsection{Science}


It has become commonplace to streamline domain-specific tasks in scientific research using machine learning, resulting in a vast number of published papers. Even just to mention a few that come to mind, there are studies on molecular biology \cite{jumper2021highly,senior2020improved}, material science \cite{ramprasad2017machine}, medical science \cite{vamathevan2019applications,shorten2021deep}, quantum mechanics \cite{carleo2017solving}, cosmology \cite{carleo2019machine}, genetics \cite{libbrecht2015machine}, and nuclear physics \cite{degrave2022magnetic}. It is impossible to cover all of these applied studies of research automation of science. Therefore, in this paper, I will not go into detail about each of these studies. Instead, I will present research on automation of elements that can be applied in various fields of science. For the literature survey of domain specific automation, please refer to \cite{xu2021artificial}. \textcolor{red}{TODO: Add application studies}



\chapter{Proposal}
In this chapter, I will discuss what should be done and how to achieve the realization of autonomous artificial researchers. Firstly, I will provide an overview of Chapter 1 and Chapter 2. Building upon that, I will propose subgoals to aim for in order to achieve autonomous artificial researchers. Subsequently, for each subgoal, I will organize subtasks and propose a general approach and strategy for how to pursue these intermediate goals.

As I have reiterated multiple times, the ultimate goal is to create an artificial intelligence that can autonomously conduct research. Therefore, in order to clarify the objectives, it is necessary to define what it means for an AI to be able to conduct research and what it means for it to be autonomous. Let's first discuss these aspects.

\section{Goal and Research}
First, let's discuss what it means for an AI to be able to conduct research. We delved into this topic in detail in Chapter 2. Research is the act of producing new knowledge for a community of constituents capable of forming shared beliefs. Producing new knowledge involves posing unanswered questions, formulating hypotheses in response to those questions, and verifying those hypotheses to update beliefs. Therefore, creating an intelligent system capable of conducting research means creating a system that can perform these tasks.

\section{Goal and Autonomy}
The ultimate goal is to create an artificial intelligence that can produce knowledge. In other words, the aim is to develop a function that can autonomously generate some form of knowledge when given any input. Therefore, any attempt to achieve the realization of an artificial researcher should strive to create such a function, regardless of its specific form. As I have reiterated, if the knowledge generated through research is a shared belief in a justified and true society, then the goal can be rephrased as aiming to generate a belief in response to any input and justify it to the extent that it becomes a shared belief in a society.

Among these goals, what I am aiming for is an artificial intelligence that can conduct research completely autonomously. Therefore, the ultimate aim is to minimize predetermined modules or algorithms decided by humans and create an end-to-end system. With that in mind, I would like to express my opinion on how much autonomy should be pursued.

Firstly, autonomy ultimately boils down to the question of how much is taken for granted. The more predetermined elements there are, the lower the autonomy, and the fewer predetermined elements there are, the higher the autonomy. In this sense, autonomy is continuous and incremental.

The highest level of autonomy in pursuing the realization of an artificial researcher is a state where the system invents and conducts research without assuming its purpose is knowledge production. This means not instructing the system to conduct research, not teaching what research is beforehand, but enabling the acquisition and execution of research. Humans can be considered researchers with this level of autonomy at least at the species level. This is because while humans are probably not designed as systems dedicated solely to research, they have progressively invented research throughout history. Of course, such a level of autonomy is not achievable without any assumptions, so if one aims for this level of autonomy, it is necessary to prepare an environment that induces the acquisition of research and design some form of inductive bias. What I am referring to here is a system with autonomy in the sense that research is acquired as a result of computational processing indirectly related to knowledge production and the necessary conditions for it.

However, demanding such a level of autonomy may be somewhat excessive when it comes to pursuing an autonomous artificial researcher. Therefore, it seems reasonable to assume knowledge production as a design requirement. Since knowledge production is assumed, some form of information about what knowledge is must be provided. Here, we can classify it into two categories.

The first is implicitly providing information about what research is. This involves allowing the system to acquire the concept of research on its own based on existing knowledge and data about the process of generating hypotheses and methods of verification. The history of machine learning research has shown the power of this end-to-end direction in creating strong intelligent systems. In that sense, this direction can be considered one of the directions to pursue.

The second is explicitly providing information about what research is. This involves providing only the necessary information about the conditions related to research and aiming for their fulfillment. It does not mean explicitly instructing how to construct questions or specific means to justify beliefs. Instead, it only provides information about what needs to be done, and the system must acquire and execute them on its own. This is autonomy in the sense that one must acquire the methods oneself and execute them. I believe that if we provide information at this level, it can be considered a system capable of conducting research autonomously. On the other hand, I think that research achieved by hard-coding the processing within the construction of questions, hypothesis generation, and verification, while automatic, is not autonomous.

While achieving the autonomy in the first two stages would certainly be great, it is expected to be quite challenging to aim for that directly. Therefore, I believe it is better to aim for the third level of autonomy. Specifically, as proposed in Chapter 2, it would be beneficial to aim to create a system that divides the research process into three modules: question construction, hypothesis generation, and verification, and connects them as a pipeline. To achieve that system, we should strive to incrementally enhance the internal autonomy of each module.

\section{Pipeline}
The ultimate goal is to create an artificial intelligence capable of conducting research autonomously. Therefore, in addition to automating individual modules of research, it is necessary to create a system that can autonomously carry out the entire research process from start to finish. Building such a research process pipeline is one of the sub-goals we aim for.

This is similar to the pipelines discussed in MLOps in the context of machine learning or scientific workflows mentioned in previous studies. However, unlike those, I envision a system that does not include processing that is heavily dependent on specific research domains or tasks. Instead, I formalize it as a pipeline consisting of modules for question formulation, hypothesis generation, and planning and execution of validation, as discussed in Chapter 2. This system autonomously constructs and validates hypotheses for any given question with an unknown answer.

\section{Strategy}



\section{High Level Idea}

Proposal for the progress of automation of individual tasks

Automation of tasks and fundamentally autonomous (end-to-end) research

Proposal for the progress towards the realization of intelligence that can autonomously conduct research.

\section{Concrete Proposals}
One problem is that the research papers do not necessarily represent the way to do science, which is rather constructed retrospectively \cite{schickore2008doing}.



\chapter{Research Optimization}

\chapter{Alignment}
Understanding is subjective, so there is a need to consider alignment. Even if the process of hypothesis generation is incomprehensible to humans, as long as the verification is connected to fundamental human beliefs, it can be said to be knowledge based on a common foundation with humans.

\chapter{Conclusion}


% \bibliographystyle{unsrt}
\bibliographystyle{apalike}
\bibliography{ref}

\appendix
\chapter{Research as Social Activity}
In this paper, we will discuss automation focused on the unique elements of knowledge production as mentioned above. However, research is a social endeavor. And that society has various levels, such as research labs, universities, and research ecosystems. Therefore, if we think about optimizing the whole activity of research, we need to think about optimizing these wholes. Although it is out of the scope of this paper and therefore not discussed this time, I would like to discuss this in the future.

\end{document}