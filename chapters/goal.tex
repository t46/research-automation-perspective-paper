\section{What Would Activities Called Research Be?}
\label{section-what-is-research}

To create an agent capable of conducting research autonomously, it is crucial to understand what research is in the first place. Therefore, in this section, I would like to discuss the characteristics of what is called research. Particularly, I will strive to discuss characteristics that can be found activities called research, regardless of differences in fields such as humanities, natural sciences, and mathematics.

Please note that my aim here is not to identify the universal and singular definition of research, which is beyond my ability. Rather, I will endeavor to explore characteristics of research so that it will provide a starting point for the discussion for realizing research-capable AI. Therefore, the definition of research that I'm going to discuss is merely a tentative and operational one. What matters is to consider how research can be characterized, and the discussion here is just one example of that. I believe that finding a better characterization of research through further deepening discussions is crucial for realizing agents capable of conducting research. I hope that the discussion here can aid in such endeavors.

% I hope that further discussions among researchers will deepen towards a better characterization of research.

% Please note again that this definition is provisional. What matters is to consider how research can be characterized, and the discussion here is just one example of that. I believe that finding a better characterization of research through further deepening discussions is crucial for realizing agents capable of conducting research. I hope that the discussion here can aid in such endeavors.

\subsection{Research as Knowledge Production}

% 正当化の目的が真理促進的であることを書いたら、for society はいらない気がする？認識である以上主体は必要だというのは間違いない。全部書いてみて後で検討。

% 研究による成果には、偽であるが真であるように見える命題がある。なので、知識の生産というよりも、知識の候補を生み出しその確からしさについての確証を強めていくという方が適切かもしれない。偽であった時にも有用な情報。なので、知識を生み出すというよりも、知識を生み出すことを試みるとした方がいいかも。

% \textcolor{red}{
% 知識を生み出すではなく、知識を生み出すことを目指す・試みる、とする。なぜなら命題は必ずしも真ではないし、偽であると適切に判定されることは有用な情報をもたらすから。そう考えると、「この世界の未知の真理を明らかにしようとすることが研究」という方が適切？？？？←知識の中に「真である」ということが含まれているのと、正当化に真理促進性を要求してるので、どちらでも変わらない気がする？知識を生み出そうとするだと生み出せなかった時が単に失敗という感じになってしまう？違うな。検証が命題Aを肯定すると、Aという知識が生まれるが、命題Aを否定すると、Aの否定という知識が生まれるというだけの話。なので問題なし。なので、「知識を生み出すことを試みる行為」でいい気がする。
% }

% \textcolor{red}{
% 既存の定義に触れる
% }
It seems infeasible to find a unified, all-encompassing definition of research or science \cite{chalmers2013thing,sep-scientific-method}, but there are various thoughts on what research is. For example, there is a view that ``we do research whenever we gather information to answer a question that solves a problem'' \cite{booth2003craft}, and it is also said that research (and development) ``comprise creative and systematic work undertaken in order to increase the stock of knowledge'' \cite{manual2015guidelines}. Moreover, some present a perspective that scientific process can be viewed as ``Bayesian belief updating; i.e., processes that maximize the evidence for a generative model of the sensed and measured world'' \cite{balzandistributed}. These characterizations each capture important aspects of research, and none of them are necessarily wrong, nor is any one of them absolutely right.
% 研究や科学の統一的な全てを包括する定義というものはありえなさそうですが \cite{chalmers2013thing,sep-scientific-method}、研究とはなんであるかについてはさまざまな考えがあります。 例えば、``we do research whenever we gather information to answer a question that solves a problem'' \cite{booth2003craft} という考えもありますとし、research (and development) は ``comprise creative and
% systematic work undertaken in order to increase the stock of knowledge'' 言われることもあります \cite{manual2015guidelines}。

Among such characterizations, a widely accepted definition of research would be that research is the attempt of generating new knowledge. It seems that research, regardless of the field, requires some novelty, and creating new knowledge based on past knowledge. Therefore, this characterization seems to be a good starting point for considering what research is, at least for the current purpose. In this paper, I will tentatively adopt this characterization as a working definition of research. In particular, I will consider that research is the attempts to produce new knowledge for certain society. I included ``for society'' because knowledge appears to be relative to society. I will explain this point in later sections. 

% \textcolor{red}{
% In particular, I will consider that research is the production of new knowledge for certain society. I included ``for society'' because knowledge appears to be relative to society, as I will discuss later. 
% }

% For instance, in physics, a new explanation for a phenomenon is produced, in mathematics, a new proof for a theorem is presented, and in engineering, a new design blueprint for creating something is generated, all as new knowledge.

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figs/reseach_definition.pdf}
%     \caption{Definition of Research}
%     \label{fig:definition}
% \end{figure}

% \subsubsection{Some Notes on Research and Science}

% The reason why I deliberately use the word ``research'' instead of ``science'' is because I want to include fields like engineering, humanities and arts, which are not typically referred to as science, within the scope of automation in the long run. Science refers to a methodology for generating knowledge, and I believe that new knowledge is not necessarily produced only by scientific methods. I believe that so-called humanities and arts also share some commonality of producing new knowledge. Therefore, the definition of generating new knowledge can be said to encompass these fields as well.

% Of course I admit that science is the most rigorous and reliable framework for knowledge production. Because of its power and popularity, most of the existing analysis on research is about science. My discussion would also be centered around the discussion of science, though I try to present the view not limited to science. 

% \textcolor{red}{
% 論文の目的をもっと先に明確に書くことで、こういうただし書き的なのを減らす。
% Please note again that this definition is provisional. By viewing research as the production of knowledge, we can indeed characterize the wide range of current human research activities comprehensively, which is why I adopted this definition. However, as we will see later, in attempting to create a comprehensive definition, it can result in aspects that seem counterintuitive to characterize certain research. It's possible that aiming for a definition that is not field-specific is not desirable (i.e., striving for the realization of an intelligence capable of such a task is not desirable), and it might be better to seek a definition with a more narrowed scope of application. I will revisit and discuss how research should be defined at the end of this section.
% }

\subsection{Knowledge Production as Belief Revision}
\label{section-knowledge-production-as-belief-revision}
I have defined research as the endeavor to generate new knowledge. Then, what exactly is knowledge, and what does it mean to produce knowledge? I will explore this question in this section. 

Defining knowledge and knowledge production rigorously is a philosophical debate that has not yet been settled \cite{sep-epistemology}, and I won't delve into it deeply here. Instead, I would like to provide some primitive ideas that can serve as a starting point for further discussions.

\subsubsection{Knowledge as Belief}
The question of what is the thing called knowledge has been a subject of debate for a long time in the field of \textit{epistemology}, which is one of the branches of philosophy. So, for now, I would like to refer to the discussion in this field as an example and see how research can be characterized.

In epistemology, knowledge had been classically considered to be \textit{justified true belief (JTB)} \cite{sep-epistemology}. The term ``true'' is difficult to define rigorously, but for the purpose of discussion, let's think of it as something being fact. ``Belief'' can be provisionally understood as someone's thought or conviction about something. And ``justified'' means that it is deemed reasonable to hold such a belief. What justification means has been discussed in epistemology as a central point of contention. Since the criticism that JTB is not appropriate as the definition of knowledge \cite{gettier1963justified}, how to modify or expand JTB to make it a suitable definition of knowledge seemingly has been a major discussion in epistemology \cite{sep-epistemology}. 

While most philosophers don't appear to believe that these properties are sufficient to define knowledge, there seems to be some agreement that they may be necessary \cite{sep-epistemology}. Even in the epistemology discussion, rather than abandoning the JTB altogether, many discussions adopt JTB as a base. Therefore, let me tentatively assume that knowledge is JTB in this paper to offer a preliminary basis for discussion. In the following section, I will examine how research is perceived as an activity when adopting this definition. Examining the discrepancy and alignments between the consequences of this characterization and our expectations for a research-capable agent will provide seeds of thought for a better definition of research and hence what that agents should be able to do.

\subsubsection{Knowledge Production as Belief Update}
The purpose of research can be said to be revealing the unknown truths of this world. Therefore, the justification in research is demanded to be such that it determines propositions as true if they are true, and false if they are false. Such justification is called truth-conducive. There are various discussions about what justification is, but it can be said that justification in research should be truth-conducive. Therefore, producing knowledge could be said to be constructing a new proposition that refers to this world and determining its truth or falsity through such justification. 

In the current definition, we regard knowledge as belief. Thus, the producing knowledge can be rephrased as holding the belief that a proposition is true and updating this belief through truth-conducive 
justification. In short, the knowledge production is belief update in current definition.

It might feel counterintuitive to think of research as an updating of beliefs, but considering that hypotheses and theories are continuously updated in research, inductive reasoning does not prove propositions as deduction does, and, as will be discussed later, knowledge depends on the subject, this characterization doesn't seem so far-fetched. 

% Furthermore, as I will discuss later, we justify the belief that a certain hypothesis is true or false in an extremely robust and sound way that would convinces everyone. Given such justification, it seems not so problematic to depict research through the subjective concept of belief in a substantial sense.


% 知るということは、私たちに対してこの世界とその真実があり、私たちがその真実を認識するということです。すなわち、知るという概念はあくまで私たちと世界との両方の存在を前提としている概念であり、その意味で知る主体に依存する概念です。知識の定義に信念という主観的な概念が出てくるのはこのためです。したがって、知識が信念であるとは直感に反するように思われるかもしれませんが、この意味で妥当であるように思われます。

% 知識が信念であるというのは、知る/知っているという概念がそれを知る主体に依存する概念だからです。私たちの外側に世界があり、その中のある真実を私たちが認識するというのがこの定義の意味での知るという行為であり、それはあくまでも世界とそれを認識する主体との関係性を前提として成り立つ概念です。


% It may feel counterintuitive to hear that knowledge is belief. However, there is a reason for this. It is because the concept of ``knowing'' depends on the subject.

% It may feel counterintuitive to hear that knowledge is belief and research is the updating of belief. However, I think that perceiving the production of knowledge as the updating of beliefs is not so unreasonable. This is because, for example, the validation of a hypothesis by an experiment can be interpreted as a strengthening of the belief that the hypothesis is true. Specially in science, we become more convinced of its validity as a hypothesis survives repeated various verifications. This research practice appears to be well aligned with the view of research as a renewal of beliefs.

\subsection{To Know Depends on Knowing Subjects}

% \subsubsection{知るということと主体の存在の関係}
To know means we recognizing some truths or patterns of this world. In other words, the concept of knowing fundamentally presupposes the existence not only the known object but also the knowing subject. This is why the definition of knowledge involves the subjective concept of belief. Therefore, while the idea that knowledge is belief may seem counterintuitive, it appears valid in this sense. Particularly, since justification in research should be truth-conducive, the knowledge produced would be objective. In this sense, using belief, which is a subjective concept, to define research does not seem to be much of a problem.

Moreover, viewing research as the updating of beliefs does not seem too far removed from the practice of research itself. For example, the validation of a hypothesis by an experiment would strengthen our belief that the hypothesis is true or false. We become more convinced of its validity as a hypothesis survives repeated various verifications. This research practice appears to be well aligned with the view of research as a renewal of beliefs.


\subsubsection{Knowledge for Humans}
As mentioned earlier, research is an endeavor aimed at revealing the unknown truths of this world, and the knowledge generated there must be novel. Then, what does it mean for knowledge to be new or unknown?

In the current definition, knowledge is a justified belief about whether a certain hypothesis is true or false. Therefore, unknown knowledge could be a state where such a belief is not held at all, or even if it is held, it is not justified. In other words, under the current definition, the state of certain knowledge being unknown can be understood as a state of belief.

Since the concept of knowing depends on the knowing subject, naturally, the concept of the unknown is also subject-dependent. In research, it seems that so far, this ``subject'' has referred to humanity as a whole. Researchers do not consider knowledge unknown just because a single individual is unaware of it. It is only when none of us know it that we consider it truly unknown. This is why it seems that the knowledge we create through research is required to be knowledge for all of humanity. These are the reason why it seems better to include ``for society'' in the definition of research.


% 前述したように、研究はこの世界の未知の真理を明らかにすることを目指す営みであり、そこで生み出される知識は新規である必要があります。それではある知識が新しい、あるいは未知であるとはどういう状態でしょうか。

% 知識がある仮説が真である/偽であるという正当化された信念でした。であるならば、未知の知識とは、そのような信念がそもそも抱かれていない、あるいは信念が抱かれていてもそれが正当化されていない、状態だということができるでしょう。

% 知るという概念は知る主体に依存した概念でしたので、当然未知という概念も主体に依存した概念です。研究においては誰か一人の個人が知らないだけではその知識が未知であるとみなしません。我々の誰一人としてそれを知らない時に初めて未知であると考えます。すなわち、私たちが研究で生み出す知識は、人類全体の知識であることを要求されているように思われます。これが今回採用する定義に for society を加えた理由です。


% No matter how rigorously you produce knowledge, if it is already known, it cannot be called research. Therefore, we have defined research as the process of producing ``new'' knowledge. In this section, I will briefly discuss what would it mean for knowledge to be unknown or novel.

% It seems that something being unknown to someone can be described as a state where the person has attempted to know something, but the answer is not present in their mind. Therefore, it can be said that when knowledge born from research of a question is considered new, it refers to a situation where there has been no hypothesis that everyone believes to be sufficiently plausible for the question.\footnote{
% While presenting a question that no one has posed ensures that the knowledge is new. Even in such cases, no one also yet knows which hypothesis is correct for that question. This is why I have mentioned only about the unkownness of hypotheses
% }
% In other words, once knowledge is considered as belief, being unknown seems to be interchangeable with having a low level of confidence, which sounds a bit counterintuitive. In this sense, it seems appropriate to say that research transforms a hypothesis with low confidence into one with higher confidence, rather than turning the unknown suddenly into the known.\footnote{
% Here, saying ``there has been no hypothesis that everyone believes to be sufficiently plausible'' is merely to imply nuances that knowledge is belief and that a hypothesis is never fully proven to be true. You can just read it as ``there is no answer'' for simplicity.
% }



% As mentioned later, research can be described as the act of posing questions, proposing hypotheses in response to them, and then verifying those hypotheses to update beliefs on whether the hypotheses are correct. Therefore, it can be said that when knowledge born from research of a question is considered new, it refers to a situation where there has been no hypothesis that everyone believes to be sufficiently plausible for the question\footnote{
% While presenting a question that no one has posed ensures that the knowledge is new. Even in such cases, no one also yet knows which hypothesis is correct for that question. This is why I have mentioned only about the unkownness of hypotheses
% }. For example, we do not know how to create a general-purpose artificial intelligence, which can be said to mean that we do not have an answer (a hypothesis with high confidence) to the question, ``How do you create a general-purpose artificial intelligence?''.

% I said in the paragraph above that ``research is the process of transforming the unknown into the known.'' However research can also be seen as the updating of beliefs, as I have explained. Therefore, the binary depiction of an object suddenly transitioning from the states of unknown to known does not seem appropriate. Rather, it seems more reasonable to consider that beliefs continuously change and I just call some group of belief states unknown and others known, for convenience.

% I don't know precisely what it means to be unknown. This is a difficult problem, but let's consider it naively. 
% I consider a knowledge to be unknown when for a question a subject lacks any hypothesis which he/she has a JTB that the hypothesis is true. For example, I do not have the answer to the question of ``How to realize an artif icial general intelligence.'' So the knowledge of ``the way to realize an artificial general ingelligence'' is unknown. \textcolor{red}{TODO: Add explanation}

% In research, a single verification does not always immediately turn a hypothesis into true or false. Rather, hypotheses that withstand repeated verifications through various experiments by different researchers gradually come to be regarded as more plausible. Therefore, it seems to me appropriate to say that research transforms a hypothesis with low confidence into one with higher confidence, rather than turning the unknown suddenly into the known. In that sense, it seems somewhat justified to describe new knowledge as something for which there wasn't a highly confident hypothesis before.

% The expression ``unknown'' could be more appropriately expressed as ``high degree of unknownness''. Because knowledge production is a continuous concept of belief updating, therefore, unknownness is also a continuous concept. In reality, there are multiple hypotheses with varying degrees of certainty concerning a particular question. The state of knowledge being unknown can be expressed as not having any hypothesis among these hypotheses with a particularly strong level of justified certainty.

% First, research begins with a particular question. The state of not knowing the answer to this question is what I consider the state of being unknown. In other words, it can be thought of as a state where I don't know what the candidate hypotheses, which are the potential answers, are like, or a state where I know the candidate hypotheses but don't know their plausibility. These are states where I have not been able to find hypotheses or sets of hypotheses that can be assigned a particularly high degree of confidence from the set of potential answers to a given question. Therefore, provisionally and casually, it might be said that the state of ``not having highly confident beliefs (or a set of beliefs) for a particular question'' is unknown, and the state of having highly confident justified beliefs is known.

% Of course, there are issues with this clarification. For example, it is unlikely that I can select a single highly confident hypothesis from the entire set of possible hypotheses. Also, rather than feeling equally confident about all possible hypotheses, it seems that I implicitly distinguish between hypotheses that seem relevant and those that do not. Furthermore, it is unclear to what extent I consider a state to be unknown based on the degree of confidence. However, these are highly challenging philosophical discussions, so I will refrain from delving further into them and, for now, would like to conclude with the vague and provisional definition above and move on to the next topic.

% \subsection{Publicity of Knowledge}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% What I want to emphasize here is that research can be seen as linking or replacing the weak belief in the plausibility of newly conceived hypotheses with such extremely strong beliefs. For example, believing in the effectiveness of statistical methods is strongly related to believing in the effectiveness of inductive reasoning. Therefore, if the validity of a hypothesis is confirmed using statistical methods, we would believe it as highly reliable. Hence, considering research as the updating of beliefs can be reasonably felt, and I don't intend to argue that research is meaningless just because it is the belief revision.\footnote{
% I naively assume that beliefs rooted directly in perception are more robust, and the notion that anchoring a belief on those foundations enhances its reliability. However, the question of how to justify beliefs is a complex problem with extensive discussions \cite{sep-epistemology} (my position seems to align somewhat with what is called \textit{empirical foundationalism}).Due to my limited philosophical knowledge and the ability to delve deeper into philosophical discussions, this paper will not further pursue these arguments.
% }

% It is said that we need to assume that ``under the same conditions, the same phenomenon will continue to hold'' (principle of uniformity of nature) \cite{sep-induction-problem}, which is an intuitive and natural assumption without which we could hardly even go about our daily lives.\footnote{
% I understand that justifying the validity of inductive reasoning based on our experiences would be circular reasoning. The problem of what assumptions make us inductive reasoning consider rational is a challenging unanswered philosophical issue, which is beyond the scope of this paper. Thus, I will refrain from delving into the details here and leave it for future discussion.
% } 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% For instance, while most empirical scientific research verifies hypotheses through hypothesis testing, it seems that this is because results deemed plausible through statistics or inductive reasoning are thought to be plausible by virtually any human.

% Since knowledge production is belief update, generating knowledge for humanity requires updating the beliefs of all, or at least certain number of, people.\footnote{
% Even if the knowledge is not immediately understandable, it should be potentially understandable to a considerable number of people. This means that the generated knowledge is not only for the society at the time of its production but also for the broader humanity, including future human societies. For instance, it might be challenging for children to determine whether the research outcomes in physics qualify as knowledge. However, by studying physics extensively, they may eventually comprehend cutting-edge research in the future. 
% }\footnote{
% Because it's impossible for different individuals to have exactly the same belief, as long as a justification can update the beliefs of different individuals in a similar direction in some way, I think it sufficient to be regarded as updating shared beliefs.
% } 

% Therefore, I believe research is the act of not only changing an individual's belief but also changing multiple individuals' beliefs. 

% Although I don't think it is possible for multiple people to hold exactly the same belief or for their beliefs to change in exactly the same direction, I at least need to use a way to change a collection of human beliefs in a similar direction.

% In other words, this can be said to underscore the plausibility of the belief that ``a hypothesis is true'' with the extremely robust belief that ``inductive reasoning is valid.'' These robust beliefs seem to have been shaped through the process of evolution for the long time, being rooted in humans biologically. It seems that because science appeals to such fundamental beliefs inherent in humans as biological entities, its verification manages to convince many people.
% are rooted in our biological structures and perceptions acquired through the processes of evolution and development, as briefly introduced in the preceding section. For instance, beliefs such as the validity of logical reasoning, the uniformity of nature, and the tendency to find something more credible with an increase in observations fall into this category of beliefs. 

% And I believe that I achieve this by reducing hypotheses to strong convictions that everyone, regardless of individual differences, possesses as human beings. For example, assumptions underlying inductive reasoning, as I described earlier, will fall into this category. Another strong conviction humans believe in is that the more I observe an increasing number of results derived from a certain hypothesis, the more reliable and certain that hypothesis feels. Research need to be objective because it's required to generate knowledge for humanity, and I believe it is because I strive to reduce hypotheses to such strong convictions that it is called objective. 

\subsubsection{Knowledge for Non-Humans}
If the act of knowing is dependent on the subject, then it's theoretically possible to consider non-human knowledge and non-human research by assuming knowing subjects to be non-human. 

Although it might seem insignificant since most people desire AI that produces knowledge for humans, I believe the observation that what is unknown to a machine may differ from what is unknown to humans provides implications for what we should do to realize an AI capable of conducting research.

When what is unknown differs between humans and machines, creating a machine that can research in the same way as humans may not necessarily produce new knowledge for humans. This is because some of the methods developed by humans are aimed at revealing truths unknown to themselves, and even if a machine can mimic these methods, it would only reveal truths that are unknown to the machine, which is not necessarily unknown to human.

Considering this, if the goal is to create AI that produces knowledge for humans, then AI should either be used as just a tool to assist human research, develop new methodologies that discover the unknown for humans through machines rather than merely mimicking human methods, or, if the AI is allowed to research autonomously, there might be a need to teach it what is unknown to humans. These are merely a list of possibilities, but it is hoped that there will be further discussion in the future about whether these could truly become issues.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Truth-Conducive Justification Developed by Non-Human Agents?}

Even though beliefs are indeed subjective, in research, justification is expected to be conducive to truth, thus making justified knowledge objective. That is, whether a proposition is true or not determines whether that belief is justified, regardless of what we think.

I admit that we cannot ``prove'' that our empirical justification methods are strictly truth-conducive. However, considering the numerous discoveries we have made through them, it would be pointless to doubt the legitimacy of them. If AI can fully master and appropriately utilize these justifications developed by humans, it would be enough for them to uncover numerous unknown truths. Few researchers would likely doubt the importance of this.

Then, what about when the agent itself constructs its own methods of justification? Is it possible for an agent to construct new truth-conducive justification methods on its own instead of just mastering human developed methods? Human beings, for example, have developed statistical hypothesis testing as a method for evaluating hypotheses, so can AI similarly develop its own unique methods for evaluating new hypotheses? Even if it were possible, how ``significant'' would those be?

Our justification is based on several premises. This means that we make an interpretation of ``in what sense a justification is truth-conducive'' or ``what is justification'' in some sense, and depending on that interpretation, there actually exist multiple methods of justification even in the case of humans \cite{otsuka2022thinking}. Therefore, when allowing artificial agents to autonomously construct up to the method of justification, the agent is expected to make value judgments about ``in what sense a justification is truth-conducive'' and ``what is justification,'' conceiving and selecting the optimal method of justification. 

% In the case of humans, who have bodies formed through interaction with nature in the process of evolution and development and a belief system based on these bodies, it seems that they have already acquired a value system useful for understanding nature. Whether machines, which do not possess such a value system, can construct such methods of justification is an open problem.
% \footnote{
% For example, validating a hypothesis through statistical hypothesis testing essentially means believing that the inductive reasoning, which is a premise of statistical hypothesis testing, is valid. Using inductive reasoning implies believing in its premises, such as the principle of uniformity of nature \cite{sep-induction-problem}. Of course, there is no guarantee that this principle always holds, but it is such a natural assumption that has been shaped by nature through evolution and development and doubting it would make living practically impossible.
% }
% \footnote{
% I naively assume that beliefs rooted directly in perception are more robust, and the notion that anchoring a belief on those foundations enhances its reliability. However, the question of how to justify beliefs is a complex problem with extensive discussions \cite{sep-epistemology} (my position seems to align somewhat with what is called \textit{empirical foundationalism}).Due to my limited philosophical knowledge and the ability to delve deeper into philosophical discussions, this paper will not further pursue these arguments.
% }

% \textcolor{red}{may remove the part of nature}


This is not merely a philosophical inquiry. This is because, as long as we do not understand what constitutes the most truth-conducive justification, there is a possibility that better methods of justification may exist. Moreover, machines, not bound by cognitive constraints like humans, theoretically have the potential to discover such justifications. Particularly, the quality of a truth-conducive justification can be determined regardless of whether it involves humans or machines. Therefore, in the sense that its quality can be determined without human judgment, there is a possibility that machines could autonomously construct it. If this were to happen, it might enable the production of knowledge that humans alone could not have achieved. However, whether these are realistic, how they can be realized, and how significant they actually are, remains an open question.

\subsection{Conclusion}
In this section, I have discussed a provisional working definition of research. I started with the naive intuition that research is the endeavor to generate new knowledge for certain society. I then speculatively explored the ideas that knowledge is belief and the production of knowledge involves updating beliefs. Based on these discussions, I presented a conjectural discussion around the implications for non-human agents to conduct knowledge production.

The definition discussed here is just a starting point. By combining insights from philosophers, scientists, and all working researchers, we can engage in a deeper analysis of the definition of research, developing more fruitful and reliable guidelines for our goal.

\section{Question, Hypothesis, and  Verification}
\label{section-question-hypothesis-verification}
% \textcolor{red}{question, hypothesis, verification を並べて、
% context of discovery と context of verification だとどこがまとめられるか
% question answering だとどこがまとめられるか
% みたいなのをまとめた図はあってもいいかもしれない}
In the previous section, I examined the conceptual definition of research and its implications. In this section, I will discuss the widely recognized essential elements in research: question construction, hypothesis generation, and hypothesis verification. What I plan to discuss is how these can be characterized, what roles they play in the production of knowledge, and what it could mean for artificial agents to perform them. At the end of this section, I also briefly explore how we combine them and common topics that connect all of these. Throughout this section, I aim to advance the discussion more concretely than in the previous section on what research is and what it could mean for machines to be capable of it.

% 問いの構築、仮説の生成、仮説の検証が大事なのはパッと聞くと当たり前なので、なぜわざわざ言ってるのかを書く。

% \textcolor{red}{TODO: more focus on the implication for research automation}

% It is believed that research began with individual and concrete tasks. Among them, common actions were patterned and crystallized as a scientific method. I currently recognize this abstract set of behaviors as research. For example, hypothetico-deductive method and hypothesis testing are abstracted scientific method.

% Also, researchers use a research paper as a medium of knowledge transfer. Therefore, there are patterned activities related to a research paper. Examples of these include conducting surveys, gathering information from papers, and writing a thesis.

% Note that these are necessary tasks just because I use a paper as a medium of knowledge transfer, but they may not necessarily be indispensable for generating new knowledge. There are other such tasks as well. For example, peer review and fund raising are essential to current research practices in society, but they may not necessarily be indispensable for knowledge production.

% In this way, various tasks arise in conjunction with research. When considering the automation and optimization of research, it is desirable to consider streamlining all of these tasks. However, in this article, I focus on the process from determining a research topic to publishing a research paper. I will refer to this process simply as the \textit{research process} from here on.

% \subsection{Overview}

% As mentioned earlier, research is an attempt to turn the unknown into the known. Therefore, the research process can be seen as a function that takes the unknown as input and outputs the known. However, in reality, a single research paper may not be enough to turn the unknown into the known. Therefore, in practice, the research process is considered to be a procedure that takes the unknown as input, and outputs a text that describes the procedures and their results, as well as their interpretation, in order to turn the unknown into the known.

% First, let us structure the common research process. In particular, I will base the structuring of the research process on the method of empirical science, which many researches rely on as a foundation. However, I believe that this framework can be applied to other research activities, such as mathematics, as well. I will explain the reason for this later.

% The research process, especially that of empirical science, is carried out through the following steps: topic decision, hypothesis generation, verification design, verification, and analysis of experimental results. The outputs of these steps are then written into a paper, which undergoes peer review and is eventually published.

% Note that some commonly seen items, such as surveys, are not included here for a reason. First, as mentioned earlier, gathering information from papers is only a means of knowledge transfer through the use of a thesis. Second, information extraction from papers can be done at any stage of the research process. Thus, I believe that processing related to a paper, such as \textit{reading papers} and \textit{writing a paper}, needs to be considered separately from the aforementioned research process.

% \subsection{Overview}

% \textcolor{red}{TODO: reconsider the research process, structure of knowledge production system, and the scope of this paper}

% The definition of research stated in this section is somewhat too abstract to serve as a concrete guideline for building something based on it. Moreover, it feels distant from the research practices employed by humans, making it challenging to immediately connect our standing point to with the goal. While the ultimate goal is to achieve a fully aunotomous system for conducting research, it is practical and useful to start by discussing how to realize individual sub-processes within the research process. Breaking down the research process into partial processes will make the functions that need to be achieved much clearer.

% Therefore, I view research process as a \textit{knowledge production system} and attempt to structure the process by decomposing it into its constituting modules. During this division, each sub-process will be structured with the level of abstraction required for all types of research. This is because the focus of this paper is a general artificial researcher. 

% Hence, even if an element seems crucial in current research, if it is not necessarily essential to the definition of research seen so far, I will treat those elements separately from the constituents of the research process in this study. By separating these elements, I intend to clarify the indispensable components for realizing an artificial researcher. 

% \subsubsecti on{Outline of the Structure}
% In this section, I will discuss the three things tightly related to the knowledge production. I will first discuss the functionally essential elements for knowledge production system, which is the abstract structure of the research process that has been emphasized so far. These elements correspond to the modules in knowledge production system. This is a main topic of this chapter. 

% For convenience, I will refer to the entire structured research process as the \textit{knowledge production system} or just \textit{research process} throughout the rest of this paper.

% In the previous chapter, I discussed the definition of research. In this chapter, I will focus on the high-level abstract structuring of the research process while paying attention to its functional aspects. By emphasizing the functional aspects, I mean paying attention to the role that each step plays in knowledge production. By higher-level abstract structuring, I intend to focus on the processes that is as universal as possible across research fields, regardless of the specific domain. The purpose of this kind of structuring is to clarify what kind of modules should be created as intermediate steps of research when aiming for research automation. In the following, I will structurize the research process into a chronological sequence for the purpose of clarity. However, it is important to note that the focus lies not on the temporal order nor human convention, but rather on the functionality in relation to knowledge production and the inputs and outputs of each of these processes. Also, as previously mentioned, because humans are currently the primary knowledge generators, there are many constraints that come from human society. Thus I will do my best to distinguish and organize what is dependent on humans and what is not. Before delving into specific discussions, let us first explain the scope of this chapter. After that, I will discuss the outline to be addressed in this section, followed by the main discussions.


% \textcolor{red}{TODO: add the excuse that research is social activity}



% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=\textwidth]{figs/researchprocess.jpg}
%     \caption{Caption}
%     \label{fig:research_process}
% \end{figure}


% Next, I will discuss a high level description of how human beings have been conducting research. I'll structurize the abstract pattern of the process (which I will call\textit{ research process}) from determining the unknown to it turning into known. 


% \subsubsection{Note}

% note, direction
% Though my structuring may seem to represent scientific methods, I believe this pattern cam apply to other research fields, such as mathematics and humanities as well. When describing the structure, I will make a conscious effort to clearly distinguish between essential elements for knowledge production and those that are not. As previously mentioned, because humans are currently the primary knowledge generators, there are many constraints that come from human society. When considering the possibility of machines conducting research in the future, it will be important to distinguish and organize what is dependent on humans and what is not.

% I believe that the conduct of human research activities can be roughly divided into three stages: knowledge production, knowledge evaluation, and knowledge sharing. 

% Although these may not necessarily be distinctly separable from each other, I adopt this classification because it is useful for advancing discussion. The process of knowledge production consists largely of the steps: problem determination, hypothesis generation, and hypothesis verification. And in this process, the ability to read and write documents and analyze data are required as necessary skills. Below, I will examine each of these in more detail. 


% This structuring is tentative and there may be a better way to structure the research process. However, I have created this structure for practical purposes in order to move the discussion forward. I hope the structure of this article be a starting point for conceiving a better structurization. I believe that structuring and deepening understanding of the elements that are essentially important for knowledge production is extremely crucial when aiming for the automation of research.

% Though I explained that research is belied revision, it would be convenient to see as a function that takes the unknown as input and outputs the known.  % TODO: rearrange

% \subsubsection{Three Sub-Modules in Knowledge Production System}
% In the following sections, I will explain three essential elements that I think are functionally necessary in knowledge production: \textit{question construction}, \textit{hypothesis generation}, and \textit{hypothesis verification}. These are the abstract structure of the research process that has been emphasized so far.


% \footnote{
% While the term ``verification'' is used here, it does not imply a definitive determination of the truth or falsehood of propositions. It is used in the sense of just strengthening beliefs as used in everyday language. Thus, ``confirmation'' may be a more accurate term, but verification is more widely recognized, so I will use that.
% }

% When summarizing these intermediate steps, it goes as follows: First, upon receiving any input, I formulate a question. Taking this question as input, hypotheses are generated. Finally, these hypotheses are input and validated resulting in the creation of knowledge as a research outcome. These processes are considered a proper decomposition of the research process, as they request each other's outputs as inputs and seamlessly connect the entire research process from input to output without any gaps.
% \textcolor{red}{TODO: However, I can identify discovery with justification because both are belief updates. I'll add this point wherever in this paper.} Fig. \ref{fig:knowledge_production_system}.

% \textcolor{red}{TODO: Add fig like this. add this sentence to the explanation of the fig "we approach the unknown to the known by asking questions, providing tentative answers to them, and then verifying the validity of those answers. " }.
% Fig. \ref{fig:research_process}.

\subsection{Question Construction}
\label{section-question-construction}

\textcolor{red}{← 問いの生成のところで関係するかも（Distributed Science - The Scientific Process as Multi-Scale Active Inference）}

The first element is \textit{question construction}. In order to produce new knowledge, one must be aware of what they don't know and strive to generate that unknown knowledge. This process of deciding the unknown to be investigated can be regarded as questioning. And generating the candidates of the answer for that question is hypothesis generation. That is, research may be rephrased as the act of posing questions and answering them. Moreover, as long as there is an attempt to reduce uncertainty, questions are necessary, and we generate multiple questions in the process of research other than the research question. Thus, in the act of research, which confronts the unknown, question construction is inevitable.

In relation to efforts concerning the generation of questions by machines, there are studies aimed at discovering research questions and challenges from academic literature \cite{lahav2022search,liu2023creative,oppenlaender2023mapping,surita2020can}, and others focused on identifying research trends \cite{krenn2020predicting,krenn2022predicting}. However, efforts to let machines autonomously generate questions on their own are not as prevalent. In the field of question answering, there is a task for generating questions \cite{pan2019recent,zhang2021review}, but these studies have different motivations than generating research questions. There has been research on creating artificial curiosity to generate non-textual questions \cite{schmidhuber1991possibility}, but there is yet nothing that can generate research questions like a human. With the advent of large-scale language models in recent years, there have been attempts to generate research questions \cite{liu2023creative,lahat2023evaluating}, but this field is still in the early stage.

While the attempts for automation exist, compared to the those for generation and verification of hypotheses, these efforts are limited. Automating the construction of questions, or setting the goal behind, is recognized as a challenge that needs to be addressed in research automation \cite{coley2020autonomousII,zenil2023,kitano2021nobel}. In this section, I would like to start by speculatively exploring what questioning would be. Then, I would extend the discussions to open problems to realize an agent to pose questions.

% As for attempts to formalize the discovery of problems in the field of machine learning, there is work by Zhang \cite{zhang2021problem}. On the other hand, in this section, we aim to examine the characteristics by qualitatively emphasizing aspects related to the generation of research questions.

\subsubsection{What is Questioning?}
Asking questions seems to be characterized as an information-seeking behavior \cite{watson_2021,taylor1962process}. The act of seeking information is considered to consist of two steps: recognizing an \textit{information need} and then taking action to acquire that information \cite{wilson1997information,case2016looking}. Not all information-seeking behaviors involve linguistic expressions \cite{watson_2021}, but in research, we always form a query expressed in text between the steps of the information need recognition and the onset of information seeking behavior. Especially, in research, the process of question construction generally seems to refer to the process leading up to this query formulation. Therefore, in this paper, we regard question construction as the process leading up to the query formulation. The subsequent process of seeking information will be treated as part of hypothesis generation and verification.

The process of recognizing an information need appears to involve at least two sub-processes: recognizing the missing knowledge and deciding to fill it (judging that missing information is ``need'').\footnote{
Here, I explained the process of recognizing information need as first determining something as unknown and then deciding if you construct question about the unknown. However, the order doesn't matter. For instance, there may be knowledge that you want to know first, and then you confirm subsequently that it is indeed unknown. What matters is that the process involves these two elements.
} Therefore, to have an agent to autonomously construct questions, it would be necessary to consider how to instill these abilities to it. In the following sections, I will engage in speculative analysis and investigation of these steps.\footnote{
Please note that the question in research is not a personal one, but a question for society. This difference can bring about slight variations in the process of constructing questions. I will touch upon this point again later.

}
% 情報ニードを認識するという過程は、少なくとも、未知を認識することと、それを埋めことを判断する（欠落した情報がニードであると判断する）という二つの過程を伴うように思われる。したがって、これらの過程について検討する。

% Questioning seems to be an act of trying to fill a gap in the information that the questioner possesses. For example, a philosopher Watson defines a question as an information seeking act \cite{watson_2021} and curiosity, a concept related to questioning, is defined as intrinsically motivated information seeking \cite{kidd2015psychology}. In that sense, the act of trying to produce missing knowledge for humanity is the very act of posing and answering a question. In this paper, we provisionally consider that constructing questions in research is an act of seeking missing knowledge.

% The process of questioning seems to involve two steps: 1. Recognizing missing information, and 2. Attempting to fill that gap. For example, the process of asking why the sky is blue starts as follows: when prompted by, for example the vision of sky to your eyes, you suddenly (with complex perceptual and cognitive processes) realize that you don't know why the sky is blue, and then, if you would like to know the answer, utter the question, ``Why is the sky blue?'' Therefore, to have an AI to construct questions, it would be necessary to consider how to instill these abilities to the AI. In the following sections, I will discuss these steps.\footnote{
% Here, I explained the process of questioning as first determining something as unknown and then deciding if you construct question about the unknown. However, the order doesn't matter. For instance, there may be knowledge that you want to know first, and then you confirm subsequently that it is indeed unknown.
% }

\subsubsection{Recognizing Unknown}
To recognize that certain knowledge is unknown to you, it seems necessary to first attempt to refer to that knowledge. It appears that when we refer to our own knowledge base and do not find the knowledge, we judge it to be unknown. For an individual, the knowledge base is the memory within the brain. On the other hand, the unknowns researchers wish to clarify are unknowns to a certain society. That is, a research-capable agent does not need to judge whether it is unknown to itself, but rather it can directly determine whether it is unknown to certain society. Therefore, the knowledge base can be not only its own memory but also the knowledge base of the society, e.g. a collection of research papers.\footnote{
As previously mentioned, being unknown means that a proposition either does not exist or, even if it does, it is not accompanied by a justified belief. Therefore, it seems that in order to precisely determine unknowingness from academic papers, it seems to be necessary to judge whether each paper has been justified, that is, whether verification has been appropriately conducted. 
}

\textcolor{red}{論文のコーパス（のグラフ表現とか）からまだ探索されていない領域を同定する研究は絶対あるので、載せる。あとは、論文の取得を自動でやってくる研究についてもここで述べる。}

As stated in Section \ref{section-what-is-research}, if we want machines to generate new knowledge for humans, they must identify unknowns for humans, not for themselves. This might be as simple as conducting a literature survey, as previously mentioned, but it might require more than that. What is needed to achieve this seems to be a topic for further discussion.

There is also a question as to whether we should consider something as unknown to us if a machine determines it to be unknown based solely on its own memory. Such judgment may sound reliable if the AI was pre-trained with all scholarly papers. However, as mentioned before, just because AI deems something unknown, it doesn't guarantee that it's unknown to humans. This topic would become increasingly realistic and important as AI accumulates more knowledge and becomes capable of handling more intelligent tasks.

% philosophy of literature survey \cite{schryen2015theory}
% GPT-3 learn to quantify the uncertinty of its answer \cite{lin2022teaching}

\subsubsection{Deciding What Knowledge to Seek}
\label{section-deciding-what-knowledge-to-seek}
We do not always formulate questions for everything we don't know since not all unknowns are equally ``important'' or ``interesting'. Instead, we construct questions for things we wish to know the answers to. This means that we assess some ``value'' of questions using some criteria to determine if it is worth pursuing. For individual, this can be an unconscious internal process, but for research, this doesn't need to be so as long as it is some value judgment process.

Knowledge in itself is value-neutral. The ``value'', ``significance'', or ``goodness'' of knowledge is determined by those who using it. The crucial point here is that the criteria for determining ``value'' are arbitrary. Therefore, if we want agents to autonomously pose questions that are meaningful to humanity, we should recognize what is ``good'' or ``significant'' questions for us and instill and make them adhere to such values. 

However, we should also be mindful that there might be questions that we would judge as ``unimportant'' but are actually ``important'' under some criteria. For instance, there is a myriad of knowledge born from fundamental research that, at first glance, might seem ``useless'' but actually leads to later innovations. Due to human cognitive limitations, there might also be instances where we cannot fully assess the utility of that knowledge. Additionally, in human society, sometimes social factors unrelated to the original purpose of the knowledge production influence value judgments. That is, our value judgments are not necessarily always optimal.

Since machines are not bound by such constraints in theory, there's a possibility they can make better value judgments. Therefore, while it might be essential to provide some minimal guidance to ensure they generates questions meaningful to humans, it would be good to aim for the development of agents that can autonomously construct good value criteria by themselves. How to create such agents seems to be one of the important open problem in building an agents capable of research.\footnote{
Kitano referred to the science in which humans adopt their own value judgment criteria to determine questions and hypotheses as \textit{value-driven science} \cite{kitano2021nobel}. He argued that advancing \textit{exploration-driven science}, which focuses on more comprehensive and thorough exploration rather than criteria based on specific human values, is important for societal development. Although a completely value-neutral system would be impossible, I agree with the idea that employing new and diverse criteria would matter for future research. By adopting more diverse and extensive criteria, we could expand the exploration space of knowledge.
} 

\subsubsection{Origin of Information Need}
I explained that the question begins with the recognition of an information need. So, what causes the recognition of an information need (or the sub-processes of the recognition of unknowns and the judgment of value) in the first place?

Various factors can be considered as triggers. Some people may generate research questions as they logically think about how to achieve a goal. Others might come up with a question upon noticing some anomaly while observing experimental data or realizing that there might be something wrong with the underlying assumptions based on a discrepancy between results deduced from some assumptions and actual observational data. Furthermore, in reality, humans do not evaluate based on a single criterion of value judgment. It seems that they combine these criteria in a complex manner, weighting them according to the situation, before arriving at a final value judgment.
If one wants to create an agent that can autonomously construct research questions like humans, it seems necessary to develop an agent with a general methodology that can construct questions in any of these situations.

How these various factors are connected to information need in humans, and how we can create an agent capable of this, remains open question. Researchers studying curiosity have been tackling this difficult problem. Curiosity, while it is difficult to precisely define, is viewed as \textit{a drive state for information} \cite{kidd2015psychology}. In this sense, curiosity can be considered as something that gives rise to an information need. Indeed, especially in the process of information-seeking, it is characterized as one of the precursors of information need \cite{case2016looking}. Efforts to instill curiosity \cite{schmidhuber1991possibility} or knowledge-based intrinsic motivation \cite{oudeyer2007intrinsic} in AI have been researched in the field of reinforcement learning. There, curiosity is formulated as novelty, information gain, or prediction error, and is perceived as something that encourages exploration \cite{aubret2019survey}.

These efforts have provided guidelines on how to implement mechanisms that drive intelligence towards questions. However, we are still halfway to realizing a system that autonomously construct research questions under complex value judgments in various situations, as humans do. Especially, while a question becomes the minimum input in hypothesis generation, and a hypothesis in hypothesis validation, in question generation, it's unclear what the minimum input should be. Need to design a complex internal driving force adaptive in various situations seems to be a major difficulty for creating an autonomous research questioner. Identifying what is necessary to realize such AI is an open problem.
% これらの取り組みは、知能を問いに突き動かす機序をどのように実装するかについての指針を与えてくれました。しかし、依然として人間のようにあらゆる状況で複雑な価値判断のもと研究対象となるような問いを自律的に生成するシステムの実現にはまだ道半ばです。特に、仮説生成であれば問いが、仮説検証であれば仮説が最低限の入力となるのに対して、問いの生成ではそのような最低限の入力が何か分からず、自律的に問いを生成するには内的報酬のようなうまい機序を作らなければいけない点が大きな困難であるように思われます。このような AI を実現するためには何が必要なのかを明らかにしていくことは重要なオープンプロブレムの一つです。

% One of the challenges in trying to create an autonomous research question constructor is determining how much input humans should provide to the AI and to what extent humans should design the mechanism for constructing questions. For example, the minimum input for an AI constructing questions for a goal is the goal itself. However, a goal might not be the minimum necessary input for other types of question generation methods. Also, as is often discussed in the context of research automation, if one aims to make the AI autonomously generate even goal itself, we would likely necessitate assuming even more primitive inputs for it, e.g. higher-level goal.

% For creating an autonomous artificial researcher, it seems important that these are driven by intrinsic motivations. However, it doesn't necessarily have to be intrinsic motivation in the commonly understood sense. In the context of research, intrinsic motivation often seems to be associated with feelings like ``interest.'' But, as we'll discuss later, the motivations for posing questions can be diverse. For instance, questions could be framed for the purpose of optimizing some externally set metric. Therefore, if we narrowly define intrinsic motivation in this way, it might be argued that an AI capable of research doesn't necessarily need to have curiosity.

% Of course, if we take a broader view of intrinsic motivation, considering it akin to the evaluation function an agent possesses, then an autonomously inquiring AI is essentially the same as a curious AI.


% Research do not always have to be intrinsically motivated.

% curiosity is intrinsically motivated information seeking \cite{kidd2015psychology}

% theory that people decide what information to seek by considering the positive and negative effect of it to their action, affection, and cognition \cite{sharot2020people}

% curiosity -> exploration \cite{oudeyer2018computational}

% book of intrinsic motivation \cite{baldassarre2013intrinsically}



% This is because the essential requirement for research is that the answer to the question is unknown, and in principle, I do not require additional properties question to have. For example, you can ask about anything if it is unknown, or you can choose to ignore it if it's not intellectually intriguing. Alternatively, you can opt for something that seems to lead towards a specific goal you are pursuing. This has a critical implication for aiming to automate research. If the value standards are always given by humans, then there is no issue. However, when considering the possibility of automating even that aspect, it becomes necessary to discuss how I can make it adhere to the desired criteria. While it is natural to want them to ask ``good'' or ``important'' questions, it also becomes challenging when it comes to thinking about this issue. Let us discuss this further in below.

% I claimed that the value judgement is arbitrary. This is because the essential requirement for research is that the answer to the question is unknown, and in principle, any question is valid. In light of the definition of knowledge production, value judgement is not necessarily required for knowledge production to be as it is. From the perspective of knowledge production, as long as the unknown is truly unknown and it be rigorously approached towards becoming known, there should be no problem. The unknown can be anything arbitrary, and knowledge production itself does not demand a specific nature for it. 

% % \subsection{Relativity of Value}
% Knowledge in itself is value-neutral. The value, significance, or goodness of knowledge is determined by those who using it. 


% The value of knowledge is inherently dependent on context, so the significance or goodness of a certain knowledge is not determined a priori from the moment it is generated. Goodness becomes an issue only when that knowledge is used by some member of the society in some form. In other words, the demand for importance and goodness is a constraint imposed by society not by knowledge production. For example, certain knowledge may be considered ``important'' in the sense that it addresses the interests of all the researchers working on similar topics, providing solutions to their concerns. Alternatively, that knowledge might be deemed ``important'' to someone simply because they find it intellectually interesting. On the other hand, if it takes a considerable amount of time for this knowledge to translate into practical applications, it may not be considered ``important'' to someone who wants to start a business immediately. Hence, this is the choice of us (or them) on what objective I would like to maximize by knowledge production.\footnote{
% The fact that the value is arbitrary and not necessary condition for knowledge production doesn't mean I do not have to discuss about the value. In the first place, it is inherently impossible for all actions to be value-neutral. So it is inevitable for us to conduct some value judgement. Moreover, the realm of possible unknowns is too vast, so without any constraints, only nonsensical questions would arise. Most of us are interested in ``good'' or ``important'' questions. So, what I should consider is how to identify them and how to instill them to machines.
% }

% This provides important implications when attempting to create an artificial intelligence that autonomously conducts research, at least in two aspects. Firstly, if I am interested in making machine autonomously construct ``good'' question for humans, I should consider what is ``goodness'' for humans and how to instill them to machines. However, it seems that I have not yet fully established a unified common understanding of what constitutes a "good" question. ``What makes a question good for humans'' and ``how to formulate them effectively'' are crucial aspects that require further in-depth discussions. Therefore, I will revisit and explore these important points separately later on.

% It seems that I still lack an understanding of what kind of research questions are important, but in the field called \textit{science of science} \cite{wang2021}, which studies the research itself, scientific studies on research impact are also advancing. The insights gained here may provide important knowledge in building new algorithms and optimization metrics.

% The second implication when attempting to create an artificial researcher is that if I do not intentionally impose any constraints, there is a possibility that the intelligence produced may not align with the knowledge I desire. This is an important point and thus will be discussed in a separate chapter.

% Secondly, it suggests the possibility of adopting values that are different from the ones I currently employ can result in better knowledge production. I have explained that I make judgments about certain questions being good or important based on some criteria or standards. However, there may be questions that are not considered ``important'' according to current criteria but actually be extremely significant. As mentioned earlier, the value of knowledge is determined by its usage and context, and it can vary over time and in different environments. Therefore, it is highly challenging to determine the importance of knowledge during the stage of knowledge production. Even in the same environment, it is difficult to assess the significance of knowledge. This is because knowledge results from complex accumulations, leading to new insights, and there is a intricate chain of connections before a particular knowledge becomes recognized as important within a society. In addition, I am bound by various cognitive limitations inherent to being human. Therefore, I can only assess the importance of knowledge within the confines of these limitations. 

% Given these circumstances, it is highly possible that the current adopted criteria for value judgments are missing out on the production of potentially important knowledge. The development of knowledge production systems that embrace new value judgment criteria can be expected to increase the potential for generating such knowledge by expanding the scope of exploration. If an artificially intelligent system capable of autonomous research is developed, it can be expected that research based on these new criteria will become more feasible. This could potentially enable the resolution of many previously unsolved problems that were not attainable before.

% As a conclusion, let us emphasize again that I first need to be aware that these ``good'' aspects do not occur naturally. To create an intelligence that constructs ``good'' questions, I first need to understand what I consider a ``good'' question. Also, it's important to turn my attention to things that are not currently considered ``good,'' but should be deemed as ``good'' in essence. Only then can I discuss how to align that value with the agent. Therefore, I think I should start by listing the criteria for determining the ``goodness'' of a question. 

% For this, discussions in the philosophy of science and meta-sciences like the Science of Science may be referenced. Alternatively, large-scale surveys of researchers engaged in actual research could also be important. Once the value is clarified, I might be able to think about creating an intelligence equipped with these values using the value alignment techniques that are currently being developed.

% Science based on the importance of questions discussed above is a \textit{value-driven science} \cite{kitano2021nobel}. However, as previously mentioned, these may be due to cognitive constraints imposed by human society, including the inability to handle knowledge that is deemed ``unimportant.'' Therefore, when automating question generation, it may be possible to explore a wide range of questions, including those that were previously considered ``unimportant.'' In doing so, it is possible that knowledge that was not considered ``important'' according to previous criteria could actually be extremely significant. This is referred to as \textit{exploration-driven science} \cite{kitano2021nobel}, and it could become a new form of research liberated from constraints imposed by humans. 

% Indeed, it is impossible to create a completely value-neutral system. All agent systems must have some form of bias. However, what I would like to emphasize is the potential to incorporate biases different from the criteria previously used by humans, and how this can enable us to consider more diverse approaches to research. This highlights the importance of creating agents capable of autonomously conducting research.

\subsubsection{What Are the Criteria for the Value of a Research Question?}

% \subsubsection{Diverse Good Research Questions}
So far, I have discussed somewhat abstract topics related to questions in general. Now, I would like to discuss the characteristics related to research questions specifically. 

The ``quality'' of a research question can be judged based on various criteria. I will introduce some examples from among them to make it clearer how humans seemingly have determined the value of a research question. Please note that the following examples are just a few of the many criteria humans use and are far from comprehensive. I hope this will aid in further deepening the discussion on how humans determine the value of a question.

The idea that questions which would bring about new perspective or understanding, or \textit{conceptual advance}, especially which would overturn our common sense or underlying assumptions are important is widely accepted within the research community. As an example, Alvesson and Sandberg point out the importance of these kind of questions and discuss the strategies to construct them \cite{alvesson2013constructing}. 
This criterion is based on the premise that a good question is one that produces knowledge which has a significant impact on our current body of knowledge.

No matter how significant a question is, if it's nearly impossible to address with current technology, producing meaningful research outcomes from that question may be infeasible. Therefore, some argue that the feasibility of answering a question should be considered when evaluating its quality, with questions that are not overly implausible being deemed good ones \cite{hulley2007designing,alon2009choose,huntington2021effect}. To determine feasibility, it seems that complex decision-making is necessary, as it requires consideration of various factors such as the resources and funding currently accessible, the capabilities of the researchers, deadlines, and even the limitations of the technology currently available to humanity. An AI capable of conducting research would likely need to be able to make such complex decisions.

The notion that research questions should be based on an individual's intellectual curiosity also seems to be widely accepted. Curiosity is the driver of exploration \cite{oudeyer2018computational}, so curiosity-driven research might have promoted exploration in the research theme space. Research can be seen as an exploration of the space of truths in this world, hence it sounds reasonable that value standards that promote exploration are important.
To discover things that are so unknown they are not even known to be unknown, exploration is essential as an entire endeavor of research.
Conversely, the criteria should not necessarily be curiosity as long as it promotes the exploration in the space of knowledge. The exploration may be merely a byproduct of curiosity, and there may exist better heuristics for exploration. If agents acquire such better heuristics or value judgement, they might be able to unravel truths more efficiently than humans have done.

% To construct such questions, we generate hypotheses by repeatedly engaging in question and hypothesis generation.\footnote{
% For instance, let's say we have a goal of ``creating AGI''. Initially, we pose the question, ``How can we create AGI?''. In response to that, we consider elements necessary to create AGI as our hypotheses. For example, we might think, ``Realizing an AI capable of logical reasoning, an AI with embodiment, etc., might be necessary.'' Then, we ask, ``How can we create an AI capable of logical reasoning?'' and consider hypotheses for that question, and this process is repeated.
% }

Contrary to research driven by bottom-up curiosity, the notion that questions contributing to the achievement of specific goals set in a top-down manner is valuable is equally common. For instance, for those aiming to realize human-like AI, questions about how to create elements deemed necessary for such AI would be significant for them, even if it is not interesting or incremental. Especially in corporate research or government-led research, there are likely many studies aimed at achieving goals set in a top-down manner. It is assumed that many people expect AI capable of conducting research autonomously to contribute to goals set by humans. Therefore, the ability of AI to make this kind of value judgment is an important requirement.

Lastly, there is also a perspective that emphasizes the value for individual. Alon expresses the view that a good research problem is one that is interesting to the individual and has an appropriate level of difficulty for them \cite{alon2009choose}.

In reality, instead of adopting just one of these criteria, we determine the value of the research question by comprehensively considering multiple criteria. For example, Hulley et al. suggest that questions that is feasible, interesting, novel, ethical, and relevant (FINER) should be considered good ones \cite{hulley2007designing}. Huntington-Klein presents the view that good research question is answerable and the answer to the question will improve our understanding of this world \cite{huntington2021effect}.

As already mentioned, these are just a part of the value judgments that humans make in determining questions. I hope that future discussions will delve deeper into what kind of value judgments humans make, what function they serve in scientific discovery, and how we can realize these functions in AI.

% Gap spotting vs problem solving \cite{alvesson2013constructing}

% Up until now, I have been explaining that values like goodness are relative and subjective. However, it is natural for artificial researchers to autonomously construct ``good'' or ``important'' questions. While I admit that adopting diverse value criteria and not being bound by traditional standards matters as Kitano said, even in that case I still have to determine a criteria that lead to ``good'' questions in some sense. Therefore, I would like to discuss what constitutes a ``good'' question for us and how I can construct such questions, drawing upon the discussion of how I have characterized ``good'' questions thus far.

% \subsubsection{Goal Oriented Question}
% One of the most general, significant, and widely accepted criteria for what I consider as ``goodness'' is that a question is deemed ``good'' if its answer contributes to achieving a desired, yet unrealized, goal. This is because researchers often seek to address long-term problems and engage in knowledge production for that purpose. For instance, physicist who pursue a unified theory would think that a question that furthers the realization of a unified theory is a good question. This can be referred to as a \textit{goal oriented research question}.

% In this approach, we first set the ultimate objective. Then, we identify the most critical bottlenecks, or sub-goals, that are essential to achieving that objective. We once again consider sub-goals for these identified bottlenecks. This process is repeated, converging on more specific and feasible sub-goals that are of high importance. Finally, I frame the question to address these sub-goals as the research question. 

% For identifying the knowledge required to achieve the ultimate goal, we typically start by listing the necessary elements to accomplish it. For example, to achieve general artificial intelligence, we may think that it requires the ability to handle language, understand the real world, be proficient in systematic reasoning, and align with human values, etc. 

% Then, we break down them again into the necessary things to achieve the them. For instance, to understand the real world, for instance, we may need the capability for interacting with the physical world, processing visual information, and so on. These requirements can be further broken down into multiple necessary elements. By repeating this process, we can narrow down the specific tasks that can be directly addressed. Then, the required knowledge to accomplish those tasks is demanded, and that's where it directly connects to the research question. The process is conceptualized in Fig. \ref{fig:unknown_tree}.


% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=\textwidth]{figs/unknown_tree.jpeg}
%     \caption{Caption}
%     \label{fig:unknown_tree}
% \end{figure}

% Several things are happening here. Firstly, listing the elements necessary for achieving the goal means generating sub-goals from the main goal. However, it's always a challenging problem to evaluate how a particular sub-goal contributes to the achievement of a given goal. Especially in the case of research, the target might be an too general and ambitious vision that nobody has achieved before, so I need to think about what needs to be done to break it down into appropriate sub-problems. In other words, it is necessary to construct a tree with nodes representing sub-goals. Namely, this is the tree of repetition of constructing a question and generating multiple hypotheses without verification. I will delve into the discussion of hypothesis generation in the next chapter, so I won't go into the details here.

% Secondly, it is necessary to identify the most important and feasible sub-goal from the selected candidate sub-goals. This is because only one question can be addressed in the end. However, assessing and comparing sub-goals is a challenging task since I have no experience to realize the ultimate goal and so have no data what sub-goal actually is the most important.

% Thirdly, the question to ultimately arrive at must be verifiable. If the question is not specific, meaningful verification cannot be performed. Overly broad or ambiguous questions can result in countless or trivial answers, or they may be too unclear to provide practical answers. Increasing the specificity of the question corresponds to deepening the depth of the sub-goal tree, so it may be important to construct a sufficiently deep tree and find an efficient way to navigate it. The verifiability is constrained by the knowledge, resources, such as funding and technology, that I currently have. Therefore, when conducting verification in reality, it is necessary to consider such feasibility. Whether to tackle a question with high feasibility or to further divide it into more sub-tasks for its realization is a matter of judgment. In any case, it is necessary to appropriately evaluate such feasibility. The scope of feasibility is vast, so it is a challenging problem to determine how to consider it in creating intelligent systems.

% In this discussion, I considered the method of outputting questions from the goal through the construction and exploration of a tree structure. However, as mentioned by the predecessors, if an end-to-end approach ultimately becomes a powerful method, it may be more desirable to consider a direction in which questions are directly output from the goal. In particular, even when performing multi-step reasoning, it seems more natural to improve reasoning abilities using the recently developed approaches to multi-step logical reasoning, rather than explicitly considering tree structures. 

% To pursue this direction in research automation, specifically, it may be worth considering the construction of higher-quality datasets for goals and research questions. For example, it may be possible to construct a dataset by extracting only the ultimate goal and the research questions actually solved from the introductions of papers. However, an important point to note here is that the research questions created by humans so far are not necessarily optimal for achieving research goals. Firstly, machines may be capable of maximizing the objective better than humans due to cognitive constraints. Secondly, not all human research has been conducted by working backward from a clear goal. Some studies were conducted simply because they seemed interesting while reading papers. In this regard, simply learning from human data may constrain the potential capabilities that machines can possess. Therefore, it becomes important to consider how to formulate the maximization of the probability of achieving research goals as a problem, rather than naive imitation learning of human data.

% As evident from the formulation, to construct research questions for contributing to achieve a specific goal, we need to solve long range reasoning problems. This problem is widely studied in machine learning research community to improve the reasoning capabilities of machine learning models and on generate intermediate goals in reinforcement learning. If these research fields produce significant results, they can be directly applied. In this sense, it might be beneficial to seek cooperation from those who are actively conducting research in these areas. One of the unique aspects of long-distance reasoning problems in research automation is that the goal is something that has never been achieved before. This means that you cannot naively learn from data and need to generalize out of distribution. Therefore, it's essential to acquire skills not just to recognize patterns but to properly trace the path of reasoning. Moreover, because the goal has not been realized, sub-goals and the paths that connect them are ultimately based on the accumulation of hypothesis generation. In this sense, it can be said that this is a highly uncertain inference. This implies that the choice of which node to select is far from self-evident compared to other logical reasoning problems. Furthermore, there is the issue of the complexity of the distance between the goal and the question, which is far more intricate than, for example, games or planning everyday trips. For instance, to truly achieve the goal, it may be necessary to build large-scale apparatus like particle accelerators from scratch. This also means that the temporal distance between the goal and the current location is very long. Therefore, it becomes a problem that feedback on how much solving the question contributed to the goal is significantly delayed. While we've only listed a few examples here, there may be other unique challenges and issues that become more serious in research. It will be necessary to work on refining these technical challenges into specific research tasks through discussions with researchers in reasoning and planning.

% \subsubsection{Important but Unnoticed Questions}

% One example of a ``good'' question seems to be one that, in its construction itself, brings benefits to many researchers. By considering why hypotheses regarding the question are unknown, it becomes somewhat clearer what kind of question this is.

% The reasons why the answer to a question remains unknown are diverse. In some cases, the question is simply new, and no one has had the time to come up with an answer yet. For instance, a question about the internet posed on the day it was invented may still remain unanswered because it has only been a day since its birth, and no one may have delved into it yet. In other cases, the question might simply not interest anyone, leading to an absence of answers despite the availability of time. This is an example of a question that remains unanswered because nobody is willing to tackle it, even though time exists.

% Furthermore, there are questions that are difficult, and no one has been able to solve them. For instance, the question of how to achieve human immortality has been contemplated by many, but due to its complexity, no answer has been found yet. Lastly, there are questions that are important for a particular purpose but remain unnoticed by everyone. As previously mentioned, realizing challenging objectives that are yet to be achieved poses difficult problems. In such cases, it is often unclear what is not known or what is at the heart of the problem. In such situations, clarifying the question itself holds great significance.

% Thus, constructing questions of this kind seems to be important as it can help shed light on the unknown. With this in mind, it appears worthwhile to discuss this topic in detail.\footnote{
% Please note that the concept of a question being unnoticed and the answer to the question being unknown are different. The necessary condition for research is that the answer to the question is unknown. 

% If the existence of a question is not known to anyone, then naturally, its answer would also be unknown since no one would have answered it. So, if the existence of a question is unknown, then the answer to the question is also unknown.
% }

% \textcolor{red}{TODO: Add discussion}

% \subsubsection{Diverse Good Questions}
% There have been various discussions on the elements that good research possesses. For example, \cite{hulley2007designing} proposed that good research question should satisfies FINER criteria (feasible, interesting, novel, ethical, and relevant) and \cite{alon2009choose} claims that a good problem is one that is most feasible and interesting to oneself.

% \textcolor{red}{TODO: Add more discussion on ``good'' questions, examples, discussion, what is good, what is important, specific question is good, etc.}
 

% \subsubsection{Where Does a Question Come from?}
% Research questions can arise in various situations. Some people may generate research questions as they logically think about how to achieve a goal. Others might come up with a question upon noticing some anomaly while observing experimental data. Furthermore, as with Kepler, one might realize that there might be something wrong with the underlying assumptions based on a discrepancy between results deduced from some assumptions and actual observational data. If one wants to create an AI that can autonomously construct research questions like humans, it seems necessary to develop an AI with a general methodology that can construct questions in any of these situations.

% One of the challenges in trying to create such a general and autonomous research question constructor is determining how much input humans should provide to the AI and to what extent humans should design the mechanism for constructing questions. For example, the minimum input for an AI constructing questions for a goal is the goal itself. However, a goal might not be the minimum necessary input for other types of question generation methods. Also, as is often discussed in the context of research automation, if one aims to make the AI autonomously generate even goal itself, we would likely necessitate assuming even more primitive inputs for it, e.g. higher-level goal.

% For creating an autonomous system, the question construction module faces challenges due to being the initial building block of the system. This challenge is determining ``what should be the input to the question construction module.''

% Questions do not arise from nowhere. There is always something before reaching a question. In the example I mentioned earlier, for instance, the question may arise as a result of a literature review. Then, why did you conduct that literature review in the first place? It could be because there is a research theme you want to know about in the research field. And then why are you interested in that research theme? It could be because the topic of the first paper you encountered during graduate school was fascinating, or it could be because you have been interested in it since childhood. And there may be causes behind those as well.

% In this way, identifying where a question begins is a hard problem. If you think seriously about it, it will lead to an infinite regress. This is a significant problem when I want to realize an autonomous artificial researcher. As infinite regress can occur, the decision of where to terminate lies solely with the designer, and it is not automatically resolved by creating the system. Can I say that question construction is autonomous if the literature to read were given? Can I say that question construction is autonomous if research theme were given? I believe it is necessary to accumulate such discussions and determine how far to consider something as given, in order to define what qualifies as autonomous.

% In this paper, I assume that a trigger that requests a specific type of knowledge is given. And from there, it makes decisions about determining the unknown and constructing questions. The reason is that discovering questions with unknown answers is a necessary condition for research, and I believe this is the minimal requirement for it. However, one could also try to automate even the aspect of requesting a specific type of knowledge. The reasons for expecting the existence of certain knowledge can vary and are arbitrary. For example, I might have an objective and first consider what I need to do to achieve it. I then anticipate the necessary knowledge to accomplish those tasks. This corresponds to the demanding for a specific knowledge.

% As an another example, let's consider the case of a child asking, ``Why is the sky blue?'' In this case, the child may already have prior knowledge of the concept of ``sky'' and ``blue.'' Additionally, they may possess a naive concept of causality, believing that ``A is B, so there must be a reason for it.'' Thus, they may have expected to have the knowledge that ``the sky is blue because of B.'' However, when they reference their internal knowledge, they find that it does not contain the corresponding knowledge. Therefore, they may have asked the question ``Why is the sky blue?'' to evoke the knowledge they were lacking. In this case, the required knowledge is ``the sky is blue because of B.'' and this is induced just because the result of children combining known concepts.

% The question of ``why do I seek information'' has been extensively discussed in the context of curiosity. Indeed, as I proceed with the automation of these components, it becomes essential to delve into research on curiosity. Regarding this matter, I will touch upon the aspects mentioned in Chapter 3, if possible. 

% Multiple Reasons for Unknownness

% new, unimportant, difficult, unnoticed, ... etc.

% This means that I distinguish between questions that are ``good'' and those that are not, based on certain criteria. 

% This means that I distinguish between questions that are ``important'' and those that are not, based on certain criteria. For example, \cite{alon2009choose} claims that a good question is one that solves challenges facing the research community. Likewise, I consider a question to be important if it generates knowledge that greatly contributes to a certain purpose. Valuing the degree of contribution to a purpose also implies viewing research as a form of problem-solving. \textcolor{red}{TODO: Add explanation of what this sentence means}

% Thus, in realizing an agent that autonomously constructs questions, it may become important to consider how to automatically determine ``goodness'' of the questions. To achieve this, it would be important to first understand in more detail what kind of questions I consider ``good''. \textcolor{red}{TODO: Add possible directions}

% \subsubsection{How to Practically Construct a Question}

% There has been much discussion on how to actually generate questions. Off course, these discussions primarily focus on how to formulate good questions. Therefore, please note that the examples mentioned here are proposals for generating such kind of questions.

% One typical approach to formulating a research question is to conduct a literature survey, identify research gaps in existing studies, and propose a question that aims to fill those gaps.

% \textcolor{red}{TODO: Add survey of how to construct questions; gap spotting, problemization, etc}

% Next, let's consider the process of constructing purpose-driven research questions. When aiming to conduct impactful research, I believe that constructing purpose-driven research questions is crucial. In this approach, I first set the ultimate objective. Then, I identify the most critical bottlenecks, or sub-goals, that are essential to achieving that objective. I once again consider sub-goals for these identified bottlenecks. This process is repeated, converging on more specific and feasible sub-goals that are of high importance. Finally, I frame the question to address these sub-goals as the research question. 

% In practice, I seem to determine the questions I should tackle in this way, implicitly and explicitly. For example, let's say that someone try to answer a question of ``How neural networks have reasoning capability?'' in his/her study. This question may come from a thought process of ``we want to create artificial general intelligence, which requires systematic thinking, that needs ...'' In this case, the final purpose is to achieve ``artificial general intelligence'', and the question addressed as a result is ``ow neural networks have reasoning capability?'' In other words, when I want to conduct important research, I follow a process that starts with the goal I want to achieve, considers the tree of important unknowns that should be clarified for its achievement, and sets the end of that tree as the research question. This process is summarized in Fig. \ref{fig:unknown_tree}.


% Of course, the purpose mentioned here may be a sub-goal of a higher-level goal. For example, the goal of ``creating general artificial intelligence'' may be a sub-goal of a more fundamental goal of ``satisfying intellectual curiosity,'' and the goal of ``satisfying intellectual curiosity'' may be biologically demanded for better exploration of the environment. These can lead to an infinite regression when considered strictly, so I won't delve into it any further here, but it could become an important issue when considering how to realize fully autonomous agent to construct questions.

% \subsubsection{Question}

% The construction of a question is the act of seeking information \cite{watson2015ask}. Specifically, in the context of research, I consider information as knowledge. The act of seeking knowledge involves two steps: 1. Recognizing the lack of knowledge and 2. Attempting to fill that knowledge gap. In this discussion, I assume that intelligence is designed to consistently generate questions when given input. Therefore, I temporarily set aside the aspect of "triggering action" related to the second step of attempting to fill the knowledge gap.

% The recognition of a knowledge gap occurs when I expect to have certain knowledge and, upon referencing my accessible knowledge, I find that it is not available. For example, when running a program and encountering an error that I cannot resolve on my own, I recognize that I lack the necessary knowledge.

% The reasons for expecting the existence of certain knowledge can vary and are arbitrary. In this case, I assume that a purpose given by a third party creates an expectation of certain knowledge. For example, in the case of humans, I first consider what I need to do to achieve a certain purpose. I then anticipate the necessary knowledge to accomplish those tasks, and when I find that it is not present within my existing knowledge, I recognize the knowledge gap.

% Lastly, in this discussion, knowledge refers to the collective body of research findings, particularly academic papers. In actual research, a researcher may personally have a question and then investigate previous studies to confirm that it is indeed unknown before formulating it as a research question. However, what is important in the construction of a research question is that it is unknown to other entities. Therefore, for simplicity, I directly refer to the entirety of academic papers without including the step of comparing personal knowledge.

% To summarize, to create an intelligence capable of constructing questions in this setting, I need to design it to expect the necessary knowledge to achieve a given purpose provided by a third party, search for that knowledge in academic papers, assess whether the papers contain sufficient knowledge to achieve the purpose, and express any knowledge gaps as questions.

% In this case, I excluded the discussion of triggering action by design. However, when considering increasing autonomy, it is important to discuss how to incorporate this aspect into learning and acquisition. The question of "why do I seek information" has been extensively discussed in the context of curiosity.

% Furthermore, in this case, I defined the expectation of knowledge as aiming to achieve a given purpose. However, as mentioned earlier, this does not affect the formulation of questions. For example, let's consider the case of a child asking, ``Why is the sky blue?'' In this case, the child may already have prior knowledge of the concept of ``sky'' and ``blue.'' Additionally, they may possess a naive concept of causality, believing that ``A is B, so there must be a reason for it.'' Thus, they may have expected to have the knowledge that ``the sky is blue because of B.'' However, when they reference their internal knowledge, they find that it does not contain the corresponding knowledge. Therefore, they may have asked the question ``Why is the sky blue?'' to evoke the knowledge they were lacking.

% In this way, the reasons for expecting the existence of certain knowledge can vary, and what, why, and how I seek information (knowledge) are not constrained by specific conditions. Therefore, when attempting to create an intelligence capable of constructing questions in the future, it is feasible to develop a more flexible intelligence.

% Additionally, in this case, I assumed that the given purpose and its achievement are predefined goals. However, humans naturally set their own goals. When considering the design of a more autonomous intelligence, it is conceivable to aim for automation in this aspect as well. However, as mentioned earlier, the question of what I seek knowledge about is not specific to research. Therefore

% , I temporarily set it aside for now. If I were to pursue this direction further, it would ultimately lead to an infinite regress, raising the question of how much information to consider as given.


%%%%%%%%%%%%%%%%%% Rearangement %%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{question construction}
% Research is an endeavor to bring the unknown closer to the known. Therefore, it is necessary to first determine what unknown I aim to make known. And this unknown often takes the form of questions. For example, ``Why do deep neural networks with a large number of parameters generalize well?'', ``How can I prevent the problem of vanishing gradients?'', and like these. These are commonly referred to as \textit{research questions} or \textit{research problems}. Therefore, in this paper, I will refer to the step of determining this unknown as \textit{question construction}.

% \textcolor{red}{TODO: should describe question construction itself first. What is research question or research problem?}

% \subsubsection{Unknownness}
% As I have reiterated, it is a necessary condition for research that the answer to a question is unknown, or in other words, that there is a high degree of uncertainty. Therefore, it is essential in research to have methods that ensure the answer to a posed question is truly unknown, or to formulate questions that truly have unknown answers.

% Knowledge for humanity is primarily disseminated through research outcomes. Therefore, when examining all the research outcomes that have been generated thus far and finding that none of them provide an answer to a specific question, it seems reasonable to conclude that the question possesses sufficient uncertainty to warrant further investigation as a research endeavor. In particular, humanity has developed the culture to preserve the research outcomes in the form of papers. Therefore, it seems feasible to assess the unknown nature of an inquiry by examining all academic papers. However, it is impossible to review them all due to constraints in terms of time, technology, and cognitive limitations. Therefore, it is realistic to consider a question as unknown if it has been sufficiently and comprehensively explored through an extensive examination of these academic papers. In practice, I conduct literature reviews to synthesize existing research, identify research gaps in existing studies, and thereby ascertain the unknowness of my own questions or construct question for which the answers are unknown \cite{schryen2015theory}. \textcolor{red}{TODO: Consider where I will explain about literature review}

% % In the previous statement, it was mentioned that as long as the unknown is truly unknown and it can be approached towards becoming known, there should be no problem. The process of approaching the known will be explained in the next section, and here I will delve a bit more into the determination of unknownness. 

% However, in reality, such rigorous literature research is not always conducted in every case. Currently, researchers often demonstrate the unknownness of the answer to the question by referencing only a few related works and explaining that none of them have yet resolved the unknown. And when the paper is evaluated by reviewers, who are a small group of experts, if it is determined that the question has indeed not been answered so far, the provisional recognition of the unknown nature of the question is granted. This means that a subjective evaluation criterion is being used, where researchers and a small number of reviewers consider a question as unknown when none of their known studies provide an answer.

% % This implies the use of subjective evaluation criteria, where researchers examine several papers considered ``major literature'' in a field and consider them as unknown if none of them have provided an answer. Furthermore, as mentioned later, I evaluate the quality of research outcomes by having them assessed by a small number of experts in the same field. If these researchers determine that the previous studies have been sufficiently comprehensive, the determination of unknownness is considered somewhat valid. In other words, ultimately, the evaluation by a few experts may serve as the basis for establishing the unknownness.

% This current convention stems from the cognitive constraint that there is a limit to the literature that humans can examine. Since unknownness is a fundamental aspect of research, ideally, it should be evaluated objectively and rigorously. For instance, it would be desirable to quantitatively state which journals, what types of papers, and how many have been examined, and the result indicating their unknownness. Although systematic reviews already employ such approaches, there is, of course, a limit to the number of papers that can be evaluated manually and selection biases cannot be removed. \textcolor{red}{TODO: Add the explanation of systematic review, problems of it, and how AI can mitigate them.}


% However, I have not found a unified consensus on the definition of good question. but 

% \subsubsection{Questioning as Information Seeking Behavior}
% \textcolor{red}{TODO: Reconstruct}

% The construction of a question is the act of seeking information \cite{watson2015ask}. Specifically, in the context of research, I consider information as knowledge. The act of seeking knowledge involves two steps: 1. Recognizing the lack of knowledge and 2. Attempting to fill that knowledge gap. In this discussion, I assume that intelligence is designed to consistently generate questions when given input. Therefore, I temporarily set aside the aspect of "triggering action" related to the second step of attempting to fill the knowledge gap.

% The recognition of a knowledge gap occurs when I expect to have certain knowledge and, upon referencing my accessible knowledge, I find that it is not available. For example, when running a program and encountering an error that I cannot resolve on my own, I recognize that I lack the necessary knowledge.

% The reasons for expecting the existence of certain knowledge can vary and are arbitrary. In this case, I assume that a purpose given by a third party creates an expectation of certain knowledge. For example, in the case of humans, I first consider what I need to do to achieve a certain purpose. I then anticipate the necessary knowledge to accomplish those tasks, and when I find that it is not present within my existing knowledge, I recognize the knowledge gap.

% Lastly, in this discussion, knowledge refers to the collective body of research findings, particularly academic papers. In actual research, a researcher may personally have a question and then investigate previous studies to confirm that it is indeed unknown before formulating it as a research question. However, what is important in the construction of a research question is that it is unknown to other entities. Therefore, for simplicity, I directly refer to the entirety of academic papers without including the step of comparing personal knowledge.

% To summarize, to create an intelligence capable of constructing questions in this setting, I need to design it to expect the necessary knowledge to achieve a given purpose provided by a third party, search for that knowledge in academic papers, assess whether the papers contain sufficient knowledge to achieve the purpose, and express any knowledge gaps as questions.

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=\textwidth]{figs/question_formulation.jpg}
%     \caption{question construction}
%     \label{fig:enter-label}
% \end{figure}

% \textcolor{red}{TODO: Is question construction information retrieval??}


% Asking questions is an act of seeking information \cite{watson2015ask}. The act of seeking information (or knowledge) arises from realizing the lack of one's own knowledge and the desire to fill that gap. Therefore, to create intelligence that can autonomously ask questions, it is necessary to incorporate mechanisms that induce these behaviors.

% Determining what triggers this behavior in humans is a challenging problem. However, when designing a system, it is sufficient if it can induce such behavior, regardless of what it actually is. The principle that ``behavior is triggered when it is somehow desirable for the agent'' represents this idea. You are probably familiar with this concept in reinforcement learning, where rewards (desirability of actions) are provided, and maximizing the expected value of these rewards is formulated as an appropriate way to induce behavior.

% The notion of triggering behavior by realizing the lack of one's own knowledge and attempting to fill it has been extensively studied in the context of curiosity in the field of reinforcement learning. In research, agents that pose questions can generally be formulated within this framework.

% \subsection{question construction}
% The construction of a question is the act of seeking information \cite{watson2015ask}. Specifically, in the context of research, I consider information as knowledge. The act of seeking knowledge involves two steps: 1. Recognizing the lack of knowledge and 2. Attempting to fill that knowledge gap. In this discussion, I assume that intelligence is designed to consistently generate questions when given input. Therefore, I temporarily set aside the aspect of "triggering action" related to the second step of attempting to fill the knowledge gap.

% The recognition of a knowledge gap occurs when I expect to have certain knowledge and, upon referencing my accessible knowledge, I find that it is not available. For example, when running a program and encountering an error that I cannot resolve on my own, I recognize that I lack the necessary knowledge.

% The reasons for expecting the existence of certain knowledge can vary and are arbitrary. In this case, I assume that a purpose given by a third party creates an expectation of certain knowledge. For example, in the case of humans, I first consider what I need to do to achieve a certain purpose. I then anticipate the necessary knowledge to accomplish those tasks, and when I find that it is not present within my existing knowledge, I recognize the knowledge gap.

% Lastly, in this discussion, knowledge refers to the collective body of research findings, particularly academic papers. In actual research, a researcher may personally have a question and then investigate previous studies to confirm that it is indeed unknown before formulating it as a research question. However, what is important in the construction of a research question is that it is unknown to other entities. Therefore, for simplicity, I directly refer to the entirety of academic papers without including the step of comparing personal knowledge.

% To summarize, to create an intelligence capable of constructing questions in this setting, I need to design it to expect the necessary knowledge to achieve a given purpose provided by a third party, search for that knowledge in academic papers, assess whether the papers contain sufficient knowledge to achieve the purpose, and express any knowledge gaps as questions.

% In this case, I excluded the discussion of triggering action by design. However, when considering increasing autonomy, it is important to discuss how to incorporate this aspect into learning and acquisition. The question of "why do I seek information" has been extensively discussed in the context of curiosity.

% Furthermore, in this case, I defined the expectation of knowledge as aiming to achieve a given purpose. However, as mentioned earlier, this does not affect the formulation of questions. For example, let's consider the case of a child asking, ``Why is the sky blue?'' In this case, the child may already have prior knowledge of the concept of ``sky'' and ``blue.'' Additionally, they may possess a naive concept of causality, believing that ``A is B, so there must be a reason for it.'' Thus, they may have expected to have the knowledge that ``the sky is blue because of B.'' However, when they reference their internal knowledge, they find that it does not contain the corresponding knowledge. Therefore, they may have asked the question ``Why is the sky blue?'' to evoke the knowledge they were lacking.

% In this way, the reasons for expecting the existence of certain knowledge can vary, and what, why, and how I seek information (knowledge) are not constrained by specific conditions. Therefore, when attempting to create an intelligence capable of constructing questions in the future, it is feasible to develop a more flexible intelligence.

% Additionally, in this case, I assumed that the given purpose and its achievement are predefined goals. However, humans naturally set their own goals. When considering the design of a more autonomous intelligence, it is conceivable to aim for automation in this aspect as well. However, as mentioned earlier, the question of what I seek knowledge about is not specific to research. Therefore

% , I temporarily set it aside for now. If I were to pursue this direction further, it would ultimately lead to an infinite regress, raising the question of how much information to consider as given.

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=\textwidth]{figs/question_formulation.jpg}
%     \caption{question construction}
%     \label{fig:enter-label}
% \end{figure}


% \textcolor{red}{TODO: Is question construction information retrieval??}


% \subsection{Conclusion}

% \begin{enumerate}
%     \item Determing the existence of expected knowledge:
%     \begin{enumerate}
%         \item Searching for knowledge directly related to the expected knowledge.
%         \item Determining whether the knowledge has been properly validated.
%     \end{enumerate}
% \end{enumerate}

% 1.b is specific to the automation of research. 





% I have listed what I believe are important elements in the construction of questions. However, these are considered important under the assumptions mentioned earlier. For instance, if the goal is not to acquire knowledge necessary for achieving an objective, but to generate knowledge that an individual finds interesting, the necessary elements in question construction (particularly in parts 1.a and 1.b) would change. As previously mentioned, the value of knowledge is determined in relation to context and there's a high degree of uncertainty about how the value of knowledge will evolve in the future. This makes it fundamentally important to have a diverse range of ways to generate questions. The object achievement is highly prevalent and is expected to produce ``important'' knowledge, which is why it is discussed here. However, it is important to discuss what other ways of formulating questions could exist and how they can be implemented.

% \subsubsection{Challenges}
% The first issue discussed in Chapter 2 is the problem of determining the unknown nature of the answer to a question. Given that research is an endeavor to produce new knowledge, it is necessary for the answer to the question to be unknown. Therefore, there is a need to generate such questions or later verify that the answer to the question is indeed unknown.

% The second challenge is the issue of how to make a machine generate a ``good'' question. Firstly, what researchers consider as a ``good'' question is not always consistently agreed upon among them. Furthermore, we pointed out that the ``goodness'' of a question is inherently a concept relative to the individual or society. Hence, there seems to be a need to clearly define what constitutes a good question and think about how to effectively integrate these definitions.

% The third challenge is that as we demand more autonomy from AI, the automation of question formulation becomes more difficult. Inherently, questions are constructed based on various motivations, such as pure intellectual curiosity or for specific objectives. It is very challenging to automate the construction of questions without defining which of these motivations should be the source. Moreover, as mentioned in Chapter 2, the construction of questions encounters the problem of infinite regression when pursued with strict autonomy. Even if not taken to that extreme, setting higher-order objectives behind questions for AI is an exceptionally difficult task.

% It seems that the automation of question formulation has received relatively less attention compared to other processes. Since the formulation of questions is an essential element in conducting research, it would be desirable for more focus to be directed towards the research on automating this process.

\subsection{Hypothesis Generation}
The second element is hypothesis generation. Research involves posing questions and attempting to answer them. Researchers often divide the process of answering these questions into two parts: generating hypotheses and verifying them. Generating a hypothesis is predicting an answer to a question, and verifying a hypothesis is the operation of checking whether it is actually plausible. Dividing the process of answering a question into these two stages is because research questions are such that no one in humanity knows the answers, making it practically difficult to hit upon the answer at once, and because an individual's thinking is not necessarily truth-promoting. In other words, the separation of hypothesis generation and verification is a methodology developed by humans to reveal the truth of this world under very high uncertainty.

The generation of hypotheses has been recognized as a place where human creativity in research is exhibited. The idea that human creativity cannot be analyzed has led to the long-held view that constructing questions and generating hypotheses are difficult to analyze \cite{sep-scientific-discovery}. However, attempts to characterize this creative activity have emerged since the mid-20th century. For instance, the importance of abduction in generating hypotheses for why questions \cite{hanson1965patterns,magnani2011abduction}, the importance of analogical reasoning in hypothesis generation \cite{hesse1965models,gentner2002analogy}, the interpretation of scientific discovery as exploration \cite{langley1987scientific}, and the formulation as probabilistic sampling \cite{dasgupta2017hypotheses} have been pointed out. Hypothesis generation by machines has been one of the greatest interests since the advent of artificial intelligence research, and attempts at its realization have been made early on. As early examples, the development of machines that generate hypotheses was attempted as early as the 1900s \cite{langley1987scientific,lindsay1993dendral}, and by the mid-2000s, machines capable of autonomously making scientific discoveries were developed \cite{king2004functional}. 

% 二つ目の要素は仮説の生成です。研究とは問いを立てて、その問いに対する答えようとする営みです。研究者は多くの場合この問いに答える過程を仮説の生成と仮説の検証の二つに分離して行います。仮説の生成は質問に対する答えの予想であり、仮説の検証はそれが実際に確からしいかを調べる操作です。問いに答える過程をこのように二つの段階に分けるのは、リサーチクエスチョンは人類が誰もその答えを知らないようなものであり、そのため一度に答えを言い当てることは事実上困難であること、そして一個人の思考は必ずしも真理促進的ではないことなどが理由として考えられます。言い換えるなら、仮説生成と検証への分離は人間が極めて不確実性の高い対象の真実を明らかにするために開発した方法論であると言えます。

% 仮説の生成は研究における人間の創造性が発揮される場所として認識されてきました。人間の創造性は分析できないとの考えから、問いの構築と合わせて仮説の生成は長らく分析が困難なものであると認識されてきました \cite{sep-scientific-discovery}。しかし、20世紀中頃からこの創造的な営みを特徴づけようという試みが生まれてきました。例えば、why question に対する仮説生成におけるアブダクションの重要性の指摘 \cite{hanson1965patterns,magnani2011abduction}、仮説生成における類推的推論の重要性の指摘 \cite{hesse1965models,gentner2002analogy}、科学的発見の探索としての解釈 \cite{langley1987scientific}、確率的サンプリングとしての定式化 \cite{dasgupta2017hypotheses} などがなされました。機械による仮説の生成も人工知能研究の登場以来の最大の関心事の一つであり、早くからその実現が試みられました。初期の例としては1900年代にはすでに仮説を生成する機械の開発が試みられ \cite{langley1987scientific,lindsay1993dendral}、2000年代半ばには自律的に科学的発見を行う機械が開発されました \cite{king2004functional}。

\subsubsection{Hypothesis Generation and Machine Learning}

Generating hypotheses involves predicting answers to unknown questions from existing knowledge. This can be seen as making predictions about unseen data from data already seen, and can thus be reduced to the formulation of machine learning. In particular, since it involves answering a question, it can be directly equated to the question-answering tasks in machine learning. Furthermore, as it involves predicting answers that nobody knows, it can also be viewed as a machine learning prediction problem in situations with significant distribution shifts.

Indeed, machine learning models are widely used for hypothesis generation in science \cite{xu2021artificial,zhang2023artificial,wang2023scientific}. Examples include predictions of new proteins, drug candidates, or new physical properties. All such scientific discoveries made without the verification of machine learning models can be interpreted as applications of machines in scientific hypothesis generation. The specific methods of hypothesis generation in each application vary greatly, but they all share the function of generating hypotheses in the process of knowledge creation.

What is used as the basis for generating hypotheses, the nature of the hypothesis space, and how hypotheses are expressed, vary across research fields. For instance, hypotheses may be generated from papers, scientific data, or a predefined search space. They might be expressed as combinations of multiple elements, as mathematical models represented by equations, or in text form. In chemistry, for example, hypotheses are often combinatorially represented, and the discovery of hypotheses by machines has been attempted by exploring this chemical design space \cite{coley2020autonomous}. Research has been conducted to find symbolic equations that describe the laws behind scientific data \cite{kramer2023automated}. There are also many studies that generate or extract hypotheses as text from groups of papers \cite{kang2022augmenting,chan2018solvent,wang2023learning,xu2023exploring,yang2023large}.

In recent years, with the rise of large-scale language models, there have been efforts to generate hypotheses solely from the internal knowledge of the language models, without directly providing information from papers \cite{park2023can,ai4science2023impact}.

While the specifics of individual hypothesis generation vary, it is possible to uniformly describe them to some extent from a certain viewpoint. For example, if we assume that the hypothesis space is provided and fixed by humans, we could say that many scientific discoveries can be seen as search problems \cite{coley2020autonomous}. Especially in science, the hypothesis space is often combinatorially vast. Therefore, it is considered important to efficiently explore such vast search spaces \cite{coley2020autonomousII,zenil2023future}, and attempts have been made to achieve efficient exploration through methods like active learning. As an another perspective, Wang et al. have organized studies applying machine learning to hypothesis generation. They have categorized and organized how AI is being used for scientific hypothesis generation from the perspective of using it for black box prediction, aiding in the exploration of hypothesis space, and finding solutions in a differentiable hypothesis space \cite{wang2023scientific}.

As such, the application of machine learning to hypothesis generation has progressed significantly compared to question formulation and hypothesis testing. The examples mentioned here are just a fraction of the extensive research that uses machine learning for hypothesis generation. Although this paper cannot delve into each study in detail, those interested are encouraged to refer to survey papers in their respective fields.

There are indeed numerous instances where machine learning has been applied to hypothesis generation. However, how to realize AI that can generate hypotheses in response to questions remains an open question. To achieve such AI, as mentioned earlier, it may be necessary to appropriately switch the expression and design methods of hypotheses according to the question, or to design the hypothesis space from scratch. Further discussion on how to realize these capabilities is expected to deepen in the future.

% 仮説を生成することは既存の知識から未知の問いの答えを予測することです。これはすでに見たデータからまだ見ぬデータに対して予測をするという意味で、機械学習の定式化に還元されるということができます。特に、問いに対する答えであるという点から、機械学習における質問応答タスクそのままであるということもできます。また、誰も答えを知らないような答えの予想であるという点から、大きな分布シフトがあるような状況での機械学習の予測問題だと捉えることもできます。

% 実際に、機械学習モデルは科学の仮説生成に広く使われています \cite{xu2021artificial,zhang2023artificial,wang2023scientific}。たとえば、新しいタンパク質や新薬候補や新物性の予測などの、機械学習モデルによる検証を伴わないあらゆる科学的発見は、機械の科学的仮説生成への応用例だと解釈することができます。各々の応用例における具体的な仮説生成の方法は通常全然異なるものですが、それらは知識生成の過程において仮説生成の機能を担っているという点では共通しています。

% 何を元に仮説を生成するのか、仮説の空間がどのようなものであるか、仮説がどのように表現されるかは研究分野によってさまざまです。例えば仮説は論文から生成されるかもしれませんし、科学的データから生成されるかもしれませんし、事前に定められた探索空間から見つけられるかもしれません。また、仮説は複数の要素の組み合わせとして表現されるかもしれませんし、数式で表現される数理モデルかもしれませんし、テキストで表現されるかもしれません。たとえば化学では主に仮説を組み合わせ的に表現し、この chemical design space の中を探索することで機械による仮説の発見が試みられてきました \cite{coley2020autonomous}。仮説を方程式で表現するものとしては、科学的データから背後にある法則を記述するシンボリックな方程式を発見する研究が行われてきました \cite{kramer2023automated}。また、論文群から仮説をテキストとして生成・抽出する研究も多くあります \cite{kang2022augmenting,chan2018solvent,wang2023learning,xu2023exploring,yang2023large}。
% 近年では、大規模言語モデルの台頭によって、論文の情報を直接的に与えずに、言語モデル自身の内部の知識のみから仮説生成をさせることを試みる取り組みも現れています \cite{park2023can}。

% このように、個々の仮説生成の具体的な内実は異なりますが、これらをある観点である程度類型化することもできます。まず、仮説空間を人間が与えて所与とすれば、there is a perspective that many scientific discoveries can be seen as search problems \cite{coley2020autonomous}.特に、科学における仮説空間は組み合わせ的に広大であることが往々にしてあります。したがって、仮説生成する AI にはこのような広大な探索空間を効率的に探索する能力が重要であると考えられており、active learning などによって効率的な探索を実現することが試みられています。また、Wang らは機械学習を仮説生成に応用した研究群を別の観点で整理しました。彼らは、have categorized and organized how AI is being used for scientific hypothesis generation from the perspective of using it for black box prediction, aiding in the exploration of hypothesis space, and finding solutions in a differentiable hypothesis space \cite{wang2023scientific}.

% このように、機械学習の仮説生成への応用は問いの構築や仮説の検証に比べて大きく進んでいます。ここで挙げた例はほんの一部であり、機械学習を仮説生成へ用いた研究はこの他にも膨大にあります。本論文では個々の研究について詳細に検討することはできませんが、ご関心がある方はそれぞれの分野のサーベイ論文を参照してください。

% このように機械学習を仮説生成へ応用した事例は膨大にあります。しかし、問いに応じて仮説生成ができる AI をどのようにして実現するかはオープンクエスチョンです。このような AI を実現するためには、上述したような仮説の表現や設計法を問いに合わせて適切に切り替えたり、仮説空間を自らゼロから設計したり \cite{coley2020autonomousII} などする必要もあるでしょう。このようなものをいかに実現していくかについて今後議論が深まっていくことが望まれます。

\subsubsection{Things Seemingly Important in Hypothesis Generation by Machines?}

Realizing AI capable of autonomously generating hypotheses in response to arbitrary questions have already garnered considerable attention compared to question generation and hypothesis verification. This is largely because, as previously mentioned, hypothesis generation is essentially predicting answers to certain questions, a problem that many researchers in machine learning are already addressing. Particularly, AI capable of generating hypotheses in response to questions is a special case of a widely recognized ``strong AI'', and the challenges in realizing them largely overlap. For instance, while systematic thinking such as deduction, out-of-distribution generalization, causal inference, efficient exploration, and decomposing problems into sub-problems seem important for better hypothesis generation, these are already considered crucial in creating stronger AI.

In this section, I will consider elements that seem important for the realization of an AI capable of generating hypotheses. However, as mentioned above, discussion here may overlap with issues already debated in the context of creating strong AI and so lack the novelty. Acknowledging that there might not be much novelty, I'll speculatively explore two aspects thought to be important for the realization of such AI from among many elements that seem important. 

Firstly, an important aspect to note of hypothesis generation is that the answers to the questions are unknown to anyone. For a machine generating hypotheses, it's not necessary that the generated hypothesis is unknown to the machine itself. However, if the answer is unknown to both humans and the machine, hypothesis generation becomes a challenging task even for strong AI. In such cases, like humans, machines will need to autonomously develop and utilize methods to approach subjects whose answers they do not know. Although this essentially becomes a problem of out-of-distribution generalization, it's particularly difficult as the answers are unknown to both humans and machines. To solve such problems, machines, like humans, may need to gradually reduce uncertainty and approach the answer. Current AI still does not even understand what it doesn't know \cite{guo2017calibration,maynez2020faithfulness}. How AI capable of reasoning under such high uncertainty can be realized remains an open question.

Secondly, let me re-emphasize the importance of mathematics in hypothesis generation. Firstly, the deductive nature of mathematics has been incredibly powerful in human hypothesis generation. This is because as long as the premises are true, the results obtained through deduction are guaranteed to be true, regardless of how counterintuitive they may be. This provides AI, which primarily relies on inferences from experience, a significant advantage. Just like humans have done with the hypothetico-deductive method, deduction allows us to discuss the plausibility of hypotheses that cannot be directly tested. Rejection of the deductive results means the hypothesis is false, and acceptance strengthens the belief in its plausibility. This is an important element in expanding the empirical knowledge boundaries in research. While the importance of deduction/hypothetico-deductive method has been widely discussed, it holds these advantages in hypothesis generation.

Additionally, mathematics is abstract. From the ancient time, even in the absence of formal deduction, mathematics dealt with the concepts such as numbers, which is greatly abstract concept of great interest to humans \cite{david2010history}. Through the introduction of symbolic representation and manipulation, mathematics has been further enhanced in its abstract nature. Moreover, mathematics not only abstracts objects in this world but also engages in a cycle of further abstraction. By repeatedly abstracting the abstracted, it has constructed highly abstract systems \cite{bochner1968role}. It seems that such abstraction enables refering to subjects not directly experienced \cite{heisenberg2008abstraction} and representation of laws rather than individual cases. I believe these characteristics make mathematics an indispensable part of hypothesis generation.

% Furthermore, through the premises established by abstract concepts, further deductions lead to a broader understanding of the world \cite{heisenberg2008abstraction}. I believe these characteristics make mathematics an indispensable part of hypothesis generation.

% Lastly, mathematics is abstract. From the ancient time, even in the absence of formal deduction, mathematics dealt with the concepts such as numbers, which is greatly abstract concept of great interest to humans \cite{david2010history}. Through the introduction of symbolic representation and manipulation, mathematics has been further enhanced in its abstract nature. Moreover, mathematics not only abstracts reality but also engages in a cycle of further abstraction. By repeatedly abstracting the abstracted, it has constructed highly abstract systems \cite{bochner1968role}. Abstraction involves extracting only partial common properties of objects, and it is closely related to understanding, generating more universal knowledge. Additionally, through the premises established by abstract concepts, further deductions lead to a broader understanding of the world \cite{heisenberg2008abstraction}. I believe these characteristics make mathematics an indispensable part of knowledge production. 

I have explored two examples of seemingly important elements for machine hypothesis generation. Though treated separately here, systematic thinking (systematic generalization or constructive generalization) seems to be required for both. Therefore, I believe, like many others, that the acquisition of systematic thinking or high-level thought is important for the realization of such AI. However, since the importance of systematic thinking is widely agreed upon and already extensively discussed \cite{goyal2022inductive}, I won't delve further into it here. 

Due to my limitations, this paper only provides a superficial discussion. I would appreciate any feedback from those who have thoughts on elements not widely recognized in the machine learning community but considered important for autonomous hypothesis generation.

% 問いに応じて仮説を自律的に生成できる AI の実現についての課題は、問いの生成や仮説の検証と比べると、すでに多くの注目を集めていると言っても良いでしょう。その結果、すでに多くの議論が蓄積されていると思われます。

% なぜならば、すでに述べたように仮説の生成はある問いに対する答えの予測であって、これはすでに機械学習で多くの研究者が取り組んでいる問題だからです。特に、問いに応じて仮説を生成できるAIとは汎用的な予測ができるAIのある特殊な場合であるため、このようなAIを実現する上での課題は、汎用人工知能を実現するための課題と大部分が重複するように思われます。例えば、より良い仮説生成をする AI を実現するためには、演繹などの系統的思考、分布外汎化、因果推論、効率的な探索、問題の部分問題への分割などが重要であるように思われますが、これらはより強い人工知能を作る上ですでに重要だと考えられているものです。

% このセクションでは、問いに応じて自律的に仮説生成ができる AI の実現にとって重要だと思われる要素について少し検討しようと思いますが、上述の理由から、ここで議論する内容はすでに強いAIを実現する課題として議論されているものと大部分が重複しています。その意味で、ここで展開する議論は新規性に乏しいとは思いますので、すでになされている議論の繰り返しになるかもしれません。以下では、新規性があまりないであろうことを承知の上で、仮説の生成において重要であろうと思われる観点を２つ取り上げて、それがなぜ重要だと思われるのかについて、簡単に議論したいと思います。ただし、これらは重要だと考えられている要素のほんの一部であり、その全てを網羅できていないであろう点についてはご了承ください。

% まず、仮説生成の一つの重要な要素として、その問いの答えがまだ誰にも知られていないという点があります。人間にとって答えが未知な仮説を生成する機械にとっては、生成する仮説が必ずしもその機械にとって未知である必要はありません。しかし、その答えが人間だけではなく機械にとっても未知な場合は強いAIにとっても仮説生成はチャレンジングなタスクになるでしょう。このような場合、これまで人間がやってきたのと同様に、機械は自らが答えを知らない対象に接近する方法を自律的に構築して活用できなければいけないように思われます。これは定式化の上では単に分布外汎化になりますが、その答えが人類にも機械にも誰一人わかっている人がいないようなものであるという点で、特に難しい分布外汎化の問題であると考えられます。このような問題を解くためには、機械は人間がやってきたように何らかの方法で段階的に不確実性を削減することで答えに接近していく必要があるように思われます。このような不確実性の高い状況下での推論ができる AI をどのように実現できるかはオープンクエスチョンです。

% 二つ目に、仮説生成における演繹の重要性について再度強調したいと思います。人間の仮説生成においては演繹、特に数学が非常に強力であったことは疑いがありません。なぜなら、演繹においては、前提が真である限り演繹で得られた結果はどれだけ直観に反していても真であることが保証されるからです。これによって、経験からは自然には出てこないような予測もすることができるようになります。経験からの推論を基本とするAIにとってこれは非常に強力なアドバンテージとなります。また、人間が仮説演繹法として行なってきたように、演繹によって直接は検証され得ない仮説の確からしさについて議論することができるようになります。演繹結果が棄却されれば仮説は偽であるとわかりますし、採択されてもその仮説が確からしいという信念を強めることにつながります。これは研究について経験的に知りうる可能範囲を押し広げる上で重要な要素でしょう。演繹/仮説演繹法の重要性についてはすでにに様々に議論されていますが、仮説生成においてはこのような利点があると考えられます。

% 以上、２つの仮説の生成に関連しうる要素について議論しました。ここではこれらを二つの別々の要素として議論しましたが、これらのいずれにとっても、系統的思考（系統的汎化や構成的汎化）の獲得が重要であるように思われます。そのため、私はこのような AI の実現にとっては、多くの人が思っているのと同様に系統的思考の獲得が重要だと考えています。しかし、系統的思考の重要性についてはすでに多くの人が同意するところだと思われますし、すでに多くの優れた議論がなされているので \cite{goyal2022inductive}、ここでこれ以上は立ち入りません。本論文では私の能力の限界で表層的な議論しかできませんでしたが、機械学習のコミュニティからは注目されていないが仮説生成において重要であると思われる要素としてどのようなものがあるか考えがあるかたはぜひフィードバックを頂けますと幸いです。



\subsection{Hypothesis Verification}
The final element is the verification of hypotheses. We justify our belief in the truth or falsehood of a hypothesis by confirming the plausibility of our prediction in response to a question through verification. Thus, verification is essential for generating knowledge.

What constitutes verification naturally depends on what the question and hypothesis are. For instance, if the question is a ``why'' question, then the verification must involve methods that can clarify causal relationships. If the question pertains to the physical world, then verification will necessarily require interaction with the physical world. If the hypothesis can be proven through mathematical proof, then mathematical proof will serve as verification. An agent capable of verification is expected to have a high-level understanding of what verification is and, in all these situations, choose and construct an appropriate method of verification based on the question and hypothesis.

Compared to hypothesis generation, there's less discussion about letting AI itself perform verification. While there have been many studies utilizing AI for parts of verification tasks, such as experimental design \cite{chaloner1995bayesian} and simulations in science \cite{baker2019basic}, initiatives that make AI understand what verification is and think from scratch about what needs to be done to verify hypotheses are still not widespread. Certainly, in machine learning research, studies on judging the validity of scientific claims \cite{wadden2020fact}, confirming whether predictions are factual \cite{guo2022survey}, searching for evidence to support hypotheses \cite{koneru2023can}, and studies making machines self-verify their answers \cite{dhuliawala2023chain} are strongly related to verification, but none of them aim to construct and execute verification like human scientific research. The automation of peer review \cite{kousha2022artificial,lin2021automated1} is also related to understanding verification in the sense that it demands judgment on the validity of the verification proposed in papers, but it does not generate verification.

In this section, I will briefly explore the concept of verification to provide seeds of thought. Since I have already discussed what verification, i.e., justification, is in Section \ref{section-what-is-research}, I will skip that discussion here. Instead, in this section, I will discuss experimentation that is inevitable when conducting human-like verification in science.

% 最後の要素は仮説の検証です。私たちは問いに対する予想の確からしさを検証によって確かめることで、ある仮説が真である・偽であるという信念を正当化します。したがって、知識を生み出すためには検証は不可欠です。

% 検証がどのようなものになるかは、当然ですが問いと仮説が何であるかに依存します。例えば、問いが ``Why-question'' であれば、検証は因果関係を明らかにできる方法ではないといけません。問いが物理的世界についての言及であれば検証は当然物理的世界との相互作用を必要とします。仮説が数学的証明で証明され得るものであれば、数学的証明が検証となるでしょう。検証ができる AI は、検証とは何かというハイレベルな理解を持ち、これらのいずれの状況でも、問いと仮説に基づいて、適切な検証法を自ら選択・構築し実行できることが期待されます。

% 仮説の生成に比べると AI 自身に検証をさせるという話は多くはありません。科学における実験計画やシミュレーションなど検証の一部分のタスクを代替させる研究は以前から多くありますが \cite{baker2019basic}、AI 自身に検証とは何かを理解させ、仮説を検証するためには何をすべきかゼロから考えさせるような取り組みは、まだ多くないと言って良いでしょう。確かに機械学習の研究では、科学的主張の妥当性を判断する研究や \cite{wadden2020fact}、予測が事実であるかを確認する研究 \cite{guo2022survey}、仮説をサポートする証拠を探す研究 \cite{koneru2023can}、自らの回答を機械に自己検証させる研究 \cite{dhuliawala2023chain} は検証に強く関連する研究ですが、いずれも人間が行う科学研究のような検証を構築して実行するものではありません。査読の自動化 \cite{kousha2022artificial,lin2021automated1} も論文で提案されている検証の妥当性の判断を求められるという意味で、検証の理解に関連する研究ですが、検証を生成するものではありません。

% 本セクションでは、自律的に研究ができる AI を作るために必要そうなことを考える思考の種を提供できるよう、検証について分析していきます。検証とは何か、すなわち正当化とは何かについてはすでにセクション 2 で話をしましたので、ここではそれに関する議論については割愛します。代わりに、このセクションでは人間のような検証をする際に重要であると思われる要素について議論します。特に、科学における検証である実験とその重要な要素である統計的分析について議論します。これによって、検証ができる AI は何ができなければならないのかについて少しだけ解像度を上げる手助けとなれば幸いです。

\subsubsection{Experimentation}
\label{section-experimentation}
No researcher would deny the importance of experiments. An experiment is the planning and execution of a series of procedures to empirically test a hypothesis, essentially the verification itself in empirical science. Therefore, any agent capable of verification must necessarily be able to conduct experiments.

In experiments, phenomena that are difficult to observe or the effects of various conditions are precisely investigated by artificially generating phenomena in a controlled manner and actively intervening in them \cite{radder2009philosophy}. This creates differences in the relationships and causal connections of the variables of interest. These are observed and stored as experimental data, and the analysis of this data determines whether the hypothesis is true or not.\footnote{
Experiments are not only conducted during the verification phase but also when generating hypotheses. Moreover, new questions and hypotheses are often formulated based on the results obtained from experiments. In these instances, the process leading up to data generation, or conducting experiments not solely for verification but for data generation and some form of data analysis, seems to be what is referred to as an experiment. This paper defines an experiment as planning, preparing, generating data, analyzing it, and determining verification results. However, be aware that this definition may not always reflect actual practice
}

To conduct an experiment, one must first design the experiment, write down the procedures, and plan the experiment. This requires an understanding of what constitutes a successful hypothesis test and the ability to conceive of ways to realize this using existing technology. Preparations for the experiment are also essential. This can include purchasing chemicals, preparing flasks, training animals, constructing necessary equipment, sometimes even building large apparatus like accelerators from scratch, applying to ethics committees, and creating clean rooms. Unfortunately, since research aims to uncover the unknown, constructing equipment from scratch for experiments is not uncommon in research. The autonomous execution of these preparations by a non-human agent from scratch is considerably challenging.

After the preparation for the experiment is complete, the experiment is conducted according to the experimental protocol. This is also very challenging for a machine to perform autonomously. The reason is that even for a single experiment, a myriad of low-level operations such as grasping, cutting, carrying, mixing, moving, pouring, dispensing, washing, and opening lids need to be flexibly combined to execute the experiment. An autonomous machine capable of conducting experiments needs the ability to generate these operations flexibly according to the self-generated experimental protocol.

% 研究者で実験の重要性を否定する人はいないでしょう。実験とは経験的にある仮説をテストするための一連の手続きの計画と実行のことで、経験科学における検証そのものです。したがって、検証ができるエージェントは当然実験ができなければなりません。

% 実験では観察が困難な現象や様々な条件の影響を正確に調べるために、統制された方法で人工的に現象を生成し、これ対して積極的に介入をすることで調べたい変数の関係や因果関係などの違いを人為的に作り出します \cite{radder2009philosophy}。これらを観測して実験データとして保存し、このデータを解析することで仮説が真であるか否かを判定します。

% 必ずしも検証の時だけではなく、仮説を生成する際にも実験は行われます。また、実験で出てきた結果を受けて新しい問いや仮説を生成することが行われます。この時、データを生成するまでの過程を、あるいは必ずしも検証のためだけではなくデータ生成となんらかのデータ解析をすることを指して実験と呼んでいるように思われます。本論文では計画を立てて、準備をして、データを生成してそれを解析して検証結果を判断することを指して実験と呼びますが、それが必ずしも実際の実践を反映していない可能性がある点には注意してください。

% 実験を実施するためには、まず実験のデザインをし、実験の手続きを書き下し、実験の計画を立てなければなりません。このためには、何をどうしたら仮説を検証したことになるのかという検証についての理解、そして既存の技術を駆使して具体的にそれを実現する方法を考える能力が必要になります。そして、これらを実際に実行可能なものとするため、実験の準備をしなければなりません。例えば薬品を購入したりフラスコを準備したり動物を訓練したり必要な装置, 時には加速器のような巨大な装置, をゼロから作ったり、倫理委員会に申請を出したり、クリーンルームを作ったり、しなければなりません。さらには、研究はまだ誰も知らないことを明らかにすることを目指すので、実験のために使う装置をゼロから作ることも研究においては珍しいことではありません。これらの準備を機械がゼロから自律的に実行するのは相当な困難です。

% 実験の準備ができた後は、実験プロトコルに従って実験を実行します。これも機械が自律的に実施するのはとても難しいことです。なぜなら、一つの実験をするのにも、掴む、切る、運ぶ、混ぜる、移動する、注ぐ、分注する、洗う、蓋を開ける、などなどの無数の低レベルの操作を自在に組み合わせて実験を実行する必要があるからです。自律的に実験ができる機械には与えられた実験プロトコルに応じてこれらの操作を自在に生成できる能力が必要となります。

% I recognize that the the way to verify a hypothesis is strikingly diverse, as it can significantly differ depending on the subject of research. For instance, if one wishes to study the behavior of rat, they may need to train the rat. On the other hand, if you want to test a theory of particle physics, you may have to construct and run a huge particle accelerator. In the field of history, the existence of historical records might serve as evidence, while in mathematics, the verification process revolves around the proofs themselves. Due to this high degree of flexibility, hypothesis verification can be the most challenging aspect to automate for realizing a ``general'' artificial researcher.

\subsubsection{Automating Experimentation}

As we can see, making a machine fully autonomous in not only planning but also preparing and executing experiments is considerably difficult. Especially since what experiments should be conducted cannot be known until questions and hypotheses are formulated, enabling a machine to autonomously conduct research from the construction of questions demands the capability to accommodate all possible experimental scenarios. I consider this to be one of the greatest barriers in creating machines capable of autonomously conducting research.

Automating experiments is a very challenging task, yet humanity has steadily made progress in this difficult endeavor. Related to the planning stage of experiments, for example, the efficiency and automation of exploring experimental conditions have a long history. Wang et al. have summarized these studies that utilize AI to assist experiments by research planning, research guidance, and generating observational data through numerical simulations \cite{wang2023scientific}. 

Additionally, there is an initiative known as \textit{laboratory automation}, or \textit{self-driving lab} that attempts to automate experiments, including the execution of experiments, which, as mentioned earlier, is a challenging aspect \cite{holland2020automation,abolhasani2023rise}. A notable example is pioneering research in genetics by King et al., who fully automated the cycle of hypothesis generation, verification, and discovery of new hypotheses \cite{king2004functional}. Another example is A.I. Cooper, which enabled the use of the same experimental equipment as humans through autonomous robots \cite{burger2020mobile}. 

These examples of initiatives aim to autonomously drive the research cycle, including hypothesis generation, planning and execution of experiments, and generation of hypotheses based on experimental results. Such initiatives are referred to as the closed-loop automation of scientific discovery     \cite{burger2020mobile,king2004functional}. This represents an example of achieving extremely high autonomy in the quest for research automation. Furthermore, there are efforts to develop humanoid robots for experiments, capable of conducting multiple different experiments with a single robot \cite{yachie2017robotic}. This is deemed to be a foundational step towards potentially general research automation. 

In recent years, there have also been attempts to conduct experiments autonomously using LLMs \cite{boiko2023emergent,charness2023generation,qin2023gpt}. For example, Boiko et al. constructed an autonomous agent composed of multiple LLMs and let it autonomously conduct from designing to executing experiments, leading to successful performance on complex scientific problems \cite{boiko2023emergent}. 

So far, we have primarily discussed the generation of experimental data, but to validate it, interpretation is necessary. Observation always entails some theory behind it \cite{hanson1965patterns}, and to analyze and interpret experimental data, sufficient prior knowledge is required. Therefore, there are studies focused on enabling machines to interpret scientific data by embedding physical prior knowledge such as symmetries, differential equations, and intuitive physics into machine learning models \cite{hao2022physics,karniadakis2021physics}.\footnote{
The interpretation of scientific data is not always done solely for the purpose of validation. Therefore, these technologies are not limited only to the automation of validation processes.
}

Various research efforts have steadily advanced the automation of experiments. On the other hand, it's also true that there are still many challenges in realizing machines capable of conducting experiments autonomously. Coley et al. discuss these challenges in detail, focusing on the automation of experimental and computational validations and the selection of experiments, referencing studies on automated verification \cite{coley2020autonomousII}. In particular, to achieve a versatile automated experimental machine adaptable to various research tasks, robots that can manipulate low-level actions as humans do are necessary, which is exceedingly difficult to realize. Even short of this, efforts such as removing hardware constraints to increase the number of automatable research projects and reducing costs for research automation are also important \cite{coley2020autonomousII}.

% このように、実験の計画だけでなく準備や実行まで機械に完全に自律的に実行させるとなると、相当困難であることがわかります。特に、どのような実験をすべきかは問いや仮説を立ててからでないとわかりませんから、問いの構築から自律的に研究をさせるとなると、あらゆる実験の可能性に対応できるような能力が要求されます。私はこれは研究が自律的に実行できる機械を作る上での最大の障壁の一つであると考えています。

% このように実験の自動化は非常に難しいタスクですが、人類はこれまで着実にこの難しいタスクを前に進めてきました。実験の計画の段階に関連するものとしては、例えば実験条件の探索の効率化と自動化は長い歴史を持っています。Wang et al. have summarized these studies that utilize AI to assist experiments by research planning, research guidance, and generating observational data through numerical simulations \cite{wang2023scientific}. 

% また、上で困難であると述べた実験の実行も含めて実験の自動化を試みる、laboratory automation と呼ばれるイニシアチブもあります。

% ここまで実験データの生成までの話を主にしてきましたが、検証をするためにはそれを解釈する必要があります。何かを観測することは常に何らかの理論を背後に伴うため \cite{hanson1965patterns}、実験データを解析して解釈するためには、それを読み取るだけの事前知識が必要です。したがって、機械学習モデルに科学データを理解するために必要な対称性や微分方程式や直観物理といった物理学的な事前知識を入れ込むことで、機械が科学データを解釈できるようにする研究があります \cite{hao2022physics,karniadakis2021physics}。

% このように、さまざまな研究が実験の自動化を着実に推し進めてきました。一方で自律的に実験ができる機械の実現にはまだ多くの課題があるのも事実です。Coley et al. discuss in detail these challenges of automating experimental and computational validations, as well as the selection of experiments, while referencing studies on automated verification \cite{coley2020autonomousII}.特に、さまざまな研究課題に適応可能な汎用的な自動実験機械の実現には、人間のように低レベルの行動を自在に操れるロボットが必要であり、非常に実現が難しいです。そこまでいかなくても、実験の自動化を進めていくためには、ハードウェアなどの制約を取り払って自動化可能な研究を増やしていくことや、研究の自動化のためのコストを下げていくことなどの努力も重要です \cite{coley2020autonomousII}。


% Experiments are generally considered a series of procedures to test a hypothesis, particularly referring to the process of data generation. I believe that the main elements in an experiment are the interventions to test a hypothesis and the generation of data to artificially create observations of phenomena. Verification is then done through the analysis of data obtained from these experiments.

% The experiment can largely be divided into three phases: designing the verification plan, preparing to execute the verification plan, and actually carrying it out. Therefore, it seems that in order to create an AI that can conduct experiments, we must create an AI capable of performing all of these tasks.

% \subsubsection{Automating Experimentation}
% 実験の自動化のレビューの話を書く、LAとか実験計画とか

% \subsubsection{Understanding Verification}
% To become capable of designing verification plans, it seems necessary to have the ability to understand what verification is and the ability to engage in long-term planning. Additionally, preparing for verification, possessing a human-like body, and the capability to manipulate it freely will be required. Among these, the acquisition of intricate planning and flexible action has been the subject of research by many artificial intelligence researchers.

% On the other hand, the acquisition of the ability to understand verification seems to receive relatively little attention. There are indeed studies that focus on the importance of verification for reducing hallucinations and verifying the reliability of scientific claims, but these do not aim to determine what constitutes verification in scientific research. Therefore, I believe that considering how to make machines comprehend what can qualify as verification remains one of the challenges ahead.

% \subsubsection{Statistical Inference and Verification}
% Science can be seen as the activities to find the law of this world by inductive reasoning based on the observations. Scientists have formalized inductive reasoning with terms from probability theory and constructed methods to perform inductive reasoning as statistical inference. 帰納推論は科学の前提ですので、研究ができる AI は当然これらの枠組みを当然使いこなせなければなりません。

% 検証は帰納推論を使って行いますので、人間は検証において仮説を採択するか否かの基準も確率統計の言葉を使って行います？

% law of nature as probability distributions, and by inferring them from a finite sample sampled from that distribution, we attempt to say something about it.


% In science, the validity of hypotheses is judged by inductive reasoning based on data generated from experiments.

% Inductive reasoning is a premise of science, and implementing it via statistical inference is a widely practiced approach. Therefore, when creating an AI capable of performing meaningful verification for humans, it would be sensible to take these as a given.

% \textcolor{red}{後で書く。そもそもここで何を言いたいか？何を持って検証とするかは複数あると上で話したけどここでそれについて少しだけ詳しくみようみたいな話し方をする？じゃあそれを話して出てくる帰結は？一緒？主に統計的仮設検定の話を書こう。ネイマン・フィッシャー・ベイズ。もう少し自分の中でちゃんと整理してから書く。あんまり統計的推論とか確率解釈とかそういう可燃性の高い話題は避けたい}

% On the other hand, there can be multiple interpretations of what it means for a hypothesis to be justified by statistical analysis \cite{otsuka2022thinking}. This implies that there are conflicting views on what methodology should be adopted to justify beliefs. As a result, there are multiple methodologies for statistical inference. Ideally, a machine capable of verification should understand ``why a certain inference is valid in justifying a hypothesis''  and be able to select or construct an appropriate methodology from these options. Otherwise, it might not truly understand what constitutes verification.

% To create an AI that can verify hypotheses, I believe it is necessary for the AI to understand the basis for verification. However, to create an AI that can perform human-like science, it may be sufficient if it can appropriately use statistical inference methods, which humans employ. This is because even if it doesn't understand why that constitutes verification, as long as it uses statistical inference correctly without violating its premises or rules, there should be no issue in using it just as a tool. In reality, even humans do not always seem to grasp these differences when conducting verification. For instance, it is not uncommon for people to use hypothesis testing without understanding why it is a valid means of justifying beliefs about a hypothesis. 

% In the case of hypothesis testing, just like in hypothesis generation, the tasks for automating hypothesis testing vary depending on who the hypothesis testing is for. If artificial intelligence is used as a tool for human research, it is sufficient for artificial intelligence to faithfully reproduce what humans do as verification. For example, it would be great if it could conduct, for instance, hypothesis testing. If the machine is capable of automatically preparing for statistical hypothesis testing, calling the appropriate statistical hypothesis testing libraries at the right timing, and using them correctly for the relevant subjects, then I would have no complaints about their performance.

% In this case, it is not necessary for the machine to strictly know why it constitutes verification, as long as it can learn from numerous examples of human verification and use it appropriately. In other words, in this case, the required understanding can be described as indirect and practical understanding through examples of human usage. Furthermore, if it can be confirmed that the machine not only mimics humans by using libraries but also understands what statistical hypothesis testing is and its principles, this would be a significant advancement in the automation of hypothesis testing. 

% I used statistical hypothesis testing as an example, but the same applies to other verification methods as well. The point is that in this scenario, I seek to enable artificial intelligence to appropriately utilize the verification techniques employed by humans. However, it is essential to note that the verification itself remains relevant and meaningful to humans.

% On the other hand, aiming to make machines understand and acquire the concept of verification autonomously from their own perspective becomes an immensely challenging task. This is because, as repeatedly emphasized, hypothesis verification involves the updating of beliefs, and the belief system of machines can significantly differ from that of humans. It doesn't seem that machines can acquire the concept of their own verification just by observing examples of human verification. Furthermore, as explained earlier, machines have not evolved their belief systems through interaction with the natural world. Therefore, verification methods entirely composed and developed by machines may no longer serve as effective tools for understanding nature. How to enable machines to autonomously acquire verification principles from the ground up, address the alignment issues between machines and humans, and ensure that these efforts lead to a better understanding of nature are challenges that the entire community should discuss and explore in the future.

% \subsubsection{Challenges}
% In Chapter 2, we highlighted several challenges in realizing an AI capable of verification. First and foremost, the AI itself needs to understand what verification is, and by what criteria a sequence of actions qualifies as verification. Ideally, it would be preferable for the AI to contemplate and understand from scratch what verification is. However, many humans don't do this either, and as discussed in Chapter 2, the philosophical debate on precisely defining verification is still unresolved. Therefore, it's harsh to demand this of a machine. At the very least, the machine needs to thoroughly understand and proficiently use verification concepts that humans employ, such as statistical hypothesis testing, from first principles.


% Second, the AI must be able to formulate detailed and complex plans to verify a hypothesis. With the advancements in language models in recent years, we are now much more capable of formulating superior plans than before. However, devising detailed plans remains a challenging issue.

% Third, it has to be prepared to carry out these plans and execute the plan with the combination of human-like complex actions.As mentioned in Chapter 2, to achieve this, the AI must be able to search for, create, purchase, and manipulate equipment with almost the same degree of freedom as humans, requiring it to exhibit extremely sophisticated and complex behaviors. This is an immensely challenging issue, and it might even be fair to say it's one of the biggest bottlenecks in realizing an intelligence capable of generic and autonomous research. Laboratory automation have attempted to address this challenge in real world by developing robots. We will discuss the case within the computer below.

% For AI to execute research on a computer, it must perform any operation within the computer. For instance, machine learning research entails, setting an environment, preparing datasets and models, and writing and executing codes. To allow AI to prepare these without human intervention, the AI itself must be able to autonomously search the web, select data, download it, and so on. Furthermore, once the AI generates code for verification, it must operate the shell to execute it.

% There are ongoing initiatives to enable language models to operate browsers \cite{nakano2021webgpt,act1}. While full browser operations might seem ambitious, there are already endeavors to allow language models to conduct searches \cite{mialon2023augmented}. If we achieve browser automation, it will greatly advance research automation involving web operations. Moreover, efforts like the open interpreter \cite{openinterpreter} aim to automate any computer action. This direction holds promise for automating all research confined within a computer. Although these studies are gaining traction in the machine learning domain, they're not always linked to research automation. We advocate recognizing this as a pivotal challenge in the realm of research automation.

% In the field of machine learning, it seems that the discussion on automated validation has not garnered much attention until now. However, recently, the need for verification is recognized in the machine learning community beyond outside of the context of research automation. Studies like \textit{scientific claim verification}, which received much attention during the COVID-20 pandemic \cite{wadden2020fact}, or attempts to minimize hallucination \cite{dhuliawala2023chain} are examples of them. These are not attempts to automate validation in research. Therefore, these findings cannot be directly applied to the automation of research validation. However, we expect that these studies will provide useful insights for the future development of artificial intelligence capable of understanding validation.

\section{Additional Topics}

\subsection{Combining Questions, Hypotheses, and Verification}
I have been speculatively discussing the construction of questions, generation of hypotheses, and verification of hypotheses, which are the essential elements of research. Looking back at completed studies, we can indeed find that each study had its own question, a hypothesis for that question, and a verification for that hypothesis. From this perspective, it can be said that research is an endeavor consisting of the continuous process of constructing questions, generating hypotheses, and verifying hypotheses, as has been classically described.

On the other hand, as you know, actual research is an extremely complex process of trial and error, and it is rare for these tasks to be done only once each in a single study as initially anticipated. In reality, for example, countless questions and hypotheses are generated even for formulating a single hypothesis, and not all of them necessarily lead to the final research outcome. 

Thus, it is more appropriate to view the construction of questions, generation of hypotheses, and verification of hypotheses as basic operations to reduce uncertainty, and we combine them to reduce the huge uncertainty gradually in the process of research. Especially in the work of discovery like constructing questions and generating hypotheses, these trial-and-error nature is considered to be important \cite{yanai2020hypothesis}.

To create an AI capable of conducting research, it seems essential to seamlessly integrate the construction of questions, the generation of hypotheses, and the testing of these hypotheses, enabling it to perform complex research practices. Therefore, in this section, we will focus on characteristics of research that have not been discussed so far and examine the considerations necessary for developing such an AI.

% 研究の構成要素である問いの構築、仮説の生成、仮説の検証について思索的に議論してきました。すでに終了した研究を振り返ると、実際にその研究の問いがあり、問い対する仮説があり、仮説に対する検証が見つけられます。この観点からすれば、研究とは、古典的に語られてきたように、問いの構築、仮説の生成、仮説の検証の連続からなる営みであるということもできます。

% 一方で、実際の研究は極めて複雑な試行錯誤的なプロセスであり、問いの構築、仮説の生成、仮説の検証が一度の研究で最初に想定していた通りそれぞれ一回ずつしか行われないということは稀です。実際にはたとえば一つの仮説を立てるのにも無数の問いや仮説を生成していますし、そのすべてが必ずしも最終的な研究成果につながるわけではありません。問いの構築、仮説の生成、仮説の検証はむしろ不確実性を削減するための基本的な操作と見る方が適切であり、それらを複雑に組み合わせてより大きな研究課題の不確実性を削減していると見る方が実際の研究の営みにも沿っているでしょう。特に、問いの構築や仮説の生成といった発見的な作業の中ではこのような試行錯誤的な要素がより重要になってくると思われています \cite{yanai2020hypothesis}。

% I believe that all endeavors to transform the unknown into the known, to some extent, inevitably involve the construction of questions, the generation of hypotheses, and the verification of these hypotheses.

% 研究ができる AI を作るためには、問いの構築、仮説の生成、仮説の検証を自在に組み合わせて、このような複雑な研究実践ができなければならないように思われます。そこで、本セクションでは、これまであげてこなかったような、研究の特徴に注目しながら、そのような AI を作るために考慮すべきであろう事項について検討していきます。

% \subsubsection{Human Research Practice and Knowledge Production System}
% In actual research, while addressing the initial question posed, another question may arise and the focus may shift to that new question. Also, before reaching the final hypothesis that gets reported, several different hypotheses are tested repeatedly. Like this, the actual practice of research is complex.\footnote{
% For example, the concept of \textit{night science} as proposed by Yanai and Lercher symbolizes the such complex reality of research practice \cite{yanai2019night}.
% } When compared to this trial-and-error process, the framework I have proposed may seem overly simplified at first glance. However, I believe that the framework I proposed encompasses these human practices as well.


\subsubsection{Countless Question, Hypothesis, and Verification in Single Research Process}


\textcolor{red}{問いや仮説や検証が組み合わされる小さな単位であり、それらは単線的なプロセスではないということを強調するポンチ絵を置く}

We generate countless questions and hypotheses, including implicit ones, for the purpose of creating a single question or hypothesis, or for planning and preparing for single verification. It appears that historically, some of the major scientific discoveries were also brought about through such a process \cite{hanson1965patterns,gribbin2022origin,whiteside1970before}.

For example, when questioning how an unresolved issue can be solved, we might ask why this problem hasn't been solved so far, or if there are any studies of similar challenges being addressed. In response, we might consult literature or recall our own memory, hypothesizing that ``This could be the reason it hasn't been solved,'' or that ``This could be useful in solving the current challenge.'' We repeat this process innumerably to eventually construct an answer to the original question. Similarly, when planning verification, we implicitly pose many questions and formulate numerous auxiliary hypotheses.

To generate a plausible hypothesis, there must be sufficient grounds to believe it is valid. These grounds could be knowledge from our own memory, descriptions from newly researched literature, opinions from other researchers, or some belief, such as natural law should be simple. In addition, to examine the validity of the hypothesis, we might try simple tests. We sometimes even conduct preliminary experiments to assess the plausibility of a hypothesis. In other words, each time a plausible hypothesis is generated, it undergoes a sort of verification, whether implicit or explicit, and to varying degrees of simplicity.

In this way, to conduct research, we pose countless questions and hypotheses and conduct several simple  experiments to verify the plausibility of the hypothesis if necessary. We could say that research is a hierarchical composition of question construction, hypothesis generation, and hypothesis verification. 

These numerous question, hypotheses and verification stem from the fact that research contains a lot of uncertainties and they are required for gradual reduction of these uncertainties. Therefore, agents capable of conducting research should autonomously be able to generate numerous questions and hypotheses as needed and choose more plausible hypotheses through simple verification during the knowledge production process.

% This is because research is an endeavor facing the unknown, and this process contains a lot of uncertainties. By gradually reducing these uncertainties through trial and error, research progresses. This gradual reduction of uncertainty seems essential, especially when tackling difficult questions that no one in the world knows the answers to. Therefore, AI capable of conducting research should autonomously be able to generate numerous questions and hypotheses as needed and choose more plausible hypotheses through simple verification during the knowledge production process.

% 私たちは、一つの問いや仮説を生成するために、あるいは検証の計画や準備をするために、暗黙的なものも含めると無数の問いや仮説を生成しています。

% 例えば、ある解決されてない課題をどのように解くことができるか問うた場合、ではそもそもなぜこの問題はこれまで解かれてこなかったのだろうか、似たような課題に取り組んでいる事例はないだろうか、と問うかもしれません。これに対して文献を調べたり過去の知見を思い出したりして、これが原因で解けなかったのではないか、これは今回の課題の解決に使えるのではないか、といった仮説を立てるでしょう。これを無数に繰り返して、最終的に元々答えたかった問いに対する答えを構成します。また、検証の計画を立てる際にも、私たちは暗黙的に多くの問いを立てて無数の補助仮説を立てています。

% 確からしい仮説を生成するためには、その仮説が妥当であると信ずるだけの根拠がなければなりません。その根拠となるのはは自分の記憶にある知識かもしれませんし、新しく調べた文献の記述かもしれませんし、他の研究者の意見かもしれません。これに加えて、確からしい検証をするために、私たちは試してみる、すなわち簡単な検証をすることもあります。例えば、仮説を表現するトイモデルを作って挙動を調べてみるかもしれませんし、プレ実験をしてみるかもしれません。

% このように、私たちは一つの問いを作り、それに答える一つの仮説を作り、それを一度検証するために、無数の問いと仮説を立て、必要とあれば仮説の確からしさを簡単に検証するいくつもの実験を行います。これは、研究が未知に立ち向かう営みであり、その過程にはいくつもの不確定な要素が存在するからです。それらの不確実性を一歩一歩試行錯誤的に削減していくことで、一つの研究が進んでいくように思われます。このような漸進的な不確実性の削減は、特に誰も答えを知らないような難しい問いに挑戦する際には不可欠であるように思われます。従って、研究ができる AI もこのように、必要に際して問いや仮説を生成し、簡易的な検証によってより確からしい仮説を選択するということを自律的に行えるようになる必要があるでしょう。

\subsubsection{Countless Seemingly Unrelated Operations to Knowledge Production}
\label{section-countless-seemingly-unrelated-operations-to-knowledge-production}

Research consists of numerous operations that may seem unrelated to the production of knowledge at first glance. In Section \ref{section-experimentation}, we discussed that such tasks are necessary in the context of experimentation. While Latour illustrates these realities in his study \cite{latour1987science}, it should be undoubtedly clear to many researchers that daily academic activities are shaped by these operations, even without referring to such literature.

The construction of questions, the generation of hypotheses, and the verification of these hypotheses are the goals we aim to achieve and functions in the knowledge production process, not their implementations. To implement these functions, it is essential to perform operations like those mentioned above and to combine them appropriately to achieve the desired objectives. Combining these various operations appropriately is a challenging task, even when specialized for a specific research question \cite{coley2020autonomousII}. If we aim to create an agent that can autonomously execute the entire research process, starting from generating questions, the ability to produce these actions as flexibly as humans would be essential.

% 研究は、一見すると知識生産に関係がないような無数の操作によって成り立っています。セクション \ref{section-experimentation} では実験の場合を例として、このような作業が必要となることを述べました。 Bruno Latour は一つの研究室で行われる作業を人類学的に観察することで、こうした日常的実践が一見するとその意味がわからないような操作たちによって形作られていることを描き出していますが \cite{latour1987science}、このような文献を参照するまでもなく日々の学術活動が多くの非知的な操作によって形作られていることは多くの研究者にとって疑いのないことであると思います。

% 問いの構築や仮説の生成や仮説の検証は達成したい目的であり、知識生産過程における機能であって、その実装ではありません。これらの機能を実装するためには上述したような操作ができること、そしてそれらを適切に組み合わせて目標を達成するための作業をすることが不可欠となります。このような複数の異なる操作を適切に組み合わせることは、特定の研究課題に特化したとしてもなお難しい課題です \cite{coley2020autonomousII}。問いの生成から始めて研究の過程の全てを自律的に実行できるようなエージェントを仮に目指す場合、人間のように自在に行為を生成できるような能力を獲得することが不可欠です。

\subsubsection{Discovering New Questions}
% \textcolor{red}{手法から問題を考えることがある}

Researchers often set out to solve one question, only to find themselves discovering an entirely unrelated question in the process. This new question, unrelated to the original question and even its underlying purpose, may lead them to pivot their research focus and potentially make significant scientific discoveries. Since research is full of uncertainty, it's not uncommon to be unable to foresee everything from the start. Thus, this discovery and shift of question is not rare.

If an agent capable of conducting research were tasked with achieving a single objective, such serendipitous discoveries might not occur. This is because any new questions it finds, no matter how intriguing or scientifically important, may not be directly relevant to achieving its predefined goal. Therefore, to encourage the agent to uncover such unrelated questions, it might be necessary to set multiple objectives or a broader high-level goal that encompasses both original and newly found questions. 

Furthermore, it is not sufficient to just have a common high-level goal.
To switch from the current question to a new one, the agent would need to evaluate which of the two questions holds more value. Therefore, as discussed in Section \ref{section-deciding-what-knowledge-to-seek}, the decision-making process regarding the value of a question should not only determine whether to ask a particular question but also be capable of comparing multiple questions, including those that share higher-order objectives. How to realize such capabilities remains an open question.

% したがって、セクション \ref{section-deciding-what-knowledge-to-seek} で述べたような問いの価値に関する意思決定は一つのある問いを問うべきかだけではなくこのような高次の目的を共通するものも含めた複数の問いの間の比較もできるものでなければならないように思われます。

% 研究者は、ある問いを解くために別の問いを立てるだけではなく、その問いに答えようとする中で、最初の問いやその背後の目的とは全く関係ない別の問いを見つけることがあります。そして研究の途中でそちらの問いに取り組むことに切り替え、その結果大きな科学的発見を成し遂げることもあります。不確実性が高い研究という取り組みにおいては、最初から全てを見通せることは多くありません。したがって、このようなことが起こることは少なくありません。

% もし 研究ができる AI にある一つの目的を達成することを目指させるとしたら、このようなことは起き得ないでしょう。なぜなら、そこで見つかった問いはどれだけ面白くてもどれだけ科学的に重要でも、その目的を達成するために必ずしも重要とは限らないからです。したがって、AI にこのような問いの発見をさせるためには、複数の目標を持たせる、あるいは両方の問いを包括するようなより大きな（例えば「自然を理解する」のような）大きな目標を持たせる必要があるように思われます。加えて、現在やっている問いに取り組むのをやめて新しい問いに取り組む意思決定をするためには、AI はこれらの異なる二つの問いのどちらが価値があるかを比較しなければならないように思われます。

\subsubsection{Feedback from Verification Result}
% 化学自動化論文の内容もここに追加する
% how should new data acquired through experimental/computational validation be used to
% update models pretrained on literature data?

In research, it's not always common for the initial hypothesis to directly answer the question posed. Rather, the process typically involves revising the hypothesis based on the results of verification, followed by additional rounds of testing. This cycle of hypothesis revision and retesting is crucial in the journey toward finding answers to the posed questions, especially when seeking unknown answers, as mentioned earlier. Consequently, it seems necessary for an agent capable of conducting research to have the ability to revise its hypotheses based on the outcomes of these verifications.

As attempts to automate scientific discovery in a closed-loop manner, taking into account feedback from verification results, there are studies based on laboratory automation \cite{king2004functional} and scientific workflow \cite{gil2022will}. Such research has made a significant contribution by automating the core scientific activity of revising hypotheses based on verification results.

% こうした検証結果からのフィードバックも考慮した closed-loop な科学的発見の自動化を目指す試みとしては、laboratory automation \cite{king2004functional} や scientific workflow \cite{gil2022will} に基づく研究などがあります。このような研究は検証結果から仮説を修正するという科学の根幹の営みを自動化していくという上で極めて大きな貢献をもたらしたと言って良いでしょう。

% このような先進的な事例はあるものの、検証結果からのフィードバックを自律的に人間のようにできる機械の実現にはまだ多くの課題があるように思われます。例えば、検証結果から機械にフィードバックを出力させる場合、その検証結果を何に反映させるか、それを受けて何を修正すべきかは人間が事前に与えることがほとんどです。 

Despite such advanced examples, there still seem to be many challenges in realizing machines that can autonomously provide feedback from verification results like humans. For instance, when outputting feedback to a machine from verification results, what to reflect these results on and what to modify in response are mostly predetermined by humans. 

In reality, when the result for verification were negative, identifying the cause of the result is not that straightforward. This is because the verification relies on numerous implicit and explicit hypotheses and all of them can be the cause of the result \cite{sep-scientific-underdetermination}. The cause may be the proposed main hypothesis, a premise behind the hypothesis, an auxiliary hypothesis, an observation, an experimental instrument, or all of them. An agent that can conduct research must identify which of these possibilities is the cause. Once the candidates for the cause are determined, the agent has to generate a more plausible hypothesis based on the results of verification, as we have discussed so far.

Furthermore, to identify the cause, it would be necessary to appropriately interpret the data generated by experiments. However, what one reads from the data can change depending on the individual's beliefs, prior knowledge, theory, and what they expect to find \cite{hanson1965patterns}. Therefore, these verification results may be reinterpreted multiple times, and with each reinterpretation, the hypothesis that needs to be revised may change. Additionally, as mentioned in the previous section, humans sometimes come up with totally different questions based on verification results and stop the research for a while. Current machines still fall short of achieving these abilities of complex interpretation of verification results that humans are capable of. Ideally, agent capable of conducting research are expected to have such capabilities.
% 科学であれば、原因を特定するためには、実験によって生成されたデータを適切に解釈する必要があるでしょう。しかし、データから何を読み取るかは、その人の持つ信念や事前知識やそこから何を期待するかによって変わり得ます。したがって、この検証結果は何度も再解釈される可能性があり、その度に修正すべきだと思われる仮説が違ってくるかもしれません。研究ができる AI は理想的にはこのような可能性も視野に入れて検証結果を解釈していくことができるようになることが望まれるでしょう。


\subsection{Common Topics}
So far, I have conceptually discussed elements considered important in research and their combination. In this section, I would like to explore topics that are relevant to all these elements, as well as some topics that have not been covered in this paper.

% これまで、研究において重要であると思われる要素である、問いの構築、仮説生成、仮説検証について議論し、その後でそれらを組み合わせた場合に関連する話題について議論をしました。このセクションでは、これらすべての機能に関連しうるような話題や、これまでのセクションでは触れられなかった話題のうちのいくつかについて議論していきたいと思います。

\subsubsection{Language Models}
The rapid development of language models in recent years has led to numerous innovative achievements \cite{zhao2023survey}. It is unimaginable that future research agents will not utilize the insights of language models. In this section, I will look over some attempts at scientific discoveries using language models.

Since the advent of the Transformer \cite{vaswani2017attention} and its successor BERT \cite{devlin2018bert}, it has become common to pre-train language models on large-scale corpora. In particular, with the introduction of the concept of a \textit{foundation model}, pre-trained models that can be used for many downstream tasks \cite{bommasani2021opportunities}, there has been an acceleration in efforts to construct such foundational models. In the field of science too, there have been attempts to create scientific language models pre-trained on scientific texts and to build foundational models for science \cite{beltagy2019scibert,singh2022scirepeval,nadkarni2021scientific,cohan2020specter,gupta2022matscibert,taylor2022galactica,azerbayev2023llemma,xie2023darwin,luo2022biogpt,li2023llava}. Moreover, as scientific data is not just textual but requires multiple modalities, there have been efforts to create scientific foundation models pre-trained on such multimodal data \cite{singhal2023towards,takeda2023foundation,nguyen2023climax}.

When OpenAI developed GPT-3 \cite{brown2020language}, ChatGPT (GPT-3.5) \cite{ChatGPT}, and GPT-4 \cite{GPT4} (I will refer to them as GPTs), these models garnered significant attention for their ability to perform a wide range of intellectual tasks with considerable accuracy. Following this, there was an increase in research examining the scientific understanding of GPTs, as well as studies attempting to use GPTs directly for scientific tasks \cite{bordt2023chatgpt,white2022large}. Particularly, with the initiation of efforts to connect these GPTs to form pipelines or autonomous agents, studies also emerged that involved implementing scientific tasks through such agents \cite{wang2023survey}.

There have been various attempts to use large language models for automating all processes of knowledge discovery across various fields. In terms of research areas, this includes applications in natural sciences  \cite{ai4science2023impact,bran2023chemcrow,white2022large,hatakeyama2023prompt,jablonka202314,guo2023can,boiko2023emergent,charness2023generation,qin2023gpt,zheng2023large,qian2023can,wysocka2023large,lee2023benefits,nori2023capabilities,wang2023large,singhal2023large,yang2022gatortron,deng2023learning,merchant2023scaling}, applications in mathematics and engineering \cite{wu2023empirical,pursnani2023performance,zheng2023can,zhang2023automl}, and applications in social sciences \cite{wang2023survey,bail2023can,ziems2023can,park2023generative,horton2023large,koneru2023can}. Additionally, there are efforts applicable to all research fields, which involve using GPTs for processing academic documents \cite{alzaabi2023chatgpt}. Some of them include paper processing \cite{elicit,scispace,van2023chatgpt}, paper search \cite{elicit,scispace}, paper writing \cite{transformer2022can}, abstract generation \cite{gao2023comparing}, literature review generation \cite{aydin2022openai}, and peer review \cite{wexin2023can,liu2023reviewergpt,robertson2023gpt4,hosseini2023fighting}. 

In the process of automating research, there are examples ranging from models that generate hypotheses, to those that discover and create research topics and research questions \cite{oppenlaender2023mapping,lahat2023evaluating}, and even to those used in the entire research process, including experiments \cite{boiko2023emergent,charness2023generation,qin2023gpt}. The innovative performance of such language models and their rapid spread into scientific research have sparked much debate within the scientific community \cite{birhane2023science}.

These are just a few examples of the use of language models. Furthermore, research and development on language models are progressing rapidly on a daily basis. It is hoped that further exploration will be made into the potential applications of these technologies for automating research.

% 近年の言語モデルの急速な発展は数々の革新的な成果を生み出してきました \cite{zhao2023survey}。将来研究ができるエージェントが言語モデルの知見を活用しないことは考えられないでしょう。このセクションでは、言語モデルを使った科学的発見の試みについて議論します。

% 言語モデルは Transformer と続く BERT の登場以降、大規模コーパスによって事前学習をする試みが一般的となりました。特に、多くの下流タスクに使えるモデルという基盤モデルという概念が導入されると \cite{bommasani2021opportunities}、このような基盤モデルの構築を目指す動きが加速しました。科学の分野においても、科学的テキストで事前学習をした科学的言語モデルを作成する試みや科学の基盤モデルを作ることを目指す動きが出てきました \cite{beltagy2019scibert,singh2022scirepeval,nadkarni2021scientific,cohan2020specter,gupta2022matscibert,taylor2022galactica,azerbayev2023llemma,xie2023darwin,luo2022biogpt,li2023llava}。 また、科学的データはテキストデータだけではなく複数のモダリティのデータが必要となるので、そのようなマルチモーダルなデータで事前学習した科学的汎用モデルを作る取り組みも現れました \cite{singhal2023towards,takeda2023foundation,nguyen2023climax}。

% OpenAI が GPT-3 \cite{brown2020language}, ChatGPT (GPT-3.5) \cite{ChatGPT}, GPT-4 \cite{GPT4} を開発すると（以下 GPTs）、これらのモデルがかなりの範囲の知的タスクかなりの精度でこなすことができることで大きな注目を集めました。それを受けて、GPTs の科学的理解を調べる研究や GPTs をそのまま用いて科学的タスクの遂行を試みる研究が増加しました \cite{bordt2023chatgpt,white2022large}。特に、これらの GPTs をつなぎ合わせてパイプラインや自律的エージェントを構成する取り組みが始まると、科学的タスクをこのようなエージェントに実施させる研究も現れました \cite{wang2023survey}。

% それが科学的な基盤モデルを作る試みへとつながりました \cite{taylor2022galactica}。

% さまざまな分野の知識発見のあらゆる過程の自動化に大規模言語モデルを用いる様々な試みがあります。研究分野で言えば、化学や材料科学などの自然科学への適用 \cite{ai4science2023impact,bran2023chemcrow,white2022large,hatakeyama2023prompt,jablonka202314,guo2023can,boiko2023emergent,charness2023generation,qin2023gpt,zheng2023large,qian2023can,wysocka2023large,lee2023benefits,nori2023capabilities,wang2023large,singhal2023large,yang2022gatortron,deng2023learning,merchant2023scaling}、数学や工学への応用 \cite{wu2023empirical,pursnani2023performance,zheng2023can,zhang2023automl}
% 社会科学への応用 \cite{wang2023survey,bail2023can,ziems2023can,park2023generative,horton2023large,koneru2023can} などがあります。また、全ての研究分野に共通するものとして、学術文書の処理を GPT に行わせる取り組みもあります \cite{alzaabi2023chatgpt}。 

% 化学 \cite{bran2023chemcrow,jablonka202314,white2022large,hatakeyama2023prompt,jablonka202314,guo2023can} や材料科学 \cite{jablonka202314,jablonka202314,xie2023large,kang2023chatmof,merchant2023scaling}


% 特に、自動化する研究の過程としては、仮説の生成過程はもちろんこと、研究課題やリサーチクエスションの発見や生成 \cite{oppenlaender2023mapping,lahat2023evaluating} をさせるものから、実験も含めた研究の全過程に使用するような例まで登場しました \cite{boiko2023emergent,charness2023generation,qin2023gpt}。このような言語モデルの革新的な性能と急激な科学研究への広まりは、科学コミュニティに対して多くの議論を呼んでいます \cite{birhane2023science}。

% 参考
% 自律的エージェントを科学へ応用 \cite{wang2023survey}.

% LLMs are used for scholarly document processing \cite{alzaabi2023chatgpt}, some of which include paper processing \cite{elicit,scispace,van2023chatgpt}, paper search \cite{elicit,scispace}, paper writing \cite{transformer2022can}, abstract generation \cite{gao2023comparing}, literature review generation \cite{aydin2022openai}, and peer review \cite{wexin2023can,liu2023reviewergpt,robertson2023gpt4,hosseini2023fighting}. 

% 研究課題の発見 \cite{oppenlaender2023mapping}、リサーチクエスチョンの生成 \cite{lahat2023evaluating}

% LLM の科学的知識の理解を調べる研究があります。コンピューター科学
% \cite{bordt2023chatgpt} 化学 \cite{white2022large}

% 社会科学への応用があります \cite{wang2023survey,bail2023can,ziems2023can,park2023generative,horton2023large,williams2023algorithmic}.

% 科学の基盤モデルを作る試み \cite{taylor2022galactica}

% molecular data,  protein language models,  DNA and RNA などの事前学習済みモデルを紹介してる \cite{ai4science2023impact}

% generalist model \cite{tu2023towards}

% 科学に 特化した言語モデル\cite{beltagy2019scibert,singh2022scirepeval,nadkarni2021scientific,cohan2020specter,gupta2022matscibert,taylor2022galactica,azerbayev2023llemma}. 

% マルチモーダルな基盤モデル \cite{singhal2023towards,takeda2023foundation,nguyen2023climax}.

% 科学的言語モデル（事前学習済みモデル） \cite{xie2023darwin,luo2022biogpt}

% 科学実験 \cite{boiko2023emergent,charness2023generation,qin2023gpt}.

% 化学 \cite{bran2023chemcrow,jablonka202314,white2022large,hatakeyama2023prompt,jablonka202314,guo2023can}

% 材料科学 \cite{jablonka202314,jablonka202314,xie2023large,kang2023chatmof}

% サーベイ \cite{wang2023survey}

% molecular property prediction \cite{zheng2023large,qian2023can} (Physiology, Biophysics, Physical Chemistry, and Quantum Mechanics)

% buisiness and management \cite{williams2023algorithmic}

% biomedicine, medical science \cite{wysocka2023large,lee2023benefits,nori2023capabilities,wang2023large,singhal2023large,yang2022gatortron} (not factual base)

% 地球科学 \cite{deng2023learning}

% 数学 \cite{wu2023empirical}

% アシスタント \cite{lubiana2023ten}

% エンジニアリング \cite{pursnani2023performance}

% 網羅的 by microsoft \cite{ai4science2023impact}

% 機械学習 \cite{zheng2023can,zhang2023automl}

% Instrut Tuning for Science \cite{horawalavithana2023scitune}

% Science in the age of large language models \cite{birhane2023science}

\subsubsection{Incorporating Scientific Knowledge}

I have been discussing the challenges of enabling machines to conduct research, but it's important to note that in the first place it's nearly impossible even for humans to conduct research from complete zero. 

In the first place, we humans inherently possess brains and bodies that are suited to thinking about this world, shaped through the process of evolution and development. Moreover, before conducting research, we study the academic field we are interested in and acquire basic knowledge in that area. Therefore, it would be reasonable to assume that agents should also have previously acquired fundamental knowledge about the world before it starts conducting research.

The group of studies that incorporate such biases or scientific knowledge to handle scientific data on AI is called \textit{physics-informed machine learning} \cite{karniadakis2021physics}. The incorporated biases include the ability to handle partial differential equations (PDE), symmetry, and intuitive physics \cite{hao2022physics}. Karniadakis et al. \cite{karniadakis2021physics} and Hao et al. \cite{hao2022physics} systematically organize existing research in this field. As discussed in the previous section, efforts to impart scientific knowledge through pre-training on textual and multi-modal data also common.

Moreover, we humans continue to update our scientific knowledge continuously through study parallel to research, even after initially learning the basics. It is important for machines not only to embed knowledge through pre-learning and functional biases or to search during inference, but also to continuously update their knowledge through such learning processes.

Especially in the context of knowledge acquisition in research, it's crucial to recognize that the knowledge generated in research is always being updated. Therefore, an agent must not only assimilate new knowledge but also maintain the knowledge that has been generated, be aware of how previously learned knowledge is being revised, and reflect these modifications. While it still remains an open question, there are growing discussions on this topic \cite{kitano2021nobel,zenil2023future}.

% また、私たち人間は一度基礎を学習した後も研究と並行して勉強をすることで、自分の科学的知識を継続的にアップデートしています。事前学習や機能バイアスによる知識の埋め込みや、推論時の検索だけでなく、こうした学習による継続的な知識の更新を機械にさせることも重要でしょう。

% 特に、研究における知識獲得において重要な点として、研究において生み出された知識はそれ自体が常に更新されていくという点があります。したがって、エージェントは新しい知識を摂取するだけではなく、一度生み出された知識をメンテナンスし、過去に学んだ知識もどのように修正されているのかに気を配り、これらの修正を反映させることができなければなりません。こうしたことをどのように実現していくかの議論はありますが \cite{kitano2021nobel}、依然としてオープンクエスチョンです。

% これまで機械に研究をさせることの難しさについて議論してきましたが、そもそも人間に おいても完全なゼロから研究をするのはほぼ不可能です。人間も事前に研究したい分野の勉強をしてその分野の基本的な知識を獲得した上で、研究をします。したがって、AI も研究をする前にすでにこの世界に生み出されている基礎的な知識に関しては事前に獲得していることを想定するのが妥当でしょう。

% 仮説のアップデートの話

\subsubsection{Autonomy, Generality, and Open-Endedness}
As repeatedly stated, attempts to make machines generate questions, hypotheses, and validate these hypotheses already exist. The challenge lies in executing these tasks autonomously by machines with as little human intervention as possible. Closed-loop research automation represents a highly autonomous attempt at automating research, yet even in such cases, it is said that full automation of all processes of science has yet to be realized \cite{zenil2023,coley2020autonomous,coley2020autonomousII}. 

One of the biggest issues is to let machines autonomously generate even the objectives, problems, and questions of research by themselves \cite{coley2020autonomousII}. This is because, as repeatedly stated, if machines autonomously generate goals and questions, they must also autonomously generate and validate appropriate hypotheses in response. Therefore, machines must be able to execute hypothesis generation and validation in a versatile manner that can adapt to various scenarios.

In such situations, humans cannot provide specific methods for hypothesis generation, potential hypotheses, or the necessary information in advance. Therefore, machines must be able to extract appropriate information from an open-ended environment, similar to the one humans operate in, to perform hypothesis generation and validation. Research can be understood as a process of searching for information from the outer world of scientific information and processing it in the agent's inner cognitive world, as described in \cite{hope2022computational}. Assuming an open-ended environment equates to minimizing human constraints on the scope of this outer world, and allowing the agent itself to decide as much as possible what to extract from that outer world.

Considering these autonomy-related issues, how much autonomy should be expected of machines, and to what extent constraints can be set without overly suppressing the machine's potential capabilities for autonomous hypothesis generation and validation, remains an open question.

% One of the biggest issues is to let machine generate even the objectives, problems, and questions of research by themselves \cite{coley2020autonomousII}. なぜなら、繰り返し述べてきたように、目的や問いまでも機械に自律的に生成させる場合、それに対応して機械は自律的に適切な仮説を生成したり検証したりしなければならないからです。したがって、機械はさまざまな場合に対応できる汎用的な方法で仮説生成や検証を実行なければなりません。

% このような状況では人間が事前に具体的な仮説生成の方法や仮説の候補や必要な情報を与えることができません。したがって、機械は人間が置かれているのと同じようなオープンエンドな環境から適切に情報を引き出して仮説生成や検証を実行しなければなりません。研究は outer world
% of scientific information から情報検索をしてエージェントの inner cognitive world でこれらを処理する過程だと理解できますが \cite{hope2022computational}、この outer world の範囲を可能な限り人間が制約せず、その outer world から何を取得するかは可能な限りエージェント自身に決定させるような状況です。

% このような自律性の問題を鑑みて、機械にどの程度の自律性を求めるのか、どの程度の制約であれば機械の潜在能力を過剰に抑えつけることなく自律的な仮説生成や検証を行わせられるのかは、オープンクエスチョンです。

\subsubsection{Scientific Understanding}
So far, I have mainly discussed scientific discovery, but I have barely touched upon another important goal in science: scientific understanding. Since scientific discoveries can be made without understanding \cite{krenn2022scientific}, bringing new scientific understanding to humans requires additional demands beyond what has been discussed so far.

In humans, scientific understanding involves grasping theories, or hypotheses in this paper – what they are and why they hold. Therefore, among the elements of research I discussed, the additional demand seems to be necessary for hypothesis generation. Whether this demand concerns the representation of the generated hypotheses or the description of the process of generating them is unclear, but what additional requirements are needed in this process to bring about scientific understanding and how to realize them is an important issue.

Krenn et al. propose two conditions for us to say that an AI gains scientific understanding: 1. it can recognize qualitatively characteristic consequences of a theory without performing exact computations and use them in a new context, and 2. it can transfer its understanding to a human expert. As previously mentioned, the belief systems of humans and AI may differ, so whether AI having scientific understanding is necessary for bringing new scientific understanding to humans is unclear, but it is expected to be more helpful if it does. In any case, for bringing scientific understanding to humans, it seems necessary to have the ability to transfer understanding to a human in some way, as suggested as the second condition above. The explanation of machine prediction results has already been extensively researched and discussed as explainable AI \cite{arrieta2020explainable}, and its importance has already been widely pointed out in AI for Science research, so I will not delve further into it here.\footnote{
This might sound like a far-fetched conceptual idea, but to bring about human scientific understanding through machine-driven scientific discoveries, it might be worth considering not only improving the machine's capabilities but also expanding the range of human comprehension. I stop this discussion since it might lean towards not solidly grounded conjecture, but I believe it's worth discussion in scientific community.
}


% 私はここまで主に科学的発見について話してきましたが、科学におけるもう一つの重要な目的である科学的理解についてはほとんど触れてきませんでした。科学的発見は理解なしに行いうるので \cite{krenn2022scientific}、人間に新たな科学的理解をもたらすためには、これまで議論したものに加えて追加の要請が必要です。

% 人間において科学的理解とは理論すなわちこの論文で述べてきたところの仮説がどのようなものであるかなぜそれが成り立つのかなどについての理解です。したがって、この論文における仮説生成のところに追加の要請が必要となるであると考えられます。それが生成された仮説の表現に関する要請なのか、仮説の生成過程の記述に関する要請なのかは分かりませんが、科学的理解をもたらすためにこの過程にどのような追加の要請が必要でどのようにそれを実現すべきかは重要な課題でしょう。

% Krenn らはある AI が科学的理解を持つための条件として 1. if
% it can recognize qualitatively characteristic
% consequences of a theory without performing
% exact computations and use them in a new
% context and 2. it can transfer its understanding to a human
% expert. という２つの条件をあげています。前述したように、人間と AI の信念体系は異なり得ますので、AI が科学的理解を持つことが、人間に新しい科学的理解をもたらす上で必要かは分かりませんが、それがあればより役立つであろうことは期待されます。いずれにしても人間に科学的理解をもたらすためには、Krenn らがあげている人間に何らかの方法で理解を転移させることができることは必要であるように思われます。機械の予測結果の説明は説明可能AIとしてすでに多くの研究と議論がなされており \cite{arrieta2020explainable}、AI for Science の研究ですでにその重要性は広く指摘されていますので、ここではこれ以上深入りしません。

\subsubsection{Alignment}
Similarly to other AI research, alignment is an important topic in creating autonomous AI researchers as well. First and foremost, it is necessary to ensure that autonomous researchers do not harm humans. The more autonomy is sought in machines, the more important this issue becomes. Particularly, since knowledge itself is value-neutral and can be used for good or ill, as mentioned earlier, this solving this issue challenging. This problem requires ongoing discussion.

Furthermore, as I have repeatedly stated, aligning AI with human values and worldviews is important not only for safety reasons. For example, as mentioned in Section \ref{section-question-construction}, in order to produce knowle2dge meaningful to humans, agents also need to make value judgments aligned with human judgments about the quality of questions. Similarly, agents must judge what is unknown and what is comprehensible not from their perspective but from the human perspective. Especially, these value judgments are not always explicitly expressed in existing human-generated texts, and in that sense, there is a need to actively teach these value judgments. How this will be realized is a topic that should be discussed more in the future.

% 他の人工知能の研究と同様、alignment は自律的な人工研究者を作る上でも重要な話題です。まず何より、自律的な研究者の実現が人間にとって害を与えないようにしなければなりません。機械により自律性を求めるほどこの問題は重要になりますし、知識それ自体は前述したように価値中立であり悪用することも良く使われることもできますので、この問題については今後も議論が必要です。

% そして、繰り返し述べてきたように、安全のためという以外でも AI と人間の価値観や世界観を揃えることは重要です。例えば、セクション \ref{section-question-construction} で述べたように、人間にとって意味のある知識を産出させるためには、人間の問いの良さに対する価値判断にそろった価値判断をエージェントもする必要があります。同様に、エージェントは自分にとって何が未知かではなく、人間にとって何が未知かを判断しなければなりません。そして、直前のセクションで述べたように、人間にとっての理解をもたらすためには、エージェントは人間の視点に立って人間にとって何が理解可能であるかを判断しなければなりません。特に、これらの価値判断は必ずしも人間が生み出した既存のテキストに陽に表現されているものばかりではなく、その意味で積極的にこれらの価値判断を教える必要があるでしょう。これをいかにして実現していくかは今後より議論されていくことが望まれます。

% As discussed in Section 2, when realizing an AI that autonomously conducts research, the issue of alignment arises. 

% First and foremost, it is essential to consider ways to ensure that AI does not engage in research that could harm humans. However, this is a challenging issue. The problem of ensuring that AI does not harm humans is a difficult problem in AI Alignment. Furthermore, knowledge and technology produced by research are fundamentally value-neutral. That is, the knowledge can be used for good or ill. Therefore, even if AI were to research with harmful intentions, it would be challenging to judge from the actual research results.

% The remaining two issues arise in the ultra-long term when AI becomes fully autonomous in conducting research. The second issue is that to enable meaningful knowledge production for humans, there needs to be an alignment between the knowledge systems of AI and humans. As mentioned in Chapter 2, if knowledge and verification are relative concepts to society, research conducted autonomously by AI may become meaningless to humans. On the other hand, if we were to correct AI to follow human methods entirely, we might unnecessarily limit the machine's potential capabilities. Deciding how much human methodology and values to incorporate and how much freedom to allow the machine, and finding ways to achieve this, will be a significant challenge in creating research-capable AI.

% The third issue concerns the alignment between AI and nature, not between humans and AI. As mentioned in Chapter 2, the fact that humans have come to understand nature is likely not unrelated to our long history of interacting with nature. It seems there's no guarantee that artificial machines like AI, which lack such experiences, would lead to an understanding of nature through their autonomously generated knowledge.

% The latter two issues are problems that only arise when demanding extreme autonomy from machines and are not immediately problematic. However, when discussing the limitations and possibilities of knowledge production and natural understanding by agents independent of humans, they seem to become relevant issues.



% \subsubsection{Open-Endedness}
% One of the major challenges that can be a common issue for any process, as pointed out in prior research as well  \cite{coley2020autonomousII}, is how to execute these tasks in open-ended situations. For instance, in the automation of experiments, robots use experimental equipment selected, prepared, and set up by humans. However, humans do these tasks from scratch with their own hands. Humans do not use a given corpus of papers; instead, they search for and use them on their own. Candidates for hypotheses are not explicitly provided; humans begin by identifying potential hypotheses. Even when formulating questions that serve a particular goal, humans set that goal themselves. 

% In many cases in research automation, these elements are pre-determined by humans. How to let machines autonomously perform these tasks starting only with the same initial information given to humans is crucial in realizing an autonomous artificial researcher. Moreover, having this kind of freedom is essential to achieving a general artificial researcher as well. This is because if we impose constraints on AI to research only within specific research questions or hypothesis spaces defined by humans, it cannot become an AI capable of conducting arbitrary research. Therefore, a significant challenge is how to make AI acquire complex foundational skills and fundamental reasoning abilities to realize these capacities.

% \subsubsection{Generality}
% 汎用性がなぜいるかなどの説明？上の定義はそもそもそういうモチベーションから始まってるので、ここでわざわざ書かなくても良い。

% 特定の研究課題に特化すれば、問いの構築、仮説の生成、仮説の検証を実行できるような機械はすでに開発されています。これまでのセクションでこれらの概念の定義を改めて検討したのは、研究ができる AI とは特定の研究課題に特化した方法でこれらを実行するのではなく、どのような研究対象に対してもこれらを自律的に遂行できる AI であることが望まれるからです。このような汎用的な研究ができる AI をどのように実現するかというのが課題です？

% As we have introduced so far, there are several attempts to automate research through versatile approaches. For example, the automatic generation and discovery of hypotheses \cite{kang2022augmenting,chan2018solvent,wang2023learning,xu2023exploring} or questions \cite{lahat2023evaluating,liu2023creative,oppenlaender2023mapping,surita2020can} from research papers is an approach that can automate research across a wide range of academic fields, from the humanities to natural sciences. Additionally, efforts to create general-purpose robots  \cite{yachie2017robotic}, foundation or generalist models \cite{singhal2023towards,taylor2022galactica}, or to incorporate the inductive bias for understanding physics can be considered initiatives towards general-purpose system.

% However, most automation studies target specific challenges in particular research fields. It seems that the goal of creating an AI capable of conducting any research is not receiving much attention. As explained in Chapter 2, in order to create such intelligence, it would be necessary to understand the high level concept of research, question construction, hypothesis formulation, and hypothesis verification, and to be able to execute them appropriately depending on the subject at hand.

% \subsubsection{Autonomy}
% As mentioned in the previous section, there are several attempts to automate the entire research process. A seminal early works are Adam \cite{king2004functional}, and Eve \cite{williams2015cheaper}. These are closed-loop scientific discovery systems that autonomously execute everything from hypothesis generation to research planning, based on logic AI and robotics. Furthermore, there is closed-loop automation in some research on automation using scientific workflows \cite{gil2017towards}. These are highly autonomous research automation. Recently, there has also been a movement to create autonomous agents based on language models to tackle scientific problems \cite{wang2023survey}.

% However, most of studies on research automation have targeted  only specific tasks within a sub-process of the entire research process. For example, symbolic regression focuses on automating hypothesis generation, while experiment automation pertains to data generation for hypothesis creation and testing.

% Furthermore, even in the case of closed-loop research automation,  full automation of all processes of science is yet to be realized \cite{zenil2023,coley2020autonomous,coley2020autonomousII}. One of the biggest issues is that the objectives, problems, and questions of research are given by humans \cite{coley2020autonomousII}. We discussed in Chapter 3 that there are not many studies that have attempted to automate the construction of questions. And while we pointed out in Chapter 2 that automating the construction of questions can lead to an infinite regress from the perspective of autonomy, specifying the high-level goals underlying the questions remains a significant challenge \cite{coley2020autonomousII}. Also, even after the construction of the question, there is the issue that humans are providing machines with a search space that is far more restricted than what humans themselves are given. For example, generating hypotheses from an open-ended hypothesis space has not yet been realized \cite{zenil2023,coley2020autonomousII}, and to put it in extreme terms, while humans might prepare experimental equipment from scratch, machines are given those as a given. 

% To begin with, the realization of a fully autonomous artificial intelligence is still one of the major goals yet to be achieved, not just in research. Therefore, a lot more foundational research will likely be needed to accomplish this.

% Many automation studies primarily target specific tasks within a research process. For example, symbolic regression focuses on automating hypothesis generation, while experiment automation pertains to data generation for hypothesis creation and testing. However, as mentioned earlier, some studies, such as automated research workflows and self-driving labs, aim to automate the entire research process. 

% Coley et al. discuss the advancements in automating scientific discoveries in chemistry \cite{coley2020autonomous}. Their discussion is not limited to automating chemistry but extends to the broader context of automation of science. The paper delves into the insightful topics of automated discovery, including defining scientific discoveries and criteria for assessing their autonomy. In the subsequent paper, Coley et al. organized the existing automation studies based on which processes of scientific workflow are automated \cite{coley2020autonomousII}. In that paper, they point out several challenges for science automation, ranging from dataset handling to physical and computational autonomous validation. 

% Autonomous agent for scientific problem \cite{wang2023survey}

% , and in recent years, there are attempts to automate the entire process using language models.

% \subsubsection{Generality}


% \subsubsection{Scientific Workflow}
% The workflow whose process is a research process, tasks are research tasks that receive and output scientific data is called \textit{scientific workflow} \cite{ludascher2009scientific}. By running scientific workflow systems, research process is automatically executed.  Amidst the flourishing of computational and data-driven sciences with the advancement of computers, these efforts seem to have been conceived to better manage in silico experiments \cite{liew2016scientific}. 

% Among such initiatives, just like in the case of laboratory automation, there are efforts that have achieved closed-loop automation, which completely autonomously carries out the cycle of hypothesis generation and verification \cite{gil2017towards}. Gil has presented how machine learning can be incorporated into this scientific workflow \cite{gil2022will}. She also presents a perspective on what kind of AI should be developed in order for it to become a good partner for researchers.

% Additionally, there are attempts to design workflows that are reusable beyond individual workflows, in other words, workflows with high generality \cite{hardisty2020canonical}.

% どのような研究対象に対しても問いを立て、仮説を生成し、仮説を検証できるような人工知能を作るためには、これらを汎用的な方法で実行する必要があります。このように

% \subsection{Challenges}

 % \subsection{General and Autonomous Question Construction, Hypothesis Generation, and Hypothesis Verification}

 % In Chapter 2, we explained that in order to create an AI capable of conducting any research, it is deemed necessary to realize the formulation of questions, generation of hypotheses, and validation of these hypotheses as a combination of universal skills applicable to all research. In this section, we will revisit and organize the potential challenges in achieving an AI that can conduct each of these processes.


% \subsubsection{The Difficulty of General Hypothesis Verification}
% I recognize that the the way to verify a hypothesis is strikingly diverse, as it can significantly differ depending on the subject of research. For instance, if one wishes to study the behavior of rat, they may need to train the rat. On the other hand, if you want to test a theory of particle physics, you may have to construct and run a huge particle accelerator. In the field of history, the existence of historical records might serve as evidence, while in mathematics, the verification process revolves around the proofs themselves. Due to this high degree of flexibility, hypothesis verification can be the most challenging aspect to automate for realizing a ``general'' artificial researcher.

% \subsubsection{Feedback from Verification}
% In the first place, the act of verification is a highly challenging process. Firstly, as mentioned earlier, inductive approaches cannot verify hypotheses in the same way as deductive reasoning. Also, I notice that hypothetico-deductive method, which involves verifying claims derived from a hypothesis to confirm its validity, is still widely used today. However, verifying a deduced claim does not support the hypothesis because there may be many hypothesis that can result in the same deduced claim. In response to these, Karl Popper proposed that while hypotheses cannot be confirmed, they can be falsified \cite{sep-scientific-method}. However, in practice, the verification of hypotheses involves implicitly relying on numerous auxiliary hypotheses. When using experimental apparatus, it requires many assumptions to trust them. Even when an experiment fails, determining whether the hypothesis was incorrect or the experiment itself was flawed is not as straightforward as one might think. Thus, there is inherent uncertainty in attributing the results of verification to a specific cause \cite{chalmers2013thing,sep-physics-experiment,sep-scientific-underdetermination} as I have discussed in the section of hypothesis generation. Furthermore, all reasoning and observational evidence are inevitably influenced by some form of theories, individuals, or societal factors \cite{sep-science-theory-observation}. Therefore, it is necessary to carefully examine them to ensure that they are not distorted by unintended influences.


% \subsubsection{Question Construction}



% Realizing AI that construct a ``good'' question in a generic way is challenging. As discussed in Section \ref{section:the-relativity-of-knowledge-production-to-society}, research is relative to society and different criteria can be considered for what makes a ``good'' question. Thus, some human perspective on the ``goodness'' of a question must be incorporated. We need to discuss what we consider good, what we should prioritize, and how to incorporate the value to AI.

% \textcolor{red}{TODO}

% Moreover, determining inputs to the question construction module is not trivial. In hypothesis generation, the question is the primary input, whereas in verification, it's the hypothesis. However, question formation take any input. Once you seriously try to identify the origin of question, you will encounter infinite regress. This is a unique problem that arises when aiming for a general-purpose and autonomous artificial researcher. This is because the issue revolves around how much input can be assumed while still being considered autonomous, given that it can potentially take any input.

% \cite{wang2023skillqg}

% neural question generation \cite{pan2019recent}


% \subsubsection{Hypothesis Generation}



% \subsubsection{Hypothesis Verification}



% Creating AI that autonomously verifies hypotheses is challenging. While current models can mimic human verification, truly understanding the verification strategy demands more work. Sometimes, they even need to devise the verification measure themselves.

% The biggest challenge for autonomous verification is the need to freely move around in the real world or within a computer, and to manipulate objects within that world at will. We believe this to be one of the greatest barriers to full research automation. Laboratory automation have attempted to address this challenge in real world by developing robots. We will discuss the case within the computer in Section \ref{section:behaviour-inside-the-computer}.

% \subsubsection{AI Capable of Peer Review}

% Given the difficulty of these challenges, automating peer review could be a strategic starting point. This is because peer review is a universal process across diverse research fields and it assesses the validity and quality of problems, hypotheses, and verification methods, which is easier than generating them. Despite some progress, full automation is still elusive \cite{yuan2022can,schulz2022future}.


% \subsection{Behaviour inside the Computer}
% \label{section:behaviour-inside-the-computer}


% Lab Notebook?
% \subsection{Dataset of Research Process}
% It is important to establish the necessary infrastructure for research automation. The two pillars of research automation are the development of basic models that incorporate academic knowledge and the construction of data sets. The development of an infrastructure model that incorporates scientific knowledge has already been proposed in many places and is actually under development, so I will not emphasize its necessity here again. Also, regarding data sets, the construction of data sets for the acquisition of scientific knowledge has been done in various places as well, so I will not emphasize that here either.

% Instead, we propose here to construct a research process dataset. A research process dataset is behavioral log data that incorporates all possible tasks throughout the entire process of the study, from start to finish. Ideally, individual tasks should be labeled as to whether they correspond to question construction, hypothesis generation, or hypothesis testing. We believe that building such a data set is important because, as explained in the Literacy section of Chapter 2, the current paper is not a data log of the entire research process. This makes it difficult to be data-driven and end-to-end learning how to do research itself. I believe that building a research process will help solve these problems and increase the likelihood of more flexible intelligent agents.

% However, building a dataset of the research process seems daunting. This is because researchers who do not currently keep research logs would have to go to the trouble of recording their research process.\footnote{
% In an experimental laboratory in the natural sciences, it is common to take research notes, so it may not be that difficult to record more detailed processes as an extension of this practice. However, in the machine learning field, the culture of taking research notes does not seem to be that common. (We think it is common to keep logs of experiments, but it seems to be rare to describe the details, for example, where and how the data was obtained.) 
% } Therefore, it seems necessary to devise a way to make it easier for researchers to keep logs. It may be to manage the research process on GitHub, or to take research notes as in natural science research, but it is important to discuss how to achieve these things.

% Being able to construct a dataset of the research process would be ideal, but may not be immediately feasible. As an alternative, it seems important to create a dataset designed to automate question construction, hypothesis generation, and hypothesis testing. At its simplest, one might start by building a dataset of papers labeled with the parts that correspond to the question, hypothesis, and test, respectively. This would be a relatively simple but important step in achieving a generic artificial researcher.

% Alternatively, instruction tuning could be done by viewing question construction, hypothesis generation, and hypothesis testing as tasks, respectively. This would produce a language model that can execute question construction, hypothesis generation, and hypothesis testing with greater fidelity. This could be the foundation for a general-purpose, autonomous artificial researcher.





% In the medium to long term, it's essential to devise ways to ensure that AI doesn't engage in research that could be dangerous to humans. 

% In the long term, we must contemplate how to construct a knowledge system that are mutually translatable between human society and AI society. While these issues may not arise in the short term, it's crucial to engage in discussions now, looking towards the long-term future.

% \subsubsection{Understanding}

% Extensive discourse transpires concerning scientific discoveries. Yet, discussions pertaining to scientific comprehension remain relatively unexplored. Krenn et al. delve into the conundrum of what it entails for a machine learning agent to not only unearth scientific knowledge but also to comprehend it \cite{krenn2022scientific}. They adopt a human-centric stance, positing that an agent's ability to offer explanations comprehensible to human scientists signifies the existence of its scientific understanding.

% sun-rise \cite{leslie2023does}

\section{Ideas for Prototyping}
Realizing an intelligent agent that can conduct research is an exceptionally challenging goal that will likely take a long time to achieve. The challenges discussed so far are just the tip of the iceberg found in speculative discussions, and there are undoubtedly many more yet to be identified critical issues. Therefore, it seems crucial to start by identifying and addressing unknown challenges in the first place. A good starting point might be to develop a simplified prototype of an agent capable of research so that we can explore the challenges for our goal in the process of prototyping. In this section, I would like to discuss speculatively and briefly what could be considered as such prototyping.

\subsection{Prototyping Agents that Conduct Research}
% まず初めに、研究をするエージェントのプロトタイプとしてどのようなものが考えられるか検討するところから始めましょう。
% Let me start by considering what might be a prototype of an agent for conducting research.

\subsubsection{Requirements for Prototype}
As discussed in Section \ref{section-question-hypothesis-verification}, research, I believe, consists of constructing questions, generating hypotheses, and verifying these hypotheses. Thus, it seems appropriate for this prototype to consist of these functions as modules. The question construction module takes any input and produces a question. The hypothesis generation module takes this question as input and produces a hypothesis. The hypothesis verification module takes the hypothesis as input and provides verification results. The prototype would conduct research by flexibly combining these modules at various levels.

For the prototype agent to be autonomous, human design, implementation, and intervention should be minimized. Consequently, each module, aside from receiving minimal inputs, should autonomously gather information from the open-ended world, which humans interact with to get information for research. That is, the agent should interact with the physical world or the digital realm to get information necessary for research, as humans do.

% For instance, aside from the question input from the question construction module, the hypothesis generation module shouldn't have predetermined inputs.

Furthermore, for the system to be general, the internal workings of each module mustn't depend too much on specific research topics. For example, if the verification method is an experimentation for a specific physics research, it can't be used for psychological research. The human designed inner workings of each module should be as minimal as possible, limited to only what is necessary for the function of each module. 
% This is akin to an abstract class of research.

Creating a system that meets both autonomy and generality requirements while properly constructing questions, generating hypotheses, and verifying them only with this abstract class is infeasible, even in simpler scenarios. Hence, it might be necessary to impose some constraints on this abstract framework. Discussing the extent of these constraints, why they're needed, and how they can be eliminated will help elucidate the challenges in realizing a autonomous research agent. In the following, I will list up some candidates for potential constraints to provide a first step.

\subsubsection{Candidate Constraints in Prototyping}

In Section \ref{section-what-is-research}, I discussed the view that research can be considered as updating beliefs. I also discussed the possibility of autonomously constructing verification from its foundational concepts and autonomously contemplating the value of questions when conducting autonomous research. However, these ideas are too visionary and challenging to expect immediate, meaningful results for humans by prototyping. Therefore, it seems desirable as a prototype to aim for agents that can master the values system and verification methods humans have built so far.

% \subsubsection{Grand Goal is Given}
As said in Section \ref{section-question-construction}, constructing questions from open-ended situations is a too challenging task where even where to start from is not evident. Therefore, it seems prudent to start by determining in advance what the input for constructing the question should be, rather than assuming the unrestricted information sources. A candidate for the input is a high-level goal since it is 
assumed in many studies. Particularly, it would be desirable as a first step to provide high-level goals that are recognized as a research goal in a specific research field.

% As stated in Section \ref{section-question-construction}, not all research questions are necessarily constructed to achieve some grand goal. However, many studies build their questions by breaking down such goal. Therefore, if research can be conducted to construct questions from overarching goals, then a vast majority of human-conducted research, and notably ``meaningful'' research, potentially becomes feasible.

% \subsubsection{Research is Completed Entirely within a Computer}
One of the biggest bottlenecks in realizing a fully autonomous research agent is the necessity for excellence at complex low-level actions, as discussed in Section \ref{section-countless-seemingly-unrelated-operations-to-knowledge-production}. Especially, developing a robot capable of acting freely in the physical world like humans is an extremely challenging task. Therefore, for prototyping purposes, it seems reasonable to first consider research that does not require interaction with physical world but is confined within a computer. Of course, realizing an agent that can freely operate within a computer is also a very challenging issue, but it seems more feasible than an agent freely operating in the physical world. In fact, there have been attempts to make language models perform any operation on a computer \cite{openinterpreter,openai_chatgpt_plugins_code_interpreter_2023}, or operating a web browser \cite{nakano2021webgpt,act1}.

The purpose of prototyping is to materialize the concept, even if it's rudimentary, and identify challenges. Therefore, it seems desirable for prototyping to first limit the target environment to within a computer and wait for the advancement of foundational research for the realization of free activity in the physical world.

The examples mentioned here are merely a few ideas and are neither absolute nor comprehensive. Instead, it seems important in prototyping to discuss to what extent and what kind of constraints should be applied. It is expected that more appropriate constraints will become apparent as such discussions deepen in the future.

% \subsubsection{Verification is Limited to Humans'}
% The issue that allowing machines to conduct research entirely autonomously could result in constructing knowledge systems meaningless to humans arises when we try to let AI construct even verification from scratch. This is necessary if we want to truly say an AI can verify on its own. On the other hand, many people probably have no interest in generating knowledge that is meaningless to humanity. In the first place, even if such a thing were realized, humans might not be able to evaluate whether the AI has truly constructed a meaningful knowledge system for them. Also, I don't think human researchers always understand or construct verification from first principles. Therefore, it might seem harsh to demand these of AI. So, it seems preferable to first ensure that AI understands and can always use the verification methods that humans use. Specifically, we aim to make sure AI can always proficiently use experiments, statistical hypothesis testing, proofs, etc. By doing so, I believe there's a possibility to realize an AI that conducts highly generalized, autonomous research that is meaningful to humans.

% \textcolor{red}{参考}
% There are ongoing initiatives to enable language models to operate browsers \cite{nakano2021webgpt,act1}. While full browser operations might seem ambitious, there are already endeavors to allow language models to conduct searches \cite{mialon2023augmented}. If we achieve browser automation, it will greatly advance research automation involving web operations. Moreover, efforts like the open interpreter \cite{openinterpreter} aim to automate any computer action. This direction holds promise for automating all research confined within a computer. Although these studies are gaining traction in the machine learning domain, they're not always linked to research automation. We advocate recognizing this as a pivotal challenge in the realm of research automation.

\subsubsection{Implementing Each Module with Large Language Models}

Considering the necessity for generality and the remarkable performance of LLMs, it would be inevitable to instantiate each module as a LLM. In reality, as stated in previous sections, studies to construct automated research pipeline as LLM pipeline have emerged. I believe we should start from prototyping agents as such LLM pipelines. Particularly, I believe that we should create an autonomous research agent, in line with attempts to realize autonomous agents using language models \cite{wang2023survey,xi2023rise}.

Here is one provisional idea modeled after a typical autonomous agent. The research agent start from formulating a question given a high-level goal input by human. Once the question is posed, the agent then automatically generates hypotheses that could answer this question and subsequently verifies them. Once the final verification results are produced, they are interpreted in light of the original objective and research question, leading to the generation of the next question. However, as described below, the agent will iteratively and hierarchically repeat the processes of question construction, hypothesis generation, and hypothesis verification to execute each of these subprocesses.
% まず初めに、人間が目標を入力して、それを元に問いを立てるところから出発します。エージェントが問いを立てたら、次にエージェントは自動的にその問いの答えの候補となる仮説を生成し、最終的にこれを検証します。最終的な検証結果が出力されたらそれを当初の目的とリサーチクエスチョンに照らして解釈し、次の問いの生成などに向かいます。

The agent is assumed to perform essentially four actions: 1. formulating questions, 2. determining whether the task is completed, 3. verifying hypotheses, and 4. executing any low-level action on a computer. The processes of formulating questions, generating hypotheses, and verifying them are primarily realized by performing low-level actions on a computer.

If the agent chooses to generate a question, it temporarily suspends the current task, such as hypothesis verification, and always starts generating a hypothesis for that question. Once the generation of hypotheses for that question is deemed complete, the agent chooses whether to verify them or not, and after verification, it updates the hypotheses based on the results. Regardless of whether the hypotheses were verified, the agent then resumes the higher-level process that was previously interrupted, using the outcome of the low-level process. In this manner, the agent repeats the lower-level question construction, hypothesis generation, and verification until the highest-level hypothesis is generated. However, when the highest-level hypothesis, hypothesis to the original question, is generated, the agent always starts verifying that hypothesis.
% エージェントは大きく分けて、1. 問いを立てる、2. タスクが終了したかを判定する、3. コンピュータ上の任意のアクションを実行する、の基本的には3つの行動を取ると想定します。そして、この問いを立てる過程や仮説を生成するや検証をする過程は基本的にはコンピュータ上の任意のアクションを取ることで実現します。ただし、エージェントが問いを生成することを選択した場合、現在取り組んでいる仮説生成などのタスクを一時中断し、新しくその問いに対する仮説を生成することを開始します。その問いに対する仮説生成が終了したと判断されたら、それを検証するかしないかを選択し、検証を終えたら検証結果を受けて仮説を更新します。検証をしたにせよしなかったにせよ、これらによって生成された仮説の出力結果を受けて、先ほど中断していた上位の仮説生成の過程を再開します。このようにして、最上位の仮説が生成されるまで下位の問いの構築と仮説生成及び検証を繰り返します。ただし、最上位の仮説が生成された場合だけは、必ずその仮説の検証を開始します。

To ensure this system is general to be adaptable to many types of research questions, prompts given to these language models should consist only of general instructions. For instance, an instruction like ``generate a hypothesis for the following question'' is so general that it can be used for any research questions. Naturally, merely providing such instructions won't automatically yield research outcome from scratch, so there may be a need to provide additional as general as possible auxiliary instructions; one of the main purposes of prototyping is to explore them. 

For open-ended operations within a computer space, ideally, the LLMs should only be given access to nearly all operations on the computer. As mentioned above, efforts to develop language models capable of taking any action in such environments have already begun \cite{openai_chatgpt_plugins_code_interpreter_2023,openinterpreter}. Minimal access to web browsers, search engines, or shells might be acceptable, but provision of custom corpora or predefined hypothesis spaces should be avoided. If research can be autonomously conducted under such conditions, it would indeed signify that the system is capable of independent research.

% I worked with a research group attempting to automate research, and together, we created a simple mock-up expressing such a concept \textcolor{red}{CITATION}. We assumed a given question and examined how much GPT-4 could generate and validate hypotheses using only the most general prompts possible. While this initiative is still in its early stages and is limited to very basic problem settings, based on our findings here, I hope to eventually develop a system that more closely resembles human research capabilities.

% 言語モデルすごいし、言語モデルでやってくの考えたいよね。汎用的なのを言語モデルでやってみるならどんなになるだろう？→できるだけ汎用的な指示ということになるよね！みたいな。言語モデルが研究ができるというのはどう調べるか←汎用的な指示というのが一つのテストとして使う？

\subsubsection{Agents that Conduct Machine Learning Research}
To give a high-level goal, we should determine which research field's what type of objectives to provide. It seems desirable for this research field to to meet the aforementioned constraints and to be suitable for prototyping.

I believe that machine learning research is good for such prototyping. Firstly, some machine learning research can be fully completed on a computer, meeting the aforementioned constraints. Secondly, it has a shorter research cycle compared to other fields, allowing for faster feedback cycles. Thirdly, machine learning is essential for the realization of research-capable agent and also currently serves as a foundational technology in many research fields. Thus, automation of it will advance our original goal itself, while contributing the automation across many research fields at the same time. Finally, there already have been efforts for automation, such as AutoML \cite{hutter2019automated,bischl2023hyperparameter,lindauer2020best,white2023neural} and MLOps \cite{kreuzberger2023machine}. Especially in recent years, there have been attempts to perform these tasks using language models \cite{vijay2023prompt,zheng2023can}. These accumulated achievements will likely further assist in creating agents that can conduct machine learning research.

In summary, I believe it would be beneficial to start with the prototyping of autonomous agents consisting of language models given as generic instructions as possible, capable of conducting certain types of machine learning research. Such efforts have already begun, but I hope that more people will join in and further accelerate this movement.

% We believe that automating machine learning research may be suitable to start with in terms of these decision axes. First, let's discuss bootstrapping. Many of the challenges to automating research will be how to get machines to acquire from experience what they are currently hardcoding and doing in the real world. Learning from experience is exactly what machine learning does, and in this sense, many of the challenges in realizing autonomous artificial researchers can be formulated as machine learning research challenges.

% Next, let us discuss feasibility. In the first place, many machine learning studies are conducted entirely on computers. As mentioned earlier, the greatest difficulty in achieving general automation lies in the interaction with the real world. Technologies related to real-world interaction are used for hypothesis verification rather than the verification itself. This requires advancements in robotics research. Therefore, to pursue the automation of the entire research process, it may be best to set aside fields that require interaction with the real world and initially focus on automating research that can be done solely on PCs. 

% Also, many attempts to automate machine learning processes have already been made. For example, in MLOps, various pipelines for automating tasks such as experiment management and training in machine learning have been proposed and put into practical use. AutoML, which is a field of machine learning research, has also produced numerous innovations in automating many of the tasks involved in machine learning. Moreover, the culture of machine learning and related engineering fields already has a wealth of knowledge and insights regarding automation. This means that we do not have to devote many resources to automating research domain-specific tasks. This allows us to focus on more essential questions in our quest to become general-purpose artificial researchers, such as ``How do we allow people to test hypotheses?'' Furthermore, many studies in machine learning and related research areas are open-source. Consequently, it is considered easier to retrieve information from papers compared to other fields. 

% Finally, we would like to discuss the impact on other sectors. As you are already aware, many research fields are currently using machine learning technologies. The AI for Science initiative, which aims to automate the scientific field, also uses machine learning technology. Therefore, the automation of machine learning research and better knowledge production will accelerate all of these efforts. For these reasons, we believe it is a good approach to start by automating a specific research project in the machine learning domain.

% \subsubsection{What Type of Research in Machine Learning Should We Start with and How?}
% There are many different types of machine learning research, but where should we start with automation? Even though we aim to automate the construction of questions, what types of research should we guide them to do?

% It seems that a example of the research appropriate as one a starting point is that on the zero-shot prompt proposal. First, the hypothesis (or proposal) in this study is the specific text of the prompt. This is much less expensive to implement, whereas many empirical machine learning proposals require composing an algorithm or architecture. Validation requires the automation of the task of preparing existing data and models, and this is certainly a difficult task. However, this is an extremely common task in machine learning research and is not unique to this research project. The ability to automate this task would benefit a significant amount of machine learning research. The validation criterion is also generic, as it is a typical validation criterion that compares the proposed group with the control group. We think we will first build a prototype by adjusting the LLM prompts, and the fact that there is no strict mathematical or logical manipulation, which language models are not good at, is another aspect that makes this research easy to do.

% With this in mind, one research project in which the author of this paper is participating is in the process of building a prototype of the pipeline of research for the prompt proposal \footnote{
% Link to the pipeline: \href{https://github.com/t46/mock-pipeline}{https://github.com/t46/mock-pipeline}
% }. It is currently still in the pilot stage and some parts are hard-coded, but will be updated as needed.

% What we have described here is just one example and a suggestion. We hope that more similar initiatives will emerge in other studies.


% We propose to start by building a research pipeline, connecting the modules of the knowledge production system. The research pipeline is a software system that takes input and generates knowledge as output, encompassing the sequence of processes involved in research. Since this process does not require human intervention, it can be considered as an autonomous research system. We propose this system to be composed of the sub-processes of ``question construction,'' ``hypothesis generation,'' and ``hypothesis verification.'' This creates a general system that is potentially applicable to any research. These sub-processes can be likened to abstract classes in programming. Each process automatically formulates appropriate questions, generates hypotheses, and performs verification based on the input. 

% Initially, we will assume a specific research problem, and this research problem can be a simple one. And the inner workings of detailed hypothesis testing and hypothesis generation can be guided (but not hard-coded) to achieve the desired results. Anyway, the high-level concept is to create the minimum necessary to automatically execute each process of question construction, hypothesis generation, and hypothesis testing. Then, by gradually making the contents of each module autonomous and gradually loosening the restrictions on the research problem, we will lead to a general-purpose and autonomous artificial researcher.

% \subsubsection{Guided but Not Hard-Coded}

% It is important to note, however, that even in the prototype stage, the internal implementation of question construction, hypothesis generation, hypothesis testing, etc., should be ``guided'' and not ``hard-coded'' as much as possible. In machine learning, induction is the process of adding words to the prompt that make it easier to output the expected answer, and hardcoding is the process of actually inserting the desired processing into the algorithm. For example, if a statistical hypothesis test is expected to be used as a means of testing a hypothesis, rather than having a human write a program that contains a process for performing a statistical hypothesis test, we would instead instruct to the machine learning model with prompting, ``Statistical hypothesis testing is one of the leading methods in verification. The hypothesis is A. Verify this.'' This is a very important point to emphasize.

% This is a very important point, so let me emphasize it. The reason this is important is that we do not want to automate a particular hypothesis testing process, but rather we want the machine to test the hypothesis itself. Only when you make sure that the machine decides on its own the appropriate verification method according to the hypothesis, will you be able to provide collateral evidence that the machine itself is able to verify the hypothesis. If this can be done not only in hypothesis testing, but also in all aspects of question construction and hypothesis generation, we can call it a prototype of a general-purpose, autonomous artificial researcher.

% \subsubsection{Why Pipeline?}

% There are two reasons why we think it is a good idea to start by building such a research process pipeline. The first is that this is one simplified representation of a generic and autonomous research system. Research is a very complex task, so when we try to automate validation, we inevitably focus on automating individual tasks. In addition, many research automation efforts are aimed at making things better, which often leads to a strong dependence on the domain, for example, in automating hypothesis generation. However, as emphasized above, what we want to achieve is not specific hypothesis generation or verification, but the ability to generate and verify hypotheses themselves. This system emphasizes that point, and once realized, it will be an example of what a general-purpose, autonomous artificial researcher could look like. The creation of such an example will serve as a guidepost for more people to become versatile and autonomous artificial researchers.

% Second, building on this would further clarify the challenges in achieving a general-purpose and autonomous artificial researcher. In this paper, we have discussed the challenges that would be necessary to realize a general-purpose, autonomous artificial researcher. However, we believe that this is a very difficult task and that there are many areas where we do not even know what the problems really are. Therefore, it is important to first identify what the problems are in the first place and where the uncertainties lie. When we move toward such a complex problem, we start with a simple example to understand the structure of the problem. For example, we build toy models in physics, concrete examples in mathematics, and prototypes in programming. The research process pipeline falls under such simple examples in autonomous artificial researchers. In the process of trying to achieve this, we will discover what are the bottlenecks and what are the essentials. In this way, I think it is important to build a research process pipeline in order to first increase the resolution of the problem and clarify the issues.

% \subsubsection{Where to Start?}
% It is advisable to start by representing a specific research as a pipeline. Initially, creating a concrete system helps clarify the actions involved in actual research and makes the specific challenges to be addressed more tangible. When dealing with projects with high uncertainty, it is crucial to concretize the problems to be solved. Specifically, the goal is to programmatically represent the actions that researchers perform as comprehensively as possible. It is acceptable to consider certain aspects as constants if their execution is too challenging to represent as a program. Then, running the system should reproduce the original research. The next step is to progressively automate the processes and constants provided by humans to enhance autonomy. Naturally, automating a specific research pipeline alone does not guarantee the development of an autonomous pipeline. However, this approach allows for the identification of research automation challenges and paves the way for their resolution through research and development. Importantly, it is essential to express individual tasks as components or sub-processes of question construction, hypothesis generation, or hypothesis verification. This is similar to inheriting an abstract class, ensuring that the automation of these processes is achieved as individual tasks are automated.

% In practice, it becomes evident that fully automating an entire research is highly challenging. Therefore, before automating specific research, it may be advantageous to start by creating simplified toy models and aiming to build systems that can execute them automatically. For example, certain parts that require obtaining and using a real dataset can be replaced with appropriately created sample datasets. The approach is similar to that of specific research pipelines, addressing research challenges while aiming to increase autonomy and generality.

% \textcolor{red}{Remarks about Autores PJ}

% \subsection{Which Field of Research to Start with?}
% We suggested that we might start by automating specific research areas and research tasks. So what research areas should we start with? As it turns out, we think it might be a good idea to start by automating machine learning research. There is, of course, a bias due to the fact that the authors of this paper are machine learning researchers and that we are writing this paper primarily for machine learning researchers, but aiming to automate machine learning research makes a lot more sense than that. To illustrate this, let me first introduce some of the perspectives involved in decision making in the area of research to be automated.

% \subsubsection{How to Choose Research Area}
% The first perspective is how much automation of that research area will help achieve a versatile and autonomous artificial researcher. We believe that it would be a good idea to automate research areas that would accelerate the automation of research. For example, if there is a problem to be solved in order to generate a hypothesis, the problem itself could be set as a research problem and automation of this research could be realized. If we can automate such a task, we have not only achieved our goal of automating the entire research process, but we have also solved the problem of research automation. Such bootstrapping will accelerate the automation of the research process and allow it to reach its goals more efficiently. \footnote{
% Inspired by the feedback from Hiroshi Yamakawa
% }

% The second aspect is feasibility. Since the objective of this project is to create a prototype, it is an important policy to start with the least difficult to realize. There are various levels of difficulty, but the most important is whether the level of difficulty is high, especially in areas other than those essential to the automation of a general-purpose research process. For example, it would be more feasible to generate hypotheses from papers now that language models have been developed than to actually construct and conduct a experiment and generate hypotheses from the observations, in the sense that it would be fully automated. Also, if we were to start with automation using a language model, it would be better not to include tasks that the language model is not good at. As emphasized above, it is better to start where it is as easy as possible here, because focusing on automation of the parts that depend on individual research tasks is not important for the goal of acquiring generalizable knowledge to achieve a general-purpose artificial researcher.

% The third aspect is whether the field has an impact on many studies. First of all, an area that has an impact on many studies is one that is used as an elemental technology in those studies. This would be appropriate as a research area to automate for general-purpose artificial researchers, in the sense that it is a general-purpose technology. And if areas that affect many studies can be automated, it will also accelerate the knowledge production of those studies. The efficiency of knowledge production for humanity as a whole will increase, and research automation projects will also benefit from the knowledge produced by them. It would also increase the population of people involved, which may lead more people to pay attention to research automation. This will spawn new flows of people, money, and knowledge, and as a result, projects are expected to move forward more quickly.


\subsection{Prototyping Agents that Conduct Peer Review}
In order to identify challenges for realizing agents capable of conducting research, aiming to automate the peer review process of academic papers might also be a good initial step. There are several reasons why it may be suitable. I'd like to explain them in the following section.

\subsubsection{Why Agents that Conduct Peer Review?}

First and foremost, the elements necessary for peer review are closely related to those required by a research-capable agent. This is because peer review involves judging essential aspects of research, such as the soundness of the verification.

Secondly, automating the peer review may be relatively less challenging than realizing an agent capable of conducting research. The reason is that while peer review only requires judging whether the necessary elements for research are present, to realize a research-capable agent, it's not just about judgment but also about being able to compose those elements. It seems desirable to start by tackling simpler problems first as a prototype to highlight challenges.

% To create an AI that can research, one must generate studies from scratch. In contrast, automating peer reviews involves automating the evaluation of already completed research. Typically, it is expected that generative tasks are easier to automate than classification or judgment tasks. As previously mentioned, understanding the essence of research is necessary for automating peer reviews. Hence, the challenges that arise during this automation process are expected to be beneficial for creating an AI capable of conducting research.

Thirdly, peer review is a widely practiced convention regardless of the research field. Therefore, the insight found during peer review automation can be beneficial to realize a general research agent. 
% Third, peer review is a discipline-agnostic practice. Therefore, automating it is expected to be important in gaining insights to realize a general-purpose artificial researcher. 

% Fourthly, compared to the general automation of other processes such as constructing questions, there is already a substantial accumulation of prior research on the automation of peer reviews.

% Fourth, there is more prior research in automating peer review than in automating generic hypothesis testing or hypothesis generation. Therefore, it is an area where the hurdles for starting a new research project are relatively low. 

Fourthly, peer review is mostly completed through text manipulation alone. There is no need for interactions with the physical world or the computer realm that are necessary for autonomously conducting research. While searches to investigate prior research might be necessary, tasks like executing experiments are, at the very least, not required. Thanks to the advancement of LLMs, we are now capable of handling text at a significant level. Therefore, we can purely focus on challenges related to the evaluation of research. This is advantageous when identifying challenges to achieve our objective.

% Fifth, peer review is almost always completed by text manipulation. With the development of language models, the cost of doing this has come down considerably. 

Finally, peer review requires a judgment of value, such as the ``significance'' of a question. As mentioned above, alignment is one of the biggest challenges. To solve this problem, it matters to first understand how humans make value judgments in research. And peer review is a rare example where such values are explicitly assessed. In this sense, the automation of peer reviews could be a good start point.

% One major barrier to automating peer review is the perceived lack of sufficient data for peer review. First, in many research fields, peer review is done in a closed manner and there is no access to peer review data. This makes it difficult to automate peer review in a data-driven way in those fields. However, at least in the field of machine learning, there are many peer-reviewed comments that are open to the public, so this is not so much of a problem in the machine learning field.

% Second, the quality of peer review comments varies. Especially in the machine learning field, the number of reviewers is insufficient for the number of conference submissions. As a result, reviewers sometimes have to review papers in fields in which they do not specialize. This undermines our credibility as the gold standard for peer review comments and evaluations. But even if there were no such circumstances, peer review would still vary from person to person in the first place. This is because there is no clear-cut correct answer to the non-epistemic value judgment of what constitutes ``importance'' and the epistemic value judgment of what constitutes ``validity'' of verification. Currently, each researcher merely makes subjective decisions according to his or her own axis of judgment. In fact, it is known that machine learning research has shown that peer review results vary.
% \textcolor{red}{(citation needed)}

% However, this second point is a difficulty that occurs inherently in the process of peer review, and it is a difficulty that must be resolved in order to realize automation of research. Rather, the main issue is how to make machines acquire the value judgments that are currently tacit knowledge. Therefore, it would be useful to first analyze and discuss these value judgments, at least among humans, and then proceed to form some kind of consensus. \textcolor{red}{TODO:(move to challenge)}

\subsubsection{Peer Review Automation}

There is a bunch of studies that have tried to automate the peer review process. Researchers have tried to automate review generation \cite{yuan2022can,yuan2022kid,wang2020reviewrobot}, paper screening \cite{schulz2022future}, research paper assessment \cite{kousha2022artificial}, reviewer assignment \cite{zhao2022reviewer}, and more. As in other fields, recent years have seen research on the automation of peer review using large language models such as GPTs \cite{wexin2023can,liu2023reviewergpt,robertson2023gpt4,hosseini2023fighting}. For traditional research on the automation of peer review,  Kousha et al. \cite{kousha2022artificial} and \cite{lin2021automated1} Lin et al. have conducted comprehensive literature reviews. 

The existence of such efforts is important in the context of the aforementioned prototyping. It is desirable that further discussions deepen on how to realize AI capable of peer review, while referencing the insights gained from these attempts.

\section{Conclusion}
In this paper, I conducted a speculative examination of the concept of an intelligent agent that can conduct research. I began by discussing what it would mean to conduct research, implying that research could be seen as the act of updating beliefs in hypotheses. I then discussed the construction of questions, generation of hypotheses, and verification of hypotheses, which are seen as essential elements in research. After discussing the additional topics, I pointed out the importance of highlighting challenges in realizing such agents and shared some simple ideas for prototyping.

The discussions in this paper are all speculative. The definition of research discussed is provisional, the challenges and implications mentioned are just a fraction of the vast possibilities, and the prototyping ideas are akin to simple toys. Also, the literature cited is far from exhaustive, with many important works not covered. My capacity to adequately evaluate each reference might have been insufficient, leading to one-sided assessments or errors. The paper is planned to be updated in the future, and these shortcomings will be addressed in these updates. Any feedback or corrections are highly appreciated.

The reason for publishing this paper in its current, idea-stage form, despite its many insufficiencies, is to provide a starting point for thinking about the concept of a research-capable intelligent agent. I hope this paper will be of some help to researchers aiming to realize such agents and will contribute to more vibrant discussions in the future.

% この論文では、研究ができる知的エージェントという概念について思索的な検討を行いました。まずはじめに、研究をするということはどのようなことと言えそうかを議論しました。認識論の議論を手がかりに、研究とはある答えがわからない仮説が真であるという信念を更新する行為とみなすことができるのではないかという議論をしました。次に、研究において不可欠な要素であると思われる、問いの構築、仮説の生成、仮説の検証について議論しました。これらの特徴とそれを機械が実行することについての示唆について議論したのち、これらを組み合わせた場合やこれらに共通して言える課題やトピックについて話しました。最後に、このようなエージェントの実現を目指すために課題をあぶり出すことの重要性を指摘し、そのためのプロトタイピングの簡単なアイデアを共有しました。

% この論文で展開した議論はいずれも speculative な議論です。この論文で議論した研究の定義は暫定的なものであり、得られた課題や示唆も膨大に考えられるもののうちのほんの一部の側面のみであり、プロトタイピングのアイデアも簡単なおもちゃのようなものです。また、本論文で紹介した文献は私が触れられなかった重要な文献は山ほどあり到底網羅的ではありません。今回の論文が言及した範囲が広すぎるため各文献を適切に評価するには私の能力が不足しており、私の各文献に対する評価も一面的であったり誤りが含まれていたかもしれません。また、今回は触れられなかった重要な論点はいくつもあります。この論文は今後もアップデートをしていく予定で、これらの不足している点についてはアップデートしていきたいと思っています。もし何かお気づきの点がありましたら、ぜひご指摘いただけますと幸いです。

% このようにまだ多くの不十分な点を抱えたアイデア段階でこの論文を公開することにしたのは、研究ができる知的エージェントという概念について是非考えていく一つのきっかけを提供できればとの思いからです。研究ができる知的エージェントの実現を目指す研究者たちの少しでも助けになり、今後の議論がより一層盛り上がっていく一助となれば幸いです。

% In this paper, we explored what needs to be done to create intelligence capable of autonomously conducting any research. Firstly, we examined the definition of what constitutes research. We then proposed the idea that research might be the act of producing new knowledge for a society and that producing knowledge might be updating the collective beliefs of a society. As a result, we discussed that the core of research lies in formulating questions, generating hypotheses, and verifying those hypotheses. We also discussed the implications provided by the relativity of the research subject to society. Next, we briefly introduced examples of initiatives trying to automate research. While there has been significant progress, we explained that there are still barriers to realizing a general-purpose and autonomous artificial researcher. Lastly, based on these discussions, we debated the challenges we believe are crucial in realizing a general-purpose and autonomous artificial researcher. As a first step, we proposed building a prototype of an autonomous research pipeline driven solely by general instructions.